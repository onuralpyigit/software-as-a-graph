{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure Topological Criticality Analysis with Impact-Based Validation\n",
    "\n",
    "## Refined Methodology\n",
    "\n",
    "This notebook demonstrates an improved approach that separates **prediction** from **validation**:\n",
    "\n",
    "### **Composite Criticality Score (Pure Topology)**\n",
    "$$C_{\\text{score}}(v) = \\alpha \\cdot C_B^{\\text{norm}}(v) + \\beta \\cdot AP(v)$$\n",
    "\n",
    "Where:\n",
    "- $C_B^{\\text{norm}}(v) \\in [0,1]$ = normalized betweenness centrality\n",
    "- $AP(v) \\in \\{0,1\\}$ = articulation point indicator\n",
    "- $\\alpha = 0.6, \\beta = 0.4$ (default weights)\n",
    "\n",
    "### **Impact Score (Validation Ground Truth)**\n",
    "$$I(v) = 1 - \\frac{|R(G-v)|}{|R(G)|}$$\n",
    "\n",
    "Where $R(G)$ = set of all reachable vertex pairs\n",
    "\n",
    "### **Topic Importance (Context)**\n",
    "QoS policies inform which components are important for interpretation, but do NOT directly affect criticality scores.\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Approach Makes Sense\n",
    "\n",
    "1. **Scientific Rigor**: Separating prediction (topology) from validation (actual impact)\n",
    "2. **Computational Efficiency**: Impact calculation is O(n²) - only computed for validation\n",
    "3. **Clear Interpretation**: Topology predicts criticality; impact validates the prediction\n",
    "4. **Falsifiability**: If correlation is low, we know the approach needs refinement\n",
    "5. **Practical Utility**: Fast topology-based scoring for real-time analysis; slower impact calculation for periodic validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from typing import Dict, List, Set\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "print(\"✓ Environment ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Sample System\n",
    "\n",
    "Generate a realistic pub-sub system for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pubsub_system() -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Create a representative pub-sub system\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Physical nodes\n",
    "    for i in range(1, 6):\n",
    "        G.add_node(f'Node{i}', type='Node', name=f'Node{i}')\n",
    "    \n",
    "    # Node connectivity\n",
    "    connections = [(1,2), (2,3), (3,4), (4,5), (1,5), (2,4)]\n",
    "    for i, j in connections:\n",
    "        G.add_edge(f'Node{i}', f'Node{j}', type='CONNECTS_TO')\n",
    "        G.add_edge(f'Node{j}', f'Node{i}', type='CONNECTS_TO')\n",
    "    \n",
    "    # Brokers\n",
    "    for i in range(1, 3):\n",
    "        G.add_node(f'Broker{i}', type='Broker', name=f'Broker{i}')\n",
    "        G.add_edge(f'Broker{i}', f'Node{i}', type='RUNS_ON')\n",
    "    \n",
    "    # Topics with QoS\n",
    "    topics = [\n",
    "        ('T1', 'payment/orders', 'PERSISTENT', 'RELIABLE', 0.95),\n",
    "        ('T2', 'user/events', 'TRANSIENT_LOCAL', 'RELIABLE', 0.75),\n",
    "        ('T3', 'metrics/system', 'VOLATILE', 'BEST_EFFORT', 0.40),\n",
    "        ('T4', 'notifications', 'TRANSIENT', 'RELIABLE', 0.65),\n",
    "        ('T5', 'inventory/updates', 'PERSISTENT', 'RELIABLE', 0.90)\n",
    "    ]\n",
    "    \n",
    "    for tid, name, dur, rel, importance in topics:\n",
    "        G.add_node(tid, type='Topic', name=name, durability=dur, \n",
    "                  reliability=rel, qos_importance=importance)\n",
    "        broker = 'Broker1' if int(tid[1]) % 2 == 1 else 'Broker2'\n",
    "        G.add_edge(broker, tid, type='ROUTES')\n",
    "    \n",
    "    # Applications with pub/sub patterns\n",
    "    apps = [\n",
    "        ('PaymentService', 'Node1', ['T1'], ['T5']),\n",
    "        ('UserService', 'Node2', ['T2'], ['T1', 'T4']),\n",
    "        ('MetricsCollector', 'Node3', ['T3'], ['T1', 'T2', 'T5']),\n",
    "        ('NotificationService', 'Node3', ['T4'], ['T2', 'T5']),\n",
    "        ('InventoryService', 'Node4', ['T5'], ['T1']),\n",
    "        ('APIGateway', 'Node1', [], ['T1', 'T2', 'T3', 'T4']),\n",
    "        ('OrderProcessor', 'Node2', [], ['T1', 'T5']),\n",
    "        ('AnalyticsEngine', 'Node5', [], ['T3', 'T5']),\n",
    "    ]\n",
    "    \n",
    "    for app, node, pubs, subs in apps:\n",
    "        G.add_node(app, type='Application', name=app)\n",
    "        G.add_edge(app, node, type='RUNS_ON')\n",
    "        for t in pubs:\n",
    "            G.add_edge(app, t, type='PUBLISHES_TO')\n",
    "        for t in subs:\n",
    "            G.add_edge(app, t, type='SUBSCRIBES_TO')\n",
    "    \n",
    "    # Create DEPENDS_ON relationships\n",
    "    for app1, _, pubs, _ in apps:\n",
    "        for topic in pubs:\n",
    "            for app2, _, _, subs in apps:\n",
    "                if app1 != app2 and topic in subs:\n",
    "                    G.add_edge(app2, app1, type='DEPENDS_ON')\n",
    "    \n",
    "    return G\n",
    "\n",
    "G = create_pubsub_system()\n",
    "\n",
    "print(\"System Overview:\")\n",
    "print(f\"  Total components: {G.number_of_nodes()}\")\n",
    "print(f\"  Total relationships: {G.number_of_edges()}\")\n",
    "type_counts = {}\n",
    "for _, data in G.nodes(data=True):\n",
    "    t = data['type']\n",
    "    type_counts[t] = type_counts.get(t, 0) + 1\n",
    "for t, c in sorted(type_counts.items()):\n",
    "    print(f\"    {t}: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Pure Topological Criticality\n",
    "\n",
    "Calculate criticality using ONLY betweenness centrality and articulation points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topological_criticality(graph: nx.DiGraph, alpha: float = 0.6, \n",
    "                                   beta: float = 0.4) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute criticality using ONLY topological metrics\n",
    "    C_score(v) = α·C_B^norm(v) + β·AP(v)\n",
    "    \"\"\"\n",
    "    # Compute centralities\n",
    "    betweenness = nx.betweenness_centrality(graph, normalized=True)\n",
    "    degree = nx.degree_centrality(graph)\n",
    "    \n",
    "    # Find articulation points\n",
    "    undirected = graph.to_undirected()\n",
    "    articulation_points = set(nx.articulation_points(undirected))\n",
    "    \n",
    "    # Build results\n",
    "    results = []\n",
    "    for node in graph.nodes():\n",
    "        data = graph.nodes[node]\n",
    "        is_ap = node in articulation_points\n",
    "        \n",
    "        # Calculate composite score (PURE TOPOLOGY)\n",
    "        composite = alpha * betweenness[node] + beta * (1.0 if is_ap else 0.0)\n",
    "        \n",
    "        results.append({\n",
    "            'node': node,\n",
    "            'name': data['name'],\n",
    "            'type': data['type'],\n",
    "            'betweenness': betweenness[node],\n",
    "            'degree': degree[node],\n",
    "            'is_articulation_point': is_ap,\n",
    "            'qos_importance': data.get('qos_importance', 0.5),\n",
    "            'composite_criticality': composite\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df = df.sort_values('composite_criticality', ascending=False)\n",
    "    return df\n",
    "\n",
    "# Compute topological criticality\n",
    "topology_df = compute_topological_criticality(G)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOPOLOGICAL CRITICALITY ANALYSIS (PREDICTION)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFormula: C_score(v) = 0.6·C_B^norm(v) + 0.4·AP(v)\")\n",
    "print(\"\\nTop 10 Components by Topological Criticality:\")\n",
    "print(topology_df[['name', 'type', 'composite_criticality', 'betweenness', \n",
    "                   'is_articulation_point']].head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\nArticulation Points: {topology_df['is_articulation_point'].sum()}\")\n",
    "aps = topology_df[topology_df['is_articulation_point']]['name'].tolist()\n",
    "if aps:\n",
    "    print(f\"  → {', '.join(aps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate Impact Scores for Validation\n",
    "\n",
    "Measure actual reachability loss to validate topological predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_reachable_pairs(graph: nx.DiGraph) -> int:\n",
    "    \"\"\"Count |R(G)| - total reachable vertex pairs\"\"\"\n",
    "    count = 0\n",
    "    for source in graph.nodes():\n",
    "        try:\n",
    "            count += len(nx.descendants(graph, source))\n",
    "        except:\n",
    "            continue\n",
    "    return count\n",
    "\n",
    "def calculate_impact_scores(graph: nx.DiGraph) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate impact scores: I(v) = 1 - |R(G-v)|/|R(G)|\n",
    "    \"\"\"\n",
    "    print(\"Computing impact scores (validation metric)...\")\n",
    "    original_reachable = count_reachable_pairs(graph)\n",
    "    \n",
    "    if original_reachable == 0:\n",
    "        return {node: 0.0 for node in graph.nodes()}\n",
    "    \n",
    "    impact_scores = {}\n",
    "    nodes = list(graph.nodes())\n",
    "    \n",
    "    for i, node in enumerate(nodes):\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"  Progress: {i+1}/{len(nodes)}\")\n",
    "        \n",
    "        # Remove node and calculate new reachability\n",
    "        test_graph = graph.copy()\n",
    "        test_graph.remove_node(node)\n",
    "        new_reachable = count_reachable_pairs(test_graph)\n",
    "        \n",
    "        # Calculate impact\n",
    "        impact_scores[node] = 1.0 - (new_reachable / original_reachable)\n",
    "    \n",
    "    return impact_scores\n",
    "\n",
    "# Calculate impact scores\n",
    "impact_scores = calculate_impact_scores(G)\n",
    "\n",
    "# Add to dataframe\n",
    "topology_df['impact_score'] = topology_df['node'].map(impact_scores)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPACT SCORE ANALYSIS (VALIDATION GROUND TRUTH)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFormula: I(v) = 1 - |R(G-v)|/|R(G)|\")\n",
    "print(\"\\nTop 10 Components by Actual Impact:\")\n",
    "impact_sorted = topology_df.sort_values('impact_score', ascending=False)\n",
    "print(impact_sorted[['name', 'type', 'impact_score', 'composite_criticality']]\n",
    "      .head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validation: Compare Prediction vs. Reality\n",
    "\n",
    "Validate that topological metrics predict actual impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_prediction(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Validate topology-based prediction against actual impact\n",
    "    \"\"\"\n",
    "    topology_scores = df['composite_criticality'].values\n",
    "    impact_scores = df['impact_score'].values\n",
    "    \n",
    "    # Correlations\n",
    "    pearson_r, pearson_p = pearsonr(topology_scores, impact_scores)\n",
    "    spearman_rho, spearman_p = spearmanr(topology_scores, impact_scores)\n",
    "    \n",
    "    # Top-k accuracy\n",
    "    k = min(5, len(df) // 4)\n",
    "    top_k_pred = set(df.nlargest(k, 'composite_criticality')['node'])\n",
    "    top_k_actual = set(df.nlargest(k, 'impact_score')['node'])\n",
    "    accuracy = len(top_k_pred & top_k_actual) / k if k > 0 else 0\n",
    "    \n",
    "    # Mean absolute error\n",
    "    mae = np.mean(np.abs(topology_scores - impact_scores))\n",
    "    \n",
    "    return {\n",
    "        'pearson_r': pearson_r,\n",
    "        'pearson_p': pearson_p,\n",
    "        'spearman_rho': spearman_rho,\n",
    "        'spearman_p': spearman_p,\n",
    "        'top_k_accuracy': accuracy,\n",
    "        'mae': mae,\n",
    "        'validation_passed': pearson_r > 0.5 or spearman_rho > 0.5\n",
    "    }\n",
    "\n",
    "# Perform validation\n",
    "validation = validate_prediction(topology_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. CORRELATION ANALYSIS\")\n",
    "print(f\"   Pearson correlation:  r = {validation['pearson_r']:.4f} (p = {validation['pearson_p']:.4f})\")\n",
    "print(f\"   Spearman correlation: ρ = {validation['spearman_rho']:.4f} (p = {validation['spearman_p']:.4f})\")\n",
    "\n",
    "if validation['pearson_r'] > 0.7:\n",
    "    strength = \"STRONG\"\n",
    "elif validation['pearson_r'] > 0.5:\n",
    "    strength = \"MODERATE\"\n",
    "elif validation['pearson_r'] > 0.3:\n",
    "    strength = \"WEAK\"\n",
    "else:\n",
    "    strength = \"VERY WEAK\"\n",
    "\n",
    "print(f\"\\n   → {strength} correlation\")\n",
    "\n",
    "print(\"\\n2. PREDICTION ACCURACY\")\n",
    "print(f\"   Top-5 overlap: {validation['top_k_accuracy']:.1%}\")\n",
    "print(f\"   Mean absolute error: {validation['mae']:.4f}\")\n",
    "\n",
    "print(\"\\n3. VALIDATION RESULT\")\n",
    "if validation['validation_passed']:\n",
    "    print(\"   ✓ VALIDATION PASSED\")\n",
    "    print(\"   → Topological metrics successfully predict actual impact\")\n",
    "    print(\"   → The approach is scientifically sound\")\n",
    "else:\n",
    "    print(\"   ✗ VALIDATION FAILED\")\n",
    "    print(\"   → Topological metrics may not fully capture actual impact\")\n",
    "    print(\"   → Consider adjusting weights or reviewing graph construction\")\n",
    "\n",
    "# Detailed comparison\n",
    "print(\"\\n4. PREDICTION vs. ACTUAL (Top 10)\")\n",
    "comparison = topology_df[['name', 'type', 'composite_criticality', 'impact_score']].copy()\n",
    "comparison['rank_by_topology'] = comparison['composite_criticality'].rank(ascending=False)\n",
    "comparison['rank_by_impact'] = comparison['impact_score'].rank(ascending=False)\n",
    "comparison['rank_difference'] = abs(comparison['rank_by_topology'] - comparison['rank_by_impact'])\n",
    "print(comparison.nsmallest(10, 'rank_by_topology')[['name', 'type', 'composite_criticality', \n",
    "                                                     'impact_score', 'rank_difference']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization: Prediction vs. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Scatter plot: Topology vs. Impact\n",
    "ax = axes[0, 0]\n",
    "colors = {'Application': '#3498db', 'Topic': '#2ecc71', \n",
    "          'Broker': '#e74c3c', 'Node': '#95a5a6'}\n",
    "for comp_type, color in colors.items():\n",
    "    mask = topology_df['type'] == comp_type\n",
    "    ax.scatter(topology_df[mask]['composite_criticality'],\n",
    "              topology_df[mask]['impact_score'],\n",
    "              c=color, label=comp_type, s=100, alpha=0.7, edgecolors='black')\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(topology_df['composite_criticality'], topology_df['impact_score'], 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(topology_df['composite_criticality'].min(), \n",
    "                     topology_df['composite_criticality'].max(), 100)\n",
    "ax.plot(x_line, p(x_line), \"r--\", linewidth=2, alpha=0.8,\n",
    "        label=f'Trend (r={validation[\"pearson_r\"]:.3f})')\n",
    "\n",
    "# Diagonal reference line (perfect prediction)\n",
    "max_val = max(topology_df['composite_criticality'].max(), topology_df['impact_score'].max())\n",
    "ax.plot([0, max_val], [0, max_val], 'k:', linewidth=1, alpha=0.5, label='Perfect prediction')\n",
    "\n",
    "ax.set_xlabel('Topological Criticality (Prediction)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Impact Score (Ground Truth)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Validation: Topology-Based Prediction vs. Actual Impact', \n",
    "            fontsize=13, fontweight='bold')\n",
    "ax.legend(loc='upper left', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Bar chart: Top 10 by both metrics\n",
    "ax = axes[0, 1]\n",
    "top_10 = topology_df.head(10)\n",
    "x = np.arange(len(top_10))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, top_10['composite_criticality'], width, \n",
    "       label='Topology Score', color='steelblue', alpha=0.8)\n",
    "ax.bar(x + width/2, top_10['impact_score'], width,\n",
    "       label='Impact Score', color='coral', alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(top_10['name'], rotation=45, ha='right')\n",
    "ax.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Top 10 Components: Predicted vs. Actual', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Rank comparison\n",
    "ax = axes[1, 0]\n",
    "comparison = topology_df.copy()\n",
    "comparison['topology_rank'] = comparison['composite_criticality'].rank(ascending=False, method='first')\n",
    "comparison['impact_rank'] = comparison['impact_score'].rank(ascending=False, method='first')\n",
    "comparison = comparison.sort_values('topology_rank')\n",
    "\n",
    "for i, row in comparison.head(10).iterrows():\n",
    "    ax.plot([1, 2], [row['topology_rank'], row['impact_rank']], \n",
    "           'o-', linewidth=2, markersize=8, alpha=0.7)\n",
    "    \n",
    "    # Annotate\n",
    "    ax.text(0.95, row['topology_rank'], row['name'], \n",
    "           ha='right', va='center', fontsize=8)\n",
    "    ax.text(2.05, row['impact_rank'], row['name'],\n",
    "           ha='left', va='center', fontsize=8)\n",
    "\n",
    "ax.set_xlim(0.5, 2.5)\n",
    "ax.set_ylim(0, len(comparison) + 1)\n",
    "ax.set_xticks([1, 2])\n",
    "ax.set_xticklabels(['Topology Rank', 'Impact Rank'], fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Rank (1 = Most Critical)', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Rank Comparison: Top 10 Components', fontsize=13, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Component type analysis\n",
    "ax = axes[1, 1]\n",
    "type_stats = topology_df.groupby('type').agg({\n",
    "    'composite_criticality': 'mean',\n",
    "    'impact_score': 'mean'\n",
    "}).sort_values('composite_criticality', ascending=False)\n",
    "\n",
    "x = np.arange(len(type_stats))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, type_stats['composite_criticality'], width,\n",
    "       label='Avg Topology Score', color='steelblue', alpha=0.8)\n",
    "ax.bar(x + width/2, type_stats['impact_score'], width,\n",
    "       label='Avg Impact Score', color='coral', alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(type_stats.index, fontsize=10)\n",
    "ax.set_ylabel('Average Score', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Average Scores by Component Type', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('topology_validation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Validation visualizations generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Topic Importance Context Analysis\n",
    "\n",
    "Show how QoS-based topic importance provides context without affecting criticality scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze topics specifically\n",
    "topics_df = topology_df[topology_df['type'] == 'Topic'].copy()\n",
    "\n",
    "if len(topics_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TOPIC IMPORTANCE CONTEXT ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nQoS importance provides CONTEXT but does NOT affect criticality scores.\")\n",
    "    print(\"\\nTopics Ranked by Criticality vs. QoS Importance:\")\n",
    "    \n",
    "    topics_comparison = topics_df[['name', 'composite_criticality', \n",
    "                                   'qos_importance', 'impact_score']].copy()\n",
    "    topics_comparison['crit_rank'] = topics_comparison['composite_criticality'].rank(ascending=False)\n",
    "    topics_comparison['qos_rank'] = topics_comparison['qos_importance'].rank(ascending=False)\n",
    "    topics_comparison['rank_diff'] = abs(topics_comparison['crit_rank'] - topics_comparison['qos_rank'])\n",
    "    \n",
    "    print(topics_comparison.sort_values('composite_criticality', ascending=False).to_string(index=False))\n",
    "    \n",
    "    print(\"\\nKey Insights:\")\n",
    "    print(f\"  • Correlation (Topology vs QoS): {topics_df['composite_criticality'].corr(topics_df['qos_importance']):.3f}\")\n",
    "    print(f\"  • Correlation (Impact vs QoS): {topics_df['impact_score'].corr(topics_df['qos_importance']):.3f}\")\n",
    "    \n",
    "    # Find mismatches\n",
    "    high_crit_low_qos = topics_comparison[\n",
    "        (topics_comparison['composite_criticality'] > topics_comparison['composite_criticality'].median()) &\n",
    "        (topics_comparison['qos_importance'] < topics_comparison['qos_importance'].median())\n",
    "    ]\n",
    "    \n",
    "    if len(high_crit_low_qos) > 0:\n",
    "        print(\"\\n  • Topics with HIGH criticality but LOW QoS importance:\")\n",
    "        for _, row in high_crit_low_qos.iterrows():\n",
    "            print(f\"    - {row['name']}: Structural bottleneck despite low QoS requirements\")\n",
    "    \n",
    "    low_crit_high_qos = topics_comparison[\n",
    "        (topics_comparison['composite_criticality'] < topics_comparison['composite_criticality'].median()) &\n",
    "        (topics_comparison['qos_importance'] > topics_comparison['qos_importance'].median())\n",
    "    ]\n",
    "    \n",
    "    if len(low_crit_high_qos) > 0:\n",
    "        print(\"\\n  • Topics with LOW criticality but HIGH QoS importance:\")\n",
    "        for _, row in low_crit_high_qos.iterrows():\n",
    "            print(f\"    - {row['name']}: Important business requirement but not structural bottleneck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. METHODOLOGY\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   Composite Criticality: C_score(v) = 0.6·C_B^norm(v) + 0.4·AP(v)\")\n",
    "print(\"   → Uses ONLY topological structure (betweenness + articulation points)\")\n",
    "print(\"   → Fast computation, suitable for real-time analysis\")\n",
    "print(\"\\n   Validation: I(v) = 1 - |R(G-v)|/|R(G)|\")\n",
    "print(\"   → Measures actual reachability loss (ground truth)\")\n",
    "print(\"   → Slower computation, used for periodic validation\")\n",
    "print(\"\\n   Topic Importance: QoS policies\")\n",
    "print(\"   → Provides business context for interpretation\")\n",
    "print(\"   → NOT used in criticality calculation\")\n",
    "\n",
    "print(\"\\n2. VALIDATION RESULTS\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   Correlation: {validation['pearson_r']:.3f} ({strength})\")\n",
    "print(f\"   Top-5 accuracy: {validation['top_k_accuracy']:.1%}\")\n",
    "print(f\"   Status: {'✓ PASSED' if validation['validation_passed'] else '✗ FAILED'}\")\n",
    "\n",
    "print(\"\\n3. KEY FINDINGS\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   • Most critical (topology): {topology_df.iloc[0]['name']} \"\n",
    "      f\"(score: {topology_df.iloc[0]['composite_criticality']:.4f})\")\n",
    "impact_sorted = topology_df.sort_values('impact_score', ascending=False)\n",
    "print(f\"   • Highest impact (actual): {impact_sorted.iloc[0]['name']} \"\n",
    "      f\"(score: {impact_sorted.iloc[0]['impact_score']:.4f})\")\n",
    "print(f\"   • Articulation points: {topology_df['is_articulation_point'].sum()}\")\n",
    "\n",
    "print(\"\\n4. INTERPRETATION\")\n",
    "print(\"-\" * 80)\n",
    "if validation['validation_passed']:\n",
    "    print(\"   ✓ The topological approach is VALIDATED\")\n",
    "    print(\"   → Pure topology successfully predicts actual impact\")\n",
    "    print(\"   → Betweenness centrality and articulation points are sufficient\")\n",
    "    print(\"   → Can use fast topology-based scoring for operations\")\n",
    "    print(\"   → Periodic impact validation confirms accuracy\")\n",
    "else:\n",
    "    print(\"   ⚠ Topological predictions show weak correlation with actual impact\")\n",
    "    print(\"   → May need to adjust weight parameters (α, β)\")\n",
    "    print(\"   → Consider graph construction methodology\")\n",
    "    print(\"   → Evaluate if additional topological metrics needed\")\n",
    "\n",
    "print(\"\\n5. ADVANTAGES OF THIS APPROACH\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   • Scientific rigor: Clear separation of prediction and validation\")\n",
    "print(\"   • Computational efficiency: Fast topology, slower validation\")\n",
    "print(\"   • Falsifiability: Correlation shows if approach is valid\")\n",
    "print(\"   • Interpretability: Pure structure easier to explain than mixed metrics\")\n",
    "print(\"   • Practical: Can use topology daily, validate weekly/monthly\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### **Does This Approach Make Sense? YES!**\n",
    "\n",
    "This refined methodology is **scientifically superior** to mixing impact scores into the composite calculation:\n",
    "\n",
    "#### **Why It's Better**\n",
    "\n",
    "1. **Separation of Concerns**: Prediction (topology) is distinct from validation (impact)\n",
    "2. **Computational Efficiency**: O(n) topology vs. O(n²) impact - use fast method operationally\n",
    "3. **Scientific Validity**: Can measure if topology actually predicts real impact\n",
    "4. **Falsifiable**: Low correlation reveals methodological problems\n",
    "5. **Practical Utility**: Daily topology scoring + periodic impact validation\n",
    "\n",
    "#### **Role of Each Metric**\n",
    "\n",
    "- **Betweenness Centrality + Articulation Points**: Core prediction (fast, structural)\n",
    "- **Impact Score**: Ground truth validation (slow, comprehensive)\n",
    "- **QoS Importance**: Business context (helps interpret results)\n",
    "\n",
    "#### **When to Use What**\n",
    "\n",
    "- **Real-time monitoring**: Topology-based criticality (fast)\n",
    "- **Weekly/monthly validation**: Impact score calculation (comprehensive)\n",
    "- **Decision making**: Consider topology score + QoS importance context\n",
    "- **Research**: Validate approach with correlation analysis\n",
    "\n",
    "This approach provides both **operational speed** and **scientific rigor**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
