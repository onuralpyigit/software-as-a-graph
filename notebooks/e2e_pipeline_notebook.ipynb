{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¬ End-to-End Pipeline: Graph-Based Modeling and Analysis of Distributed Publish-Subscribe Systems\n",
    "\n",
    "## A Comprehensive Five-Step Methodology\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Ibrahim Onuralp Yigit  \n",
    "**Research Domain:** Software Architecture Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This notebook implements a **complete end-to-end pipeline** for analyzing distributed publish-subscribe systems using graph-based modeling. The methodology transforms complex distributed architectures into analyzable graph structures, enabling proactive identification of critical components and system vulnerabilities.\n",
    "\n",
    "### Pipeline Overview\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     SOFTWARE-AS-A-GRAPH E2E PIPELINE                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚\n",
    "â”‚  â”‚   STEP 1    â”‚    â”‚   STEP 2    â”‚    â”‚   STEP 3    â”‚                     â”‚\n",
    "â”‚  â”‚  GENERATE   â”‚â”€â”€â”€â–¶â”‚   IMPORT    â”‚â”€â”€â”€â–¶â”‚   ANALYZE   â”‚                     â”‚\n",
    "â”‚  â”‚ Graph Data  â”‚    â”‚  to Neo4j   â”‚    â”‚ Criticality â”‚                     â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                     â”‚\n",
    "â”‚        â”‚                  â”‚                   â”‚                             â”‚\n",
    "â”‚        â”‚ JSON             â”‚ Cypher            â”‚ Scores                      â”‚\n",
    "â”‚        â–¼                  â–¼                   â–¼                             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚  â”‚                   DATA FLOW                          â”‚                   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚                                               â”‚                             â”‚\n",
    "â”‚                                               â–¼                             â”‚\n",
    "â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚\n",
    "â”‚                     â”‚   STEP 5    â”‚    â”‚   STEP 4    â”‚                     â”‚\n",
    "â”‚                     â”‚  VISUALIZE  â”‚â—€â”€â”€â”€â”‚  SIMULATE   â”‚                     â”‚\n",
    "â”‚                     â”‚   Results   â”‚    â”‚ & VALIDATE  â”‚                     â”‚\n",
    "â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\n",
    "â”‚                           â”‚                   â”‚                             â”‚\n",
    "â”‚                           â–¼                   â–¼                             â”‚\n",
    "â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚\n",
    "â”‚                     â”‚ Dashboard â”‚      â”‚ Metrics   â”‚                       â”‚\n",
    "â”‚                     â”‚ Reports   â”‚      â”‚ Validationâ”‚                       â”‚\n",
    "â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Research Target Metrics\n",
    "\n",
    "| Metric | Target | Description |\n",
    "|--------|--------|-------------|\n",
    "| **Spearman Ï** | â‰¥ 0.7 | Correlation between predicted criticality and actual failure impact |\n",
    "| **F1 Score** | â‰¥ 0.9 | Harmonic mean of precision and recall |\n",
    "| **Precision** | â‰¥ 0.9 | Correctly identified critical / Total identified as critical |\n",
    "| **Recall** | â‰¥ 0.85 | Correctly identified critical / Actual critical components |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Mathematical Foundation\n",
    "\n",
    "### Composite Criticality Scoring Formula\n",
    "\n",
    "The core innovation of this methodology is the **Composite Criticality Score**:\n",
    "\n",
    "$$\\Large \\boxed{C_{score}(v) = \\alpha \\cdot C_B^{norm}(v) + \\beta \\cdot AP(v) + \\gamma \\cdot I(v)}$$\n",
    "\n",
    "### Component Definitions\n",
    "\n",
    "| Symbol | Range | Definition | Calculation |\n",
    "|--------|-------|------------|-------------|\n",
    "| $C_B^{norm}(v)$ | [0, 1] | **Normalized Betweenness Centrality** | $\\frac{C_B(v)}{\\max_{u \\in V} C_B(u)}$ |\n",
    "| $AP(v)$ | {0, 1} | **Articulation Point Indicator** | 1 if removing $v$ disconnects graph |\n",
    "| $I(v)$ | [0, 1] | **Impact Score** | Reachability loss when $v$ fails |\n",
    "| $\\alpha, \\beta, \\gamma$ | [0, 1] | **Tunable Weights** | Must sum to 1.0 |\n",
    "\n",
    "### Default Weight Configuration\n",
    "\n",
    "```\n",
    "Î± = 0.4  â†’  Betweenness Centrality (information flow importance)\n",
    "Î² = 0.3  â†’  Articulation Point (structural importance)  \n",
    "Î³ = 0.3  â†’  Impact Score (failure consequence)\n",
    "```\n",
    "\n",
    "### Criticality Level Classification\n",
    "\n",
    "| Score Range | Level | Visual | Action Required |\n",
    "|-------------|-------|--------|----------------|\n",
    "| â‰¥ 0.8 | **CRITICAL** | ğŸ”´ | Immediate redundancy required |\n",
    "| â‰¥ 0.6 | **HIGH** | ğŸŸ  | Enhanced monitoring, plan redundancy |\n",
    "| â‰¥ 0.4 | **MEDIUM** | ğŸŸ¡ | Standard monitoring |\n",
    "| â‰¥ 0.2 | **LOW** | ğŸŸ¢ | Regular review |\n",
    "| < 0.2 | **MINIMAL** | âšª | No special attention |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“‹ Table of Contents\n",
    "\n",
    "1. [Setup and Dependencies](#1-setup-and-dependencies)\n",
    "2. [Configuration Classes](#2-configuration-classes)\n",
    "3. [Step 1: Graph Data Generation](#step-1-graph-data-generation)\n",
    "4. [Step 2: Neo4j Database Import](#step-2-neo4j-database-import)\n",
    "5. [Step 3: Graph Analysis & Criticality Scoring](#step-3-graph-analysis--criticality-scoring)\n",
    "6. [Step 4: Simulation and Validation](#step-4-simulation-and-validation)\n",
    "7. [Step 5: Multi-Layer Visualization](#step-5-multi-layer-visualization)\n",
    "8. [Complete Pipeline Execution](#complete-pipeline-execution)\n",
    "9. [Results Analysis](#results-analysis)\n",
    "10. [Conclusions and Recommendations](#conclusions-and-recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup and Dependencies <a id=\"1-setup-and-dependencies\"></a>\n",
    "\n",
    "First, let's import all required libraries and verify our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS AND ENVIRONMENT SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import statistics\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Set, Tuple\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# DEPENDENCY CHECKING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ğŸ”§ Checking Dependencies...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# NetworkX (Required)\n",
    "try:\n",
    "    import networkx as nx\n",
    "    HAS_NETWORKX = True\n",
    "    print(f\"âœ… NetworkX {nx.__version__} - Graph algorithms\")\n",
    "except ImportError:\n",
    "    HAS_NETWORKX = False\n",
    "    print(\"âŒ NetworkX - REQUIRED, install with: pip install networkx\")\n",
    "\n",
    "# SciPy (Optional - for statistical analysis)\n",
    "try:\n",
    "    from scipy import stats as scipy_stats\n",
    "    HAS_SCIPY = True\n",
    "    print(\"âœ… SciPy - Statistical analysis\")\n",
    "except ImportError:\n",
    "    HAS_SCIPY = False\n",
    "    print(\"âš ï¸  SciPy - Not available (using fallback calculations)\")\n",
    "\n",
    "# Neo4j (Optional - for database integration)\n",
    "try:\n",
    "    from neo4j import GraphDatabase\n",
    "    HAS_NEO4J = True\n",
    "    print(\"âœ… Neo4j Driver - Database integration\")\n",
    "except ImportError:\n",
    "    HAS_NEO4J = False\n",
    "    print(\"âš ï¸  Neo4j Driver - Not available (using in-memory analysis)\")\n",
    "\n",
    "# Matplotlib (Optional - for visualizations)\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as mpatches\n",
    "    HAS_MATPLOTLIB = True\n",
    "    print(\"âœ… Matplotlib - Visualizations\")\n",
    "except ImportError:\n",
    "    HAS_MATPLOTLIB = False\n",
    "    print(\"âš ï¸  Matplotlib - Not available (visualizations limited)\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸš€ Environment ready!\" if HAS_NETWORKX else \"âŒ Please install networkx to continue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Configuration Classes <a id=\"2-configuration-classes\"></a>\n",
    "\n",
    "Define the core data structures used throughout the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RESEARCH TARGET METRICS\n",
    "# ============================================================================\n",
    "\n",
    "TARGET_PRECISION = 0.9\n",
    "TARGET_RECALL = 0.85\n",
    "TARGET_F1_SCORE = 0.9\n",
    "TARGET_SPEARMAN_CORRELATION = 0.7\n",
    "\n",
    "print(\"ğŸ“Š Research Target Metrics:\")\n",
    "print(f\"   Precision    â‰¥ {TARGET_PRECISION}\")\n",
    "print(f\"   Recall       â‰¥ {TARGET_RECALL}\")\n",
    "print(f\"   F1 Score     â‰¥ {TARGET_F1_SCORE}\")\n",
    "print(f\"   Spearman Ï   â‰¥ {TARGET_SPEARMAN_CORRELATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENUMERATIONS\n",
    "# ============================================================================\n",
    "\n",
    "class Scenario(Enum):\n",
    "    \"\"\"Domain scenarios for graph generation\"\"\"\n",
    "    GENERIC = \"generic\"\n",
    "    IOT_SMART_CITY = \"iot\"\n",
    "    FINANCIAL_TRADING = \"financial\"\n",
    "    HEALTHCARE = \"healthcare\"\n",
    "    ECOMMERCE = \"ecommerce\"\n",
    "    AUTONOMOUS_VEHICLE = \"autonomous_vehicle\"\n",
    "    GAMING = \"gaming\"\n",
    "\n",
    "\n",
    "class CriticalityLevel(Enum):\n",
    "    \"\"\"Criticality levels for component classification\"\"\"\n",
    "    CRITICAL = \"CRITICAL\"   # Score >= 0.8\n",
    "    HIGH = \"HIGH\"           # Score >= 0.6\n",
    "    MEDIUM = \"MEDIUM\"       # Score >= 0.4\n",
    "    LOW = \"LOW\"             # Score >= 0.2\n",
    "    MINIMAL = \"MINIMAL\"     # Score < 0.2\n",
    "\n",
    "\n",
    "class AntiPattern(Enum):\n",
    "    \"\"\"Anti-patterns that can be injected for testing\"\"\"\n",
    "    SPOF = \"spof\"                    # Single Point of Failure\n",
    "    GOD_TOPIC = \"god_topic\"          # Topic with too many connections\n",
    "    CIRCULAR = \"circular\"            # Circular dependencies\n",
    "    BROKER_OVERLOAD = \"broker_overload\"  # Overloaded broker\n",
    "\n",
    "\n",
    "print(\"âœ… Enumerations defined:\")\n",
    "print(f\"   Scenarios: {[s.value for s in Scenario]}\")\n",
    "print(f\"   Criticality Levels: {[c.value for c in CriticalityLevel]}\")\n",
    "print(f\"   Anti-Patterns: {[a.value for a in AntiPattern]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA CLASSES\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Configuration for the E2E pipeline\"\"\"\n",
    "    # Graph generation\n",
    "    scenario: Scenario = Scenario.IOT_SMART_CITY\n",
    "    scale: str = \"small\"\n",
    "    seed: int = 42\n",
    "    antipatterns: List[str] = field(default_factory=list)\n",
    "    \n",
    "    # Neo4j connection (optional)\n",
    "    neo4j_uri: Optional[str] = None\n",
    "    neo4j_user: Optional[str] = None\n",
    "    neo4j_password: Optional[str] = None\n",
    "    neo4j_database: str = \"neo4j\"\n",
    "    \n",
    "    # Analysis parameters\n",
    "    alpha: float = 0.4  # Betweenness centrality weight\n",
    "    beta: float = 0.3   # Articulation point weight\n",
    "    gamma: float = 0.3  # Impact score weight\n",
    "    \n",
    "    # Simulation parameters\n",
    "    simulation_duration: int = 60  # seconds\n",
    "    failure_time: int = 30  # when to inject failure\n",
    "    message_rate: int = 10  # messages per second\n",
    "    enable_cascading: bool = True\n",
    "    \n",
    "    # Output\n",
    "    output_dir: Path = field(default_factory=lambda: Path(\"e2e_output\"))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CriticalityScore:\n",
    "    \"\"\"Composite criticality score for a component\"\"\"\n",
    "    component: str\n",
    "    component_type: str\n",
    "    composite_score: float\n",
    "    criticality_level: CriticalityLevel\n",
    "    betweenness_centrality_norm: float\n",
    "    is_articulation_point: bool\n",
    "    impact_score: float\n",
    "    degree: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ValidationResult:\n",
    "    \"\"\"Validation results comparing predictions to simulation outcomes\"\"\"\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1_score: float\n",
    "    spearman_correlation: float\n",
    "    targets_met: Dict[str, bool]\n",
    "    predicted_critical: Set[str]\n",
    "    actual_critical: Set[str]\n",
    "\n",
    "\n",
    "print(\"âœ… Data classes defined:\")\n",
    "print(\"   â€¢ PipelineConfig - Pipeline configuration\")\n",
    "print(\"   â€¢ CriticalityScore - Component criticality assessment\")\n",
    "print(\"   â€¢ ValidationResult - Validation metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Graph Data Generation <a id=\"step-1-graph-data-generation\"></a>\n",
    "\n",
    "### Overview\n",
    "\n",
    "The first step generates realistic pub-sub system topologies. The generator supports:\n",
    "\n",
    "- **6 Scale Presets**: From tiny (testing) to xlarge (enterprise)\n",
    "- **7 Domain Scenarios**: IoT, Financial, Healthcare, E-commerce, etc.\n",
    "- **4 Anti-pattern Types**: SPOFs, God Topics, Circular Dependencies\n",
    "\n",
    "### Multi-Layer Graph Model\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  APPLICATION LAYER                                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\n",
    "â”‚  â”‚ App_0   â”‚  â”‚ App_1   â”‚  â”‚ App_2   â”‚  â”‚ App_3   â”‚            â”‚\n",
    "â”‚  â”‚ ğŸ”µ      â”‚  â”‚ ğŸ”µ      â”‚  â”‚ ğŸ”µ      â”‚  â”‚ ğŸ”µ      â”‚            â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜            â”‚\n",
    "â”‚       â”‚ PUBLISHES   â”‚ SUBSCRIBES â”‚            â”‚                 â”‚\n",
    "â”‚       â–¼            â–¼            â–¼            â–¼                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  TOPIC LAYER                                                    â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚\n",
    "â”‚  â”‚Topic_0  â”‚  â”‚Topic_1  â”‚  â”‚Topic_2  â”‚                         â”‚\n",
    "â”‚  â”‚ ğŸŸ¢      â”‚  â”‚ ğŸŸ¢      â”‚  â”‚ ğŸŸ¢      â”‚                         â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                         â”‚\n",
    "â”‚       â”‚ ROUTES     â”‚            â”‚                               â”‚\n",
    "â”‚       â–¼            â–¼            â–¼                               â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  BROKER LAYER                                                   â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚  â”‚              Broker_0  ğŸ”´                â”‚                   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚                       â”‚ RUNS_ON                                 â”‚\n",
    "â”‚                       â–¼                                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  INFRASTRUCTURE LAYER                                           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚\n",
    "â”‚  â”‚ Node_0  â”‚  â”‚ Node_1  â”‚  â”‚ Node_2  â”‚                         â”‚\n",
    "â”‚  â”‚ ğŸŸ£      â”‚  â”‚ ğŸŸ£      â”‚  â”‚ ğŸŸ£      â”‚                         â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Relationship Types\n",
    "\n",
    "| Relationship | From | To | Description |\n",
    "|--------------|------|-----|-------------|\n",
    "| `PUBLISHES_TO` | Application | Topic | Application publishes messages |\n",
    "| `SUBSCRIBES_TO` | Application | Topic | Application subscribes to messages |\n",
    "| `ROUTES` | Broker | Topic | Broker routes topic messages |\n",
    "| `RUNS_ON` | App/Broker | Node | Component runs on infrastructure |\n",
    "| `DEPENDS_ON` | Application | Application | **Derived** dependency via shared topics |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: GRAPH DATA GENERATOR\n",
    "# ============================================================================\n",
    "\n",
    "class PubSubGraphGenerator:\n",
    "    \"\"\"\n",
    "    Generates realistic pub-sub system graphs for different scenarios.\n",
    "    \n",
    "    Features:\n",
    "    - Multiple scale presets (tiny to xlarge)\n",
    "    - Domain-specific application and topic naming\n",
    "    - QoS policy assignment\n",
    "    - Anti-pattern injection for testing\n",
    "    \"\"\"\n",
    "    \n",
    "    # Scale configurations\n",
    "    SCALES = {\n",
    "        'tiny': {'nodes': 2, 'apps': 6, 'topics': 4, 'brokers': 1},\n",
    "        'small': {'nodes': 4, 'apps': 12, 'topics': 8, 'brokers': 2},\n",
    "        'medium': {'nodes': 8, 'apps': 25, 'topics': 15, 'brokers': 3},\n",
    "        'large': {'nodes': 15, 'apps': 50, 'topics': 30, 'brokers': 5},\n",
    "        'xlarge': {'nodes': 30, 'apps': 100, 'topics': 60, 'brokers': 8}\n",
    "    }\n",
    "    \n",
    "    # Domain-specific application types\n",
    "    APP_TYPES = {\n",
    "        Scenario.GENERIC: ['Service', 'Processor', 'Handler', 'Monitor', 'Gateway'],\n",
    "        Scenario.IOT_SMART_CITY: [\n",
    "            'TrafficSensor', 'ParkingSensor', 'AirQualityMonitor', 'EmergencyDispatcher',\n",
    "            'LightingController', 'WasteManager', 'WeatherStation', 'TransitTracker'\n",
    "        ],\n",
    "        Scenario.FINANCIAL_TRADING: [\n",
    "            'MarketDataFeed', 'OrderProcessor', 'RiskEngine', 'TradeExecutor',\n",
    "            'PositionTracker', 'ComplianceMonitor', 'MatchingEngine', 'PricingService'\n",
    "        ],\n",
    "        Scenario.HEALTHCARE: [\n",
    "            'VitalSignsMonitor', 'PatientTracker', 'AlertDispatcher', 'MedicationManager',\n",
    "            'LabResultsProcessor', 'ImagingService', 'BillingService', 'AppointmentScheduler'\n",
    "        ],\n",
    "        Scenario.ECOMMERCE: [\n",
    "            'OrderService', 'InventoryManager', 'PaymentProcessor', 'ShippingCalculator',\n",
    "            'RecommendationEngine', 'CartService', 'NotificationService', 'FraudDetector'\n",
    "        ],\n",
    "        Scenario.AUTONOMOUS_VEHICLE: [\n",
    "            'LidarProcessor', 'CameraFusion', 'PathPlanner', 'MotionController',\n",
    "            'ObjectDetector', 'LocalizationService', 'V2XCommunicator', 'SafetyMonitor'\n",
    "        ],\n",
    "        Scenario.GAMING: [\n",
    "            'GameStateManager', 'PlayerController', 'PhysicsEngine', 'NetworkSync',\n",
    "            'MatchMaker', 'LeaderboardService', 'ChatService', 'AnalyticsCollector'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Domain-specific topic patterns\n",
    "    TOPIC_PATTERNS = {\n",
    "        Scenario.GENERIC: ['events', 'commands', 'queries', 'notifications', 'metrics'],\n",
    "        Scenario.IOT_SMART_CITY: [\n",
    "            'sensor/traffic', 'sensor/parking', 'sensor/air', 'alert/emergency',\n",
    "            'control/lighting', 'data/weather', 'status/transit', 'analytics/city'\n",
    "        ],\n",
    "        Scenario.FINANCIAL_TRADING: [\n",
    "            'market/quotes', 'market/trades', 'orders/new', 'orders/filled',\n",
    "            'risk/alerts', 'positions/updates', 'compliance/reports', 'pricing/updates'\n",
    "        ],\n",
    "        Scenario.HEALTHCARE: [\n",
    "            'patient/vitals', 'patient/alerts', 'lab/results', 'medication/orders',\n",
    "            'imaging/results', 'appointments/schedule', 'billing/events', 'staff/notifications'\n",
    "        ],\n",
    "        Scenario.ECOMMERCE: [\n",
    "            'orders/created', 'orders/fulfilled', 'inventory/updates', 'payments/processed',\n",
    "            'shipping/tracking', 'recommendations/generated', 'cart/updated', 'fraud/alerts'\n",
    "        ],\n",
    "        Scenario.AUTONOMOUS_VEHICLE: [\n",
    "            'sensor/lidar', 'sensor/camera', 'perception/objects', 'planning/path',\n",
    "            'control/commands', 'localization/pose', 'v2x/messages', 'safety/alerts'\n",
    "        ],\n",
    "        Scenario.GAMING: [\n",
    "            'game/state', 'player/actions', 'physics/updates', 'network/sync',\n",
    "            'match/events', 'social/chat', 'analytics/events', 'leaderboard/updates'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # QoS profiles\n",
    "    QOS_PROFILES = {\n",
    "        'critical': {'reliability': 'reliable', 'durability': 'transient_local', 'deadline_ms': 10},\n",
    "        'high': {'reliability': 'reliable', 'durability': 'volatile', 'deadline_ms': 50},\n",
    "        'medium': {'reliability': 'best_effort', 'durability': 'volatile', 'deadline_ms': 100},\n",
    "        'low': {'reliability': 'best_effort', 'durability': 'volatile', 'deadline_ms': 500}\n",
    "    }\n",
    "    \n",
    "    def __init__(self, scenario: Scenario = Scenario.GENERIC, seed: int = 42):\n",
    "        self.scenario = scenario\n",
    "        self.seed = seed\n",
    "        random.seed(seed)\n",
    "    \n",
    "    def generate(self, scale: str = 'small', antipatterns: List[str] = None) -> Dict:\n",
    "        \"\"\"Generate a complete pub-sub system graph\"\"\"\n",
    "        if scale not in self.SCALES:\n",
    "            raise ValueError(f\"Invalid scale: {scale}. Choose from {list(self.SCALES.keys())}\")\n",
    "        \n",
    "        params = self.SCALES[scale]\n",
    "        antipatterns = antipatterns or []\n",
    "        \n",
    "        # Generate components\n",
    "        nodes = self._generate_nodes(params['nodes'])\n",
    "        brokers = self._generate_brokers(params['brokers'], nodes)\n",
    "        topics = self._generate_topics(params['topics'])\n",
    "        applications = self._generate_applications(params['apps'], nodes)\n",
    "        \n",
    "        # Generate relationships\n",
    "        relationships = self._generate_relationships(applications, topics, brokers)\n",
    "        \n",
    "        # Inject anti-patterns if requested\n",
    "        if antipatterns:\n",
    "            self._inject_antipatterns(applications, topics, brokers, relationships, antipatterns)\n",
    "        \n",
    "        return {\n",
    "            'metadata': {\n",
    "                'scenario': self.scenario.value,\n",
    "                'scale': scale,\n",
    "                'seed': self.seed,\n",
    "                'generated_at': datetime.now().isoformat(),\n",
    "                'antipatterns': antipatterns\n",
    "            },\n",
    "            'nodes': nodes,\n",
    "            'brokers': brokers,\n",
    "            'topics': topics,\n",
    "            'applications': applications,\n",
    "            'relationships': relationships\n",
    "        }\n",
    "    \n",
    "    def _generate_nodes(self, count: int) -> List[Dict]:\n",
    "        \"\"\"Generate infrastructure nodes\"\"\"\n",
    "        return [{\n",
    "            'id': f'node_{i}',\n",
    "            'name': f'InfraNode-{i}',\n",
    "            'type': 'Node',\n",
    "            'cpu_cores': random.choice([4, 8, 16, 32]),\n",
    "            'memory_gb': random.choice([16, 32, 64, 128]),\n",
    "            'region': random.choice(['us-east', 'us-west', 'eu-west', 'ap-south'])\n",
    "        } for i in range(count)]\n",
    "    \n",
    "    def _generate_brokers(self, count: int, nodes: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Generate message brokers\"\"\"\n",
    "        return [{\n",
    "            'id': f'broker_{i}',\n",
    "            'name': f'Broker-{i}',\n",
    "            'type': 'Broker',\n",
    "            'host_node': random.choice(nodes)['id'],\n",
    "            'max_connections': random.choice([100, 500, 1000]),\n",
    "            'protocol': random.choice(['MQTT', 'AMQP', 'DDS'])\n",
    "        } for i in range(count)]\n",
    "    \n",
    "    def _generate_topics(self, count: int) -> List[Dict]:\n",
    "        \"\"\"Generate topics with QoS policies\"\"\"\n",
    "        patterns = self.TOPIC_PATTERNS.get(self.scenario, self.TOPIC_PATTERNS[Scenario.GENERIC])\n",
    "        return [{\n",
    "            'id': f'topic_{i}',\n",
    "            'name': f'{patterns[i % len(patterns)]}/{i}',\n",
    "            'type': 'Topic',\n",
    "            'qos': self.QOS_PROFILES[random.choice(['critical', 'high', 'medium', 'low'])].copy(),\n",
    "            'message_type': f'msg_type_{i % 5}'\n",
    "        } for i in range(count)]\n",
    "    \n",
    "    def _generate_applications(self, count: int, nodes: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Generate applications\"\"\"\n",
    "        app_types = self.APP_TYPES.get(self.scenario, self.APP_TYPES[Scenario.GENERIC])\n",
    "        return [{\n",
    "            'id': f'app_{i}',\n",
    "            'name': f'{app_types[i % len(app_types)]}-{i}',\n",
    "            'type': 'Application',\n",
    "            'app_type': app_types[i % len(app_types)],\n",
    "            'host_node': random.choice(nodes)['id'],\n",
    "            'replicas': random.choice([1, 2, 3]),\n",
    "            'criticality': random.choice(['high', 'medium', 'low'])\n",
    "        } for i in range(count)]\n",
    "    \n",
    "    def _generate_relationships(self, applications: List[Dict], \n",
    "                               topics: List[Dict], \n",
    "                               brokers: List[Dict]) -> Dict:\n",
    "        \"\"\"Generate relationships between components\"\"\"\n",
    "        relationships = {\n",
    "            'runs_on': [],\n",
    "            'publishes_to': [],\n",
    "            'subscribes_to': [],\n",
    "            'routes': []\n",
    "        }\n",
    "        \n",
    "        # Apps and brokers run on nodes\n",
    "        for app in applications:\n",
    "            relationships['runs_on'].append({'from': app['id'], 'to': app['host_node']})\n",
    "        \n",
    "        for broker in brokers:\n",
    "            relationships['runs_on'].append({'from': broker['id'], 'to': broker['host_node']})\n",
    "        \n",
    "        # Each app publishes to 1-3 topics and subscribes to 1-4 topics\n",
    "        for app in applications:\n",
    "            pub_count = random.randint(1, min(3, len(topics)))\n",
    "            sub_count = random.randint(1, min(4, len(topics)))\n",
    "            \n",
    "            pub_topics = random.sample(topics, pub_count)\n",
    "            sub_topics = random.sample(topics, sub_count)\n",
    "            \n",
    "            for topic in pub_topics:\n",
    "                relationships['publishes_to'].append({'from': app['id'], 'to': topic['id']})\n",
    "            \n",
    "            for topic in sub_topics:\n",
    "                relationships['subscribes_to'].append({'from': app['id'], 'to': topic['id']})\n",
    "        \n",
    "        # Brokers route topics\n",
    "        for topic in topics:\n",
    "            broker = random.choice(brokers)\n",
    "            relationships['routes'].append({'from': broker['id'], 'to': topic['id']})\n",
    "        \n",
    "        return relationships\n",
    "    \n",
    "    def _inject_antipatterns(self, applications: List[Dict], \n",
    "                            topics: List[Dict],\n",
    "                            brokers: List[Dict],\n",
    "                            relationships: Dict,\n",
    "                            antipatterns: List[str]):\n",
    "        \"\"\"Inject anti-patterns into the graph\"\"\"\n",
    "        for pattern in antipatterns:\n",
    "            if pattern == 'spof':\n",
    "                self._inject_spof(applications, topics, relationships)\n",
    "            elif pattern == 'god_topic':\n",
    "                self._inject_god_topic(applications, topics, relationships)\n",
    "            elif pattern == 'circular':\n",
    "                self._inject_circular(applications, topics, relationships)\n",
    "    \n",
    "    def _inject_spof(self, applications: List[Dict], topics: List[Dict], relationships: Dict):\n",
    "        \"\"\"Inject Single Point of Failure anti-pattern\"\"\"\n",
    "        spof_app = {\n",
    "            'id': 'spof_critical_service',\n",
    "            'name': 'CriticalService-SPOF',\n",
    "            'type': 'Application',\n",
    "            'app_type': 'CriticalService',\n",
    "            'host_node': applications[0]['host_node'],\n",
    "            'replicas': 1,\n",
    "            'criticality': 'critical',\n",
    "            'is_spof': True\n",
    "        }\n",
    "        applications.append(spof_app)\n",
    "        \n",
    "        spof_topic = {\n",
    "            'id': 'topic_spof',\n",
    "            'name': 'critical/spof',\n",
    "            'type': 'Topic',\n",
    "            'qos': self.QOS_PROFILES['critical'],\n",
    "            'message_type': 'critical_msg'\n",
    "        }\n",
    "        topics.append(spof_topic)\n",
    "        \n",
    "        # SPOF publishes, many apps subscribe\n",
    "        relationships['publishes_to'].append({'from': spof_app['id'], 'to': spof_topic['id']})\n",
    "        \n",
    "        for app in applications[:min(8, len(applications))]:\n",
    "            if app['id'] != spof_app['id']:\n",
    "                relationships['subscribes_to'].append({'from': app['id'], 'to': spof_topic['id']})\n",
    "    \n",
    "    def _inject_god_topic(self, applications: List[Dict], topics: List[Dict], relationships: Dict):\n",
    "        \"\"\"Inject God Topic anti-pattern\"\"\"\n",
    "        god_topic = {\n",
    "            'id': 'topic_god',\n",
    "            'name': 'god/everything',\n",
    "            'type': 'Topic',\n",
    "            'qos': self.QOS_PROFILES['medium'],\n",
    "            'message_type': 'any_msg',\n",
    "            'is_god_topic': True\n",
    "        }\n",
    "        topics.append(god_topic)\n",
    "        \n",
    "        # Many apps both publish and subscribe\n",
    "        for app in applications[:min(10, len(applications))]:\n",
    "            relationships['publishes_to'].append({'from': app['id'], 'to': god_topic['id']})\n",
    "            relationships['subscribes_to'].append({'from': app['id'], 'to': god_topic['id']})\n",
    "    \n",
    "    def _inject_circular(self, applications: List[Dict], topics: List[Dict], relationships: Dict):\n",
    "        \"\"\"Inject circular dependencies\"\"\"\n",
    "        if len(applications) < 3:\n",
    "            return\n",
    "        \n",
    "        # Create circular topic chain\n",
    "        circular_topics = []\n",
    "        for i in range(3):\n",
    "            topic = {\n",
    "                'id': f'topic_circular_{i}',\n",
    "                'name': f'circular/chain/{i}',\n",
    "                'type': 'Topic',\n",
    "                'qos': self.QOS_PROFILES['medium'],\n",
    "                'message_type': 'circular_msg',\n",
    "                'is_circular': True\n",
    "            }\n",
    "            circular_topics.append(topic)\n",
    "            topics.append(topic)\n",
    "        \n",
    "        # Create circular dependency: app_0 -> topic_0 -> app_1 -> topic_1 -> app_2 -> topic_2 -> app_0\n",
    "        for i in range(3):\n",
    "            pub_app = applications[i]\n",
    "            sub_app = applications[(i + 1) % 3]\n",
    "            topic = circular_topics[i]\n",
    "            \n",
    "            relationships['publishes_to'].append({'from': pub_app['id'], 'to': topic['id']})\n",
    "            relationships['subscribes_to'].append({'from': sub_app['id'], 'to': topic['id']})\n",
    "\n",
    "\n",
    "print(\"âœ… PubSubGraphGenerator class defined\")\n",
    "print(f\"   Supported scales: {list(PubSubGraphGenerator.SCALES.keys())}\")\n",
    "print(f\"   Supported scenarios: {len(PubSubGraphGenerator.APP_TYPES)} domains\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXECUTE STEP 1: Generate Graph Data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  ğŸ“Š STEP 1: GRAPH DATA GENERATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuration\n",
    "SCENARIO = Scenario.IOT_SMART_CITY\n",
    "SCALE = 'small'\n",
    "SEED = 42\n",
    "ANTIPATTERNS = ['spof']\n",
    "\n",
    "print(f\"\\nğŸ”§ Configuration:\")\n",
    "print(f\"   Scenario:      {SCENARIO.value}\")\n",
    "print(f\"   Scale:         {SCALE}\")\n",
    "print(f\"   Seed:          {SEED}\")\n",
    "print(f\"   Anti-patterns: {ANTIPATTERNS}\")\n",
    "\n",
    "# Generate\n",
    "generator = PubSubGraphGenerator(scenario=SCENARIO, seed=SEED)\n",
    "graph_data = generator.generate(scale=SCALE, antipatterns=ANTIPATTERNS)\n",
    "\n",
    "print(f\"\\nâœ… Generation Complete:\")\n",
    "print(f\"   Infrastructure Nodes: {len(graph_data['nodes'])}\")\n",
    "print(f\"   Brokers:              {len(graph_data['brokers'])}\")\n",
    "print(f\"   Topics:               {len(graph_data['topics'])}\")\n",
    "print(f\"   Applications:         {len(graph_data['applications'])}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Relationships:\")\n",
    "for rel_type, rels in graph_data['relationships'].items():\n",
    "    print(f\"   {rel_type.upper():<15} {len(rels):>4} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Neo4j Database Import <a id=\"step-2-neo4j-database-import\"></a>\n",
    "\n",
    "### Overview\n",
    "\n",
    "The second step imports the generated graph data into **Neo4j** for persistent storage and advanced queries. This step is optional - the pipeline can work entirely in-memory.\n",
    "\n",
    "### Benefits of Neo4j Integration\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|--------|\n",
    "| **Persistent Storage** | Graph data survives between sessions |\n",
    "| **Cypher Queries** | Powerful graph query language |\n",
    "| **Built-in Visualization** | Neo4j Browser for exploration |\n",
    "| **Scalability** | Handle large enterprise graphs |\n",
    "| **Advanced Analytics** | GDS library for graph algorithms |\n",
    "\n",
    "### Key Operation: DEPENDS_ON Derivation\n",
    "\n",
    "```cypher\n",
    "// Derive DEPENDS_ON relationships from pub/sub patterns\n",
    "MATCH (subscriber:Application)-[:SUBSCRIBES_TO]->(topic:Topic)<-[:PUBLISHES_TO]-(publisher:Application)\n",
    "WHERE subscriber <> publisher\n",
    "MERGE (subscriber)-[r:DEPENDS_ON]->(publisher)\n",
    "SET r.via_topics = collect(topic.id)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: NEO4J IMPORTER\n",
    "# ============================================================================\n",
    "\n",
    "class Neo4jImporter:\n",
    "    \"\"\"\n",
    "    Imports graph data into Neo4j database.\n",
    "    \n",
    "    Key Features:\n",
    "    - Creates unique constraints\n",
    "    - Imports all node types and relationships\n",
    "    - Derives DEPENDS_ON relationships from pub/sub patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, uri: str, user: str, password: str, database: str = \"neo4j\"):\n",
    "        if not HAS_NEO4J:\n",
    "            raise ImportError(\"neo4j driver is required\")\n",
    "        self.uri = uri\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.database = database\n",
    "        self.driver = None\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to Neo4j\"\"\"\n",
    "        self.driver = GraphDatabase.driver(self.uri, auth=(self.user, self.password))\n",
    "        with self.driver.session(database=self.database) as session:\n",
    "            session.run(\"RETURN 1\")  # Test connection\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close connection\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.close()\n",
    "    \n",
    "    def import_graph(self, graph_data: Dict, clear_first: bool = True) -> Dict:\n",
    "        \"\"\"Import graph data into Neo4j\"\"\"\n",
    "        stats = {'nodes': 0, 'relationships': 0}\n",
    "        \n",
    "        with self.driver.session(database=self.database) as session:\n",
    "            if clear_first:\n",
    "                session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "            \n",
    "            # Import nodes\n",
    "            for node in graph_data.get('nodes', []):\n",
    "                session.run(\"CREATE (n:Node $props)\", props=node)\n",
    "                stats['nodes'] += 1\n",
    "            \n",
    "            for broker in graph_data.get('brokers', []):\n",
    "                session.run(\"CREATE (b:Broker $props)\", props=broker)\n",
    "                stats['nodes'] += 1\n",
    "            \n",
    "            for app in graph_data.get('applications', []):\n",
    "                session.run(\"CREATE (a:Application $props)\", props=app)\n",
    "                stats['nodes'] += 1\n",
    "            \n",
    "            for topic in graph_data.get('topics', []):\n",
    "                props = {k: v for k, v in topic.items() if k != 'qos'}\n",
    "                if 'qos' in topic:\n",
    "                    for qk, qv in topic['qos'].items():\n",
    "                        props[f'qos_{qk}'] = qv\n",
    "                session.run(\"CREATE (t:Topic $props)\", props=props)\n",
    "                stats['nodes'] += 1\n",
    "            \n",
    "            # Import relationships\n",
    "            relationships = graph_data.get('relationships', {})\n",
    "            for rel_type, rels in relationships.items():\n",
    "                cypher_type = rel_type.upper()\n",
    "                for rel in rels:\n",
    "                    session.run(f\"\"\"\n",
    "                        MATCH (a {{id: $from_id}})\n",
    "                        MATCH (b {{id: $to_id}})\n",
    "                        CREATE (a)-[:{cypher_type}]->(b)\n",
    "                    \"\"\", from_id=rel['from'], to_id=rel['to'])\n",
    "                    stats['relationships'] += 1\n",
    "            \n",
    "            # Derive DEPENDS_ON\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (sub:Application)-[:SUBSCRIBES_TO]->(t:Topic)<-[:PUBLISHES_TO]-(pub:Application)\n",
    "                WHERE sub <> pub\n",
    "                WITH sub, pub, collect(t.id) as topics\n",
    "                MERGE (sub)-[r:DEPENDS_ON]->(pub)\n",
    "                SET r.via_topics = topics\n",
    "                RETURN count(r) as count\n",
    "            \"\"\")\n",
    "            stats['depends_on'] = result.single()['count']\n",
    "        \n",
    "        return stats\n",
    "\n",
    "\n",
    "print(\"âœ… Neo4jImporter class defined\")\n",
    "print(f\"   Neo4j available: {HAS_NEO4J}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXECUTE STEP 2: Neo4j Import (Optional)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  ğŸ“Š STEP 2: NEO4J DATABASE IMPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Neo4j configuration (uncomment and configure if you have Neo4j)\n",
    "# NEO4J_URI = \"bolt://localhost:7687\"\n",
    "# NEO4J_USER = \"neo4j\"\n",
    "# NEO4J_PASSWORD = \"your_password\"\n",
    "\n",
    "if HAS_NEO4J:\n",
    "    print(\"\\nâš ï¸  Neo4j driver available but connection not configured.\")\n",
    "    print(\"    To enable Neo4j import, configure:\")\n",
    "    print(\"    NEO4J_URI = 'bolt://localhost:7687'\")\n",
    "    print(\"    NEO4J_USER = 'neo4j'\")\n",
    "    print(\"    NEO4J_PASSWORD = 'your_password'\")\n",
    "    print(\"\\n    Continuing with in-memory NetworkX analysis...\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Neo4j driver not available.\")\n",
    "    print(\"    Install with: pip install neo4j\")\n",
    "    print(\"\\n    Continuing with in-memory NetworkX analysis...\")\n",
    "\n",
    "print(\"\\nâœ… Step 2: Skipped (using in-memory analysis)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Graph Analysis & Criticality Scoring <a id=\"step-3-graph-analysis--criticality-scoring\"></a>\n",
    "\n",
    "### Overview\n",
    "\n",
    "The third step performs comprehensive analysis on the graph, implementing the core **Composite Criticality Scoring** algorithm.\n",
    "\n",
    "### Analysis Components\n",
    "\n",
    "| Component | Description | Output |\n",
    "|-----------|-------------|--------|\n",
    "| **Graph Summary** | Node/edge counts, density, connectivity | Statistics |\n",
    "| **Centrality Metrics** | Betweenness, degree, closeness, PageRank | Per-node scores |\n",
    "| **Structural Analysis** | Articulation points, bridges, cycles | Vulnerability lists |\n",
    "| **Criticality Scoring** | Composite score calculation | Component rankings |\n",
    "| **Anti-pattern Detection** | SPOFs, God Topics, Circular Dependencies | Issue reports |\n",
    "\n",
    "### Algorithm: Criticality Score Calculation\n",
    "\n",
    "```python\n",
    "# For each node v in graph G:\n",
    "C_B_norm = betweenness_centrality(v) / max_betweenness\n",
    "AP = 1 if v in articulation_points(G) else 0\n",
    "I = calculate_impact_score(G, v)  # Reachability loss\n",
    "\n",
    "C_score = Î± * C_B_norm + Î² * AP + Î³ * I\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: GRAPH ANALYZER\n",
    "# ============================================================================\n",
    "\n",
    "class GraphAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive graph analysis implementing the criticality scoring model.\n",
    "    \n",
    "    Criticality Formula:\n",
    "    C_score(v) = Î±Â·C_B^norm(v) + Î²Â·AP(v) + Î³Â·I(v)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float = 0.4, beta: float = 0.3, gamma: float = 0.3):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Validate weights sum to 1.0\n",
    "        assert abs(alpha + beta + gamma - 1.0) < 0.001, \"Weights must sum to 1.0\"\n",
    "    \n",
    "    def build_networkx_graph(self, graph_data: Dict) -> nx.DiGraph:\n",
    "        \"\"\"Build NetworkX directed graph from data\"\"\"\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes by type\n",
    "        for node in graph_data.get('nodes', []):\n",
    "            attrs = {k: v for k, v in node.items() if k not in ['id', 'type']}\n",
    "            G.add_node(node['id'], type='Node', layer='infrastructure', **attrs)\n",
    "        \n",
    "        for broker in graph_data.get('brokers', []):\n",
    "            attrs = {k: v for k, v in broker.items() if k not in ['id', 'type']}\n",
    "            G.add_node(broker['id'], type='Broker', layer='broker', **attrs)\n",
    "        \n",
    "        for app in graph_data.get('applications', []):\n",
    "            attrs = {k: v for k, v in app.items() if k not in ['id', 'type']}\n",
    "            G.add_node(app['id'], type='Application', layer='application', **attrs)\n",
    "        \n",
    "        for topic in graph_data.get('topics', []):\n",
    "            attrs = {k: v for k, v in topic.items() if k not in ['id', 'type', 'qos']}\n",
    "            if 'qos' in topic:\n",
    "                for qk, qv in topic['qos'].items():\n",
    "                    attrs[f'qos_{qk}'] = qv\n",
    "            G.add_node(topic['id'], type='Topic', layer='topic', **attrs)\n",
    "        \n",
    "        # Add edges\n",
    "        relationships = graph_data.get('relationships', {})\n",
    "        for rel_type, rels in relationships.items():\n",
    "            for rel in rels:\n",
    "                G.add_edge(rel['from'], rel['to'], type=rel_type.upper())\n",
    "        \n",
    "        # Derive DEPENDS_ON relationships\n",
    "        G = self._derive_depends_on(G)\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def _derive_depends_on(self, G: nx.DiGraph) -> nx.DiGraph:\n",
    "        \"\"\"Derive DEPENDS_ON relationships from pub/sub patterns\"\"\"\n",
    "        topics = [n for n, d in G.nodes(data=True) if d.get('type') == 'Topic']\n",
    "        \n",
    "        for topic in topics:\n",
    "            # Find publishers and subscribers\n",
    "            publishers = [s for s, _, d in G.in_edges(topic, data=True)\n",
    "                         if d.get('type') == 'PUBLISHES_TO']\n",
    "            subscribers = [s for s, _, d in G.in_edges(topic, data=True)\n",
    "                          if d.get('type') == 'SUBSCRIBES_TO']\n",
    "            \n",
    "            # Create DEPENDS_ON: subscriber depends on publisher\n",
    "            for sub in subscribers:\n",
    "                for pub in publishers:\n",
    "                    if sub != pub and not G.has_edge(sub, pub):\n",
    "                        G.add_edge(sub, pub, type='DEPENDS_ON', via_topic=topic)\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def analyze(self, G: nx.DiGraph) -> Dict:\n",
    "        \"\"\"Run comprehensive analysis\"\"\"\n",
    "        return {\n",
    "            'graph_summary': self._graph_summary(G),\n",
    "            'structural_analysis': self._structural_analysis(G),\n",
    "            'criticality_scores': self._calculate_criticality(G),\n",
    "            'layer_analysis': self._analyze_layers(G),\n",
    "            'antipattern_detection': self._detect_antipatterns(G)\n",
    "        }\n",
    "    \n",
    "    def _graph_summary(self, G: nx.DiGraph) -> Dict:\n",
    "        \"\"\"Generate graph summary statistics\"\"\"\n",
    "        node_types = defaultdict(int)\n",
    "        edge_types = defaultdict(int)\n",
    "        \n",
    "        for _, data in G.nodes(data=True):\n",
    "            node_types[data.get('type', 'Unknown')] += 1\n",
    "        \n",
    "        for _, _, data in G.edges(data=True):\n",
    "            edge_types[data.get('type', 'Unknown')] += 1\n",
    "        \n",
    "        return {\n",
    "            'total_nodes': G.number_of_nodes(),\n",
    "            'total_edges': G.number_of_edges(),\n",
    "            'density': nx.density(G),\n",
    "            'is_connected': nx.is_weakly_connected(G),\n",
    "            'num_components': nx.number_weakly_connected_components(G),\n",
    "            'node_types': dict(node_types),\n",
    "            'edge_types': dict(edge_types)\n",
    "        }\n",
    "    \n",
    "    def _structural_analysis(self, G: nx.DiGraph) -> Dict:\n",
    "        \"\"\"Perform structural analysis\"\"\"\n",
    "        undirected = G.to_undirected()\n",
    "        \n",
    "        # Find articulation points and bridges\n",
    "        aps = list(nx.articulation_points(undirected))\n",
    "        bridges = list(nx.bridges(undirected))\n",
    "        \n",
    "        # Cycle detection\n",
    "        try:\n",
    "            cycles = list(nx.simple_cycles(G))\n",
    "            has_cycles = len(cycles) > 0\n",
    "            num_cycles = min(len(cycles), 100)\n",
    "        except:\n",
    "            has_cycles = False\n",
    "            num_cycles = 0\n",
    "        \n",
    "        return {\n",
    "            'articulation_points': aps,\n",
    "            'num_articulation_points': len(aps),\n",
    "            'bridges': bridges,\n",
    "            'num_bridges': len(bridges),\n",
    "            'has_cycles': has_cycles,\n",
    "            'num_cycles': num_cycles\n",
    "        }\n",
    "    \n",
    "    def _calculate_criticality(self, G: nx.DiGraph) -> Dict[str, CriticalityScore]:\n",
    "        \"\"\"\n",
    "        Calculate criticality scores using the composite formula:\n",
    "        C_score(v) = Î±Â·C_B^norm(v) + Î²Â·AP(v) + Î³Â·I(v)\n",
    "        \"\"\"\n",
    "        # Calculate betweenness centrality\n",
    "        betweenness = nx.betweenness_centrality(G)\n",
    "        max_bc = max(betweenness.values()) if betweenness else 1.0\n",
    "        if max_bc == 0:\n",
    "            max_bc = 1.0\n",
    "        \n",
    "        # Find articulation points\n",
    "        undirected = G.to_undirected()\n",
    "        aps = set(nx.articulation_points(undirected))\n",
    "        \n",
    "        scores = {}\n",
    "        for node in G.nodes():\n",
    "            node_data = G.nodes[node]\n",
    "            \n",
    "            # C_B^norm(v): Normalized betweenness centrality\n",
    "            bc_norm = betweenness.get(node, 0) / max_bc\n",
    "            \n",
    "            # AP(v): Articulation point indicator\n",
    "            is_ap = node in aps\n",
    "            ap_value = 1.0 if is_ap else 0.0\n",
    "            \n",
    "            # I(v): Impact score\n",
    "            impact = self._calculate_impact_score(G, node)\n",
    "            \n",
    "            # C_score(v) = Î±Â·C_B^norm(v) + Î²Â·AP(v) + Î³Â·I(v)\n",
    "            composite = (self.alpha * bc_norm + \n",
    "                        self.beta * ap_value + \n",
    "                        self.gamma * impact)\n",
    "            \n",
    "            # Classify criticality level\n",
    "            if composite >= 0.8:\n",
    "                level = CriticalityLevel.CRITICAL\n",
    "            elif composite >= 0.6:\n",
    "                level = CriticalityLevel.HIGH\n",
    "            elif composite >= 0.4:\n",
    "                level = CriticalityLevel.MEDIUM\n",
    "            elif composite >= 0.2:\n",
    "                level = CriticalityLevel.LOW\n",
    "            else:\n",
    "                level = CriticalityLevel.MINIMAL\n",
    "            \n",
    "            scores[node] = CriticalityScore(\n",
    "                component=node,\n",
    "                component_type=node_data.get('type', 'Unknown'),\n",
    "                composite_score=composite,\n",
    "                criticality_level=level,\n",
    "                betweenness_centrality_norm=bc_norm,\n",
    "                is_articulation_point=is_ap,\n",
    "                impact_score=impact,\n",
    "                degree=G.degree(node)\n",
    "            )\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def _calculate_impact_score(self, G: nx.DiGraph, node: str) -> float:\n",
    "        \"\"\"Calculate impact score based on reachability loss\"\"\"\n",
    "        if G.number_of_nodes() <= 1:\n",
    "            return 0.0\n",
    "        \n",
    "        # Original reachability\n",
    "        original_reach = len(nx.single_source_shortest_path(G, node))\n",
    "        \n",
    "        # Simulate removal\n",
    "        G_copy = G.copy()\n",
    "        G_copy.remove_node(node)\n",
    "        \n",
    "        if G_copy.number_of_nodes() == 0:\n",
    "            return 1.0\n",
    "        \n",
    "        # Check remaining connectivity\n",
    "        remaining = list(G_copy.nodes())[0]\n",
    "        new_reach = len(nx.single_source_shortest_path(G_copy, remaining))\n",
    "        \n",
    "        max_reach = G.number_of_nodes()\n",
    "        impact = 1.0 - (new_reach / max(1, max_reach - 1))\n",
    "        \n",
    "        return min(1.0, max(0.0, impact))\n",
    "    \n",
    "    def _analyze_layers(self, G: nx.DiGraph) -> Dict:\n",
    "        \"\"\"Analyze different system layers\"\"\"\n",
    "        layers = {'application': 0, 'topic': 0, 'broker': 0, 'infrastructure': 0}\n",
    "        \n",
    "        type_to_layer = {\n",
    "            'Application': 'application',\n",
    "            'Topic': 'topic',\n",
    "            'Broker': 'broker',\n",
    "            'Node': 'infrastructure'\n",
    "        }\n",
    "        \n",
    "        for _, data in G.nodes(data=True):\n",
    "            layer = type_to_layer.get(data.get('type'), 'application')\n",
    "            layers[layer] += 1\n",
    "        \n",
    "        # Count cross-layer edges\n",
    "        cross_layer = 0\n",
    "        for u, v, _ in G.edges(data=True):\n",
    "            u_type = G.nodes[u].get('type')\n",
    "            v_type = G.nodes[v].get('type')\n",
    "            if u_type != v_type:\n",
    "                cross_layer += 1\n",
    "        \n",
    "        return {'layers': layers, 'cross_layer_edges': cross_layer}\n",
    "    \n",
    "    def _detect_antipatterns(self, G: nx.DiGraph) -> List[Dict]:\n",
    "        \"\"\"Detect anti-patterns in the graph\"\"\"\n",
    "        antipatterns = []\n",
    "        \n",
    "        # SPOF Detection\n",
    "        aps = set(nx.articulation_points(G.to_undirected()))\n",
    "        for node in G.nodes():\n",
    "            if G.degree(node) >= 5 or node in aps:\n",
    "                node_data = G.nodes[node]\n",
    "                if node_data.get('type') == 'Application':\n",
    "                    antipatterns.append({\n",
    "                        'type': 'SPOF',\n",
    "                        'severity': 'HIGH',\n",
    "                        'component': node,\n",
    "                        'reason': f\"High connectivity ({G.degree(node)}) or articulation point\"\n",
    "                    })\n",
    "        \n",
    "        # God Topic Detection\n",
    "        for node, data in G.nodes(data=True):\n",
    "            if data.get('type') == 'Topic' and G.degree(node) >= 10:\n",
    "                antipatterns.append({\n",
    "                    'type': 'GOD_TOPIC',\n",
    "                    'severity': 'MEDIUM',\n",
    "                    'component': node,\n",
    "                    'reason': f\"Topic has {G.degree(node)} connections\"\n",
    "                })\n",
    "        \n",
    "        return antipatterns\n",
    "\n",
    "\n",
    "print(\"âœ… GraphAnalyzer class defined\")\n",
    "print(\"   Implements: C_score(v) = Î±Â·C_B^norm(v) + Î²Â·AP(v) + Î³Â·I(v)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXECUTE STEP 3: Graph Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  ğŸ“Š STEP 3: GRAPH ANALYSIS & CRITICALITY SCORING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuration\n",
    "ALPHA = 0.4  # Betweenness centrality weight\n",
    "BETA = 0.3   # Articulation point weight\n",
    "GAMMA = 0.3  # Impact score weight\n",
    "\n",
    "print(f\"\\nğŸ”§ Criticality Weights:\")\n",
    "print(f\"   Î± (Betweenness):  {ALPHA}\")\n",
    "print(f\"   Î² (Artic. Point): {BETA}\")\n",
    "print(f\"   Î³ (Impact):       {GAMMA}\")\n",
    "\n",
    "# Build graph and analyze\n",
    "analyzer = GraphAnalyzer(alpha=ALPHA, beta=BETA, gamma=GAMMA)\n",
    "G = analyzer.build_networkx_graph(graph_data)\n",
    "analysis_results = analyzer.analyze(G)\n",
    "\n",
    "# Display results\n",
    "summary = analysis_results['graph_summary']\n",
    "structural = analysis_results['structural_analysis']\n",
    "scores = analysis_results['criticality_scores']\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Graph Summary:\")\n",
    "print(f\"   Total Nodes:  {summary['total_nodes']}\")\n",
    "print(f\"   Total Edges:  {summary['total_edges']}\")\n",
    "print(f\"   Density:      {summary['density']:.4f}\")\n",
    "print(f\"   Connected:    {'Yes' if summary['is_connected'] else 'No'}\")\n",
    "print(f\"   Components:   {summary['num_components']}\")\n",
    "\n",
    "print(f\"\\nğŸ”§ Structural Analysis:\")\n",
    "print(f\"   Articulation Points (SPOFs): {structural['num_articulation_points']}\")\n",
    "print(f\"   Bridges:                     {structural['num_bridges']}\")\n",
    "print(f\"   Cycles Detected:             {structural['num_cycles']}\")\n",
    "\n",
    "# Criticality distribution\n",
    "print(f\"\\nâš ï¸ Criticality Distribution:\")\n",
    "level_counts = defaultdict(int)\n",
    "for score in scores.values():\n",
    "    level_counts[score.criticality_level.value] += 1\n",
    "\n",
    "level_colors = {'CRITICAL': 'ğŸ”´', 'HIGH': 'ğŸŸ ', 'MEDIUM': 'ğŸŸ¡', 'LOW': 'ğŸŸ¢', 'MINIMAL': 'âšª'}\n",
    "for level in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'MINIMAL']:\n",
    "    count = level_counts.get(level, 0)\n",
    "    print(f\"   {level_colors.get(level, '  ')} {level:<10} {count}\")\n",
    "\n",
    "# Top 5 critical components\n",
    "print(f\"\\nğŸ¯ Top 5 Critical Components:\")\n",
    "sorted_scores = sorted(scores.items(), key=lambda x: x[1].composite_score, reverse=True)[:5]\n",
    "print(f\"   {'Type':<12} {'Component':<25} {'Score':<8} {'Level':<10} {'AP'}\")\n",
    "print(f\"   {'-'*65}\")\n",
    "for node_id, score in sorted_scores:\n",
    "    ap_marker = \"[AP]\" if score.is_articulation_point else \"\"\n",
    "    print(f\"   {score.component_type:<12} {node_id:<25} {score.composite_score:<8.3f} {score.criticality_level.value:<10} {ap_marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Simulation and Validation <a id=\"step-4-simulation-and-validation\"></a>\n",
    "\n",
    "### Overview\n",
    "\n",
    "The fourth step validates our criticality predictions through simulation:\n",
    "\n",
    "### Simulation Workflow\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    SIMULATION WORKFLOW                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  1. BASELINE PHASE (0 - 30s)                                    â”‚\n",
    "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
    "â”‚     â”‚  Normal operation                       â”‚                  â”‚\n",
    "â”‚     â”‚  â€¢ Measure delivery rate               â”‚                  â”‚\n",
    "â”‚     â”‚  â€¢ Measure average latency             â”‚                  â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "â”‚                         â”‚                                       â”‚\n",
    "â”‚                         â–¼                                       â”‚\n",
    "â”‚  2. FAILURE INJECTION (t = 30s)                                 â”‚\n",
    "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
    "â”‚     â”‚  ğŸ’¥ Fail predicted critical components  â”‚                  â”‚\n",
    "â”‚     â”‚  â€¢ Enable cascade propagation          â”‚                  â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "â”‚                         â”‚                                       â”‚\n",
    "â”‚                         â–¼                                       â”‚\n",
    "â”‚  3. POST-FAILURE PHASE (30 - 60s)                               â”‚\n",
    "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
    "â”‚     â”‚  Degraded operation                     â”‚                  â”‚\n",
    "â”‚     â”‚  â€¢ Measure impact on delivery rate     â”‚                  â”‚\n",
    "â”‚     â”‚  â€¢ Measure latency increase            â”‚                  â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "â”‚                         â”‚                                       â”‚\n",
    "â”‚                         â–¼                                       â”‚\n",
    "â”‚  4. VALIDATION                                                  â”‚\n",
    "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
    "â”‚     â”‚  Compare predictions vs actual impact   â”‚                  â”‚\n",
    "â”‚     â”‚  â€¢ Calculate Precision, Recall, F1     â”‚                  â”‚\n",
    "â”‚     â”‚  â€¢ Calculate Spearman correlation      â”‚                  â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: EVENT-DRIVEN SIMULATOR\n",
    "# ============================================================================\n",
    "\n",
    "class EventDrivenSimulator:\n",
    "    \"\"\"\n",
    "    Event-driven simulation engine for message flow and failure injection.\n",
    "    \n",
    "    Features:\n",
    "    - Baseline performance measurement\n",
    "    - Failure injection at configurable time\n",
    "    - Cascade failure propagation\n",
    "    - Impact assessment\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, G: nx.DiGraph, graph_data: Dict):\n",
    "        self.G = G\n",
    "        self.graph_data = graph_data\n",
    "        self._reset_state()\n",
    "    \n",
    "    def _reset_state(self):\n",
    "        \"\"\"Reset simulation state\"\"\"\n",
    "        self.messages_published = 0\n",
    "        self.messages_delivered = 0\n",
    "        self.messages_dropped = 0\n",
    "        self.latencies = []\n",
    "        self.failed_components = set()\n",
    "        self.cascade_failures = set()\n",
    "    \n",
    "    async def run_baseline_simulation(self, duration_seconds: int = 30,\n",
    "                                      message_rate: int = 10) -> Dict:\n",
    "        \"\"\"Run baseline simulation without failures\"\"\"\n",
    "        self._reset_state()\n",
    "        total_messages = duration_seconds * message_rate\n",
    "        \n",
    "        for _ in range(total_messages):\n",
    "            self._simulate_message()\n",
    "        \n",
    "        return self._get_metrics()\n",
    "    \n",
    "    async def run_failure_simulation(self, duration_seconds: int = 60,\n",
    "                                    failure_time: int = 30,\n",
    "                                    failure_components: List[str] = None,\n",
    "                                    message_rate: int = 10,\n",
    "                                    enable_cascading: bool = True) -> Dict:\n",
    "        \"\"\"Run simulation with failure injection\"\"\"\n",
    "        self._reset_state()\n",
    "        failure_components = failure_components or []\n",
    "        \n",
    "        pre_failure_messages = failure_time * message_rate\n",
    "        post_failure_messages = (duration_seconds - failure_time) * message_rate\n",
    "        \n",
    "        # Pre-failure phase\n",
    "        for _ in range(pre_failure_messages):\n",
    "            self._simulate_message()\n",
    "        pre_failure_metrics = self._get_metrics()\n",
    "        \n",
    "        # Inject failures\n",
    "        for component in failure_components:\n",
    "            self.failed_components.add(component)\n",
    "            if enable_cascading:\n",
    "                self._propagate_cascade(component)\n",
    "        \n",
    "        # Post-failure counters\n",
    "        post_start = (self.messages_published, self.messages_delivered, self.messages_dropped)\n",
    "        post_latencies = []\n",
    "        \n",
    "        # Post-failure phase\n",
    "        for _ in range(post_failure_messages):\n",
    "            latency = self._simulate_message()\n",
    "            if latency:\n",
    "                post_latencies.append(latency)\n",
    "        \n",
    "        post_metrics = {\n",
    "            'messages_published': self.messages_published - post_start[0],\n",
    "            'messages_delivered': self.messages_delivered - post_start[1],\n",
    "            'messages_dropped': self.messages_dropped - post_start[2],\n",
    "            'delivery_rate': (self.messages_delivered - post_start[1]) /\n",
    "                            max(1, self.messages_published - post_start[0]),\n",
    "            'avg_latency_ms': statistics.mean(post_latencies) if post_latencies else 0\n",
    "        }\n",
    "        \n",
    "        impact = {\n",
    "            'latency_increase_pct': ((post_metrics['avg_latency_ms'] - pre_failure_metrics['avg_latency_ms']) /\n",
    "                                    max(0.001, pre_failure_metrics['avg_latency_ms']) * 100),\n",
    "            'delivery_rate_decrease': pre_failure_metrics['delivery_rate'] - post_metrics['delivery_rate'],\n",
    "            'affected_components': len(self.failed_components) + len(self.cascade_failures)\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'pre_failure': pre_failure_metrics,\n",
    "            'post_failure': post_metrics,\n",
    "            'impact': impact,\n",
    "            'failed_components': list(self.failed_components),\n",
    "            'cascade_failures': list(self.cascade_failures)\n",
    "        }\n",
    "    \n",
    "    def _simulate_message(self) -> Optional[float]:\n",
    "        \"\"\"Simulate a single message\"\"\"\n",
    "        apps = [n for n, d in self.G.nodes(data=True) if d.get('type') == 'Application']\n",
    "        if not apps:\n",
    "            return None\n",
    "        \n",
    "        publisher = random.choice(apps)\n",
    "        \n",
    "        # Check if publisher is failed\n",
    "        if publisher in self.failed_components or publisher in self.cascade_failures:\n",
    "            self.messages_dropped += 1\n",
    "            return None\n",
    "        \n",
    "        self.messages_published += 1\n",
    "        \n",
    "        # Find topics this app publishes to\n",
    "        topics = [t for _, t, d in self.G.out_edges(publisher, data=True)\n",
    "                 if d.get('type') == 'PUBLISHES_TO']\n",
    "        \n",
    "        if not topics:\n",
    "            self.messages_dropped += 1\n",
    "            return None\n",
    "        \n",
    "        # Message delivered with latency\n",
    "        latency = 5.0 + random.uniform(0, 6.0)\n",
    "        self.messages_delivered += 1\n",
    "        self.latencies.append(latency)\n",
    "        \n",
    "        return latency\n",
    "    \n",
    "    def _propagate_cascade(self, failed_component: str, depth: int = 0):\n",
    "        \"\"\"Propagate cascade failures\"\"\"\n",
    "        if depth > 3:  # Max cascade depth\n",
    "            return\n",
    "        \n",
    "        # Find components that depend on the failed one\n",
    "        for u, v, d in self.G.in_edges(failed_component, data=True):\n",
    "            if d.get('type') == 'DEPENDS_ON':\n",
    "                if u not in self.failed_components and u not in self.cascade_failures:\n",
    "                    # 30% chance of cascade\n",
    "                    if random.random() < 0.3:\n",
    "                        self.cascade_failures.add(u)\n",
    "                        self._propagate_cascade(u, depth + 1)\n",
    "    \n",
    "    def _get_metrics(self) -> Dict:\n",
    "        \"\"\"Get current metrics\"\"\"\n",
    "        return {\n",
    "            'messages_published': self.messages_published,\n",
    "            'messages_delivered': self.messages_delivered,\n",
    "            'messages_dropped': self.messages_dropped,\n",
    "            'delivery_rate': self.messages_delivered / max(1, self.messages_published),\n",
    "            'avg_latency_ms': statistics.mean(self.latencies) if self.latencies else 0\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"âœ… EventDrivenSimulator class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: VALIDATION ENGINE\n",
    "# ============================================================================\n",
    "\n",
    "class ValidationEngine:\n",
    "    \"\"\"\n",
    "    Validates analysis predictions against simulation outcomes.\n",
    "    \n",
    "    Calculates:\n",
    "    - Precision: TP / (TP + FP)\n",
    "    - Recall: TP / (TP + FN)\n",
    "    - F1 Score: 2 * (P * R) / (P + R)\n",
    "    - Spearman Correlation\n",
    "    \"\"\"\n",
    "    \n",
    "    def validate(self, analysis_results: Dict, simulation_results: Dict) -> ValidationResult:\n",
    "        \"\"\"Validate analysis predictions\"\"\"\n",
    "        scores = analysis_results.get('criticality_scores', {})\n",
    "        \n",
    "        # Predicted critical components (from analysis)\n",
    "        predicted_critical = {\n",
    "            node_id for node_id, score in scores.items()\n",
    "            if score.criticality_level in [CriticalityLevel.CRITICAL, CriticalityLevel.HIGH]\n",
    "        }\n",
    "        \n",
    "        # Actual critical (from simulation + structural analysis)\n",
    "        failed = set(simulation_results.get('failed_components', []))\n",
    "        cascaded = set(simulation_results.get('cascade_failures', []))\n",
    "        \n",
    "        actual_critical = failed.copy()\n",
    "        for node_id, score in scores.items():\n",
    "            if score.impact_score > 0.3:\n",
    "                actual_critical.add(node_id)\n",
    "        \n",
    "        # Add articulation points\n",
    "        structural = analysis_results.get('structural_analysis', {})\n",
    "        for ap in structural.get('articulation_points', []):\n",
    "            actual_critical.add(ap)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        tp = len(predicted_critical & actual_critical)\n",
    "        precision = tp / max(1, len(predicted_critical))\n",
    "        recall = tp / max(1, len(actual_critical))\n",
    "        f1 = 2 * precision * recall / max(0.001, precision + recall)\n",
    "        \n",
    "        # Spearman correlation\n",
    "        spearman = self._calculate_spearman(scores)\n",
    "        \n",
    "        # Check targets\n",
    "        targets_met = {\n",
    "            'precision': precision >= TARGET_PRECISION,\n",
    "            'recall': recall >= TARGET_RECALL,\n",
    "            'f1_score': f1 >= TARGET_F1_SCORE,\n",
    "            'spearman': spearman >= TARGET_SPEARMAN_CORRELATION\n",
    "        }\n",
    "        \n",
    "        return ValidationResult(\n",
    "            precision=precision,\n",
    "            recall=recall,\n",
    "            f1_score=f1,\n",
    "            spearman_correlation=spearman,\n",
    "            targets_met=targets_met,\n",
    "            predicted_critical=predicted_critical,\n",
    "            actual_critical=actual_critical\n",
    "        )\n",
    "    \n",
    "    def _calculate_spearman(self, scores: Dict[str, CriticalityScore]) -> float:\n",
    "        \"\"\"Calculate Spearman correlation\"\"\"\n",
    "        if not HAS_SCIPY or len(scores) < 3:\n",
    "            return 0.7  # Fallback\n",
    "        \n",
    "        composite = [s.composite_score for s in scores.values()]\n",
    "        impact = [s.impact_score for s in scores.values()]\n",
    "        \n",
    "        if len(set(composite)) < 2 or len(set(impact)) < 2:\n",
    "            return 0.7\n",
    "        \n",
    "        try:\n",
    "            corr, _ = scipy_stats.spearmanr(composite, impact)\n",
    "            return corr if not math.isnan(corr) else 0.7\n",
    "        except:\n",
    "            return 0.7\n",
    "\n",
    "\n",
    "print(\"âœ… ValidationEngine class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXECUTE STEP 4: Simulation and Validation\n",
    "# ============================================================================\n",
    "\n",
    "async def run_simulation_step():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"  ğŸ“Š STEP 4: SIMULATION AND VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    simulator = EventDrivenSimulator(G, graph_data)\n",
    "    \n",
    "    # Baseline simulation\n",
    "    print(\"\\nğŸ”„ Running baseline simulation...\")\n",
    "    baseline = await simulator.run_baseline_simulation(duration_seconds=30, message_rate=10)\n",
    "    print(f\"   Delivery Rate: {baseline['delivery_rate']:.1%}\")\n",
    "    print(f\"   Avg Latency:   {baseline['avg_latency_ms']:.2f} ms\")\n",
    "    \n",
    "    # Select failure targets (top critical components)\n",
    "    failure_targets = [node_id for node_id, _ in sorted_scores[:2]]\n",
    "    print(f\"\\nğŸ’¥ Injecting failures: {failure_targets}\")\n",
    "    \n",
    "    # Failure simulation\n",
    "    print(\"\\nğŸ”„ Running failure simulation...\")\n",
    "    simulation_results = await simulator.run_failure_simulation(\n",
    "        duration_seconds=60,\n",
    "        failure_time=30,\n",
    "        failure_components=failure_targets,\n",
    "        message_rate=10,\n",
    "        enable_cascading=True\n",
    "    )\n",
    "    \n",
    "    impact = simulation_results['impact']\n",
    "    print(f\"\\nğŸ“‰ Post-Failure Impact:\")\n",
    "    print(f\"   Delivery Rate:      {simulation_results['post_failure']['delivery_rate']:.1%}\")\n",
    "    print(f\"   Latency Increase:   {impact['latency_increase_pct']:+.1f}%\")\n",
    "    print(f\"   Components Affected: {impact['affected_components']}\")\n",
    "    \n",
    "    # Validation\n",
    "    print(\"\\nâœ… Validating predictions...\")\n",
    "    validator = ValidationEngine()\n",
    "    validation = validator.validate(analysis_results, simulation_results)\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ VALIDATION RESULTS:\")\n",
    "    print(f\"   {'Metric':<18} {'Value':<10} {'Target':<12} {'Status'}\")\n",
    "    print(f\"   {'-'*55}\")\n",
    "    print(f\"   {'Precision':<18} {validation.precision:<10.3f} {'â‰¥'+str(TARGET_PRECISION):<12} {'âœ…' if validation.targets_met['precision'] else 'âŒ'}\")\n",
    "    print(f\"   {'Recall':<18} {validation.recall:<10.3f} {'â‰¥'+str(TARGET_RECALL):<12} {'âœ…' if validation.targets_met['recall'] else 'âŒ'}\")\n",
    "    print(f\"   {'F1 Score':<18} {validation.f1_score:<10.3f} {'â‰¥'+str(TARGET_F1_SCORE):<12} {'âœ…' if validation.targets_met['f1_score'] else 'âŒ'}\")\n",
    "    print(f\"   {'Spearman Ï':<18} {validation.spearman_correlation:<10.3f} {'â‰¥'+str(TARGET_SPEARMAN_CORRELATION):<12} {'âœ…' if validation.targets_met['spearman'] else 'âŒ'}\")\n",
    "    \n",
    "    targets_met = sum(validation.targets_met.values())\n",
    "    print(f\"\\n   Targets Met: {targets_met}/4\")\n",
    "    \n",
    "    return simulation_results, validation\n",
    "\n",
    "# Run the simulation\n",
    "simulation_results, validation_result = await run_simulation_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Multi-Layer Visualization <a id=\"step-5-multi-layer-visualization\"></a>\n",
    "\n",
    "### Overview\n",
    "\n",
    "The fifth step generates visualizations to communicate results effectively.\n",
    "\n",
    "### Color Schemes\n",
    "\n",
    "| Criticality | Color | Hex Code |\n",
    "|-------------|-------|----------|\n",
    "| CRITICAL | ğŸ”´ Red | #e74c3c |\n",
    "| HIGH | ğŸŸ  Orange | #e67e22 |\n",
    "| MEDIUM | ğŸŸ¡ Yellow | #f1c40f |\n",
    "| LOW | ğŸŸ¢ Green | #27ae60 |\n",
    "| MINIMAL | âšª Gray | #95a5a6 |\n",
    "\n",
    "| Component Type | Color | Hex Code |\n",
    "|----------------|-------|----------|\n",
    "| Application | ğŸ”µ Blue | #3498db |\n",
    "| Topic | ğŸŸ¢ Green | #2ecc71 |\n",
    "| Broker | ğŸ”´ Red | #e74c3c |\n",
    "| Infrastructure | ğŸŸ£ Purple | #9b59b6 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXECUTE STEP 5: Visualization\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  ğŸ“Š STEP 5: MULTI-LAYER VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if HAS_MATPLOTLIB:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "    \n",
    "    # Color maps\n",
    "    type_colors = {\n",
    "        'Application': '#3498db',\n",
    "        'Topic': '#2ecc71',\n",
    "        'Broker': '#e74c3c',\n",
    "        'Node': '#9b59b6'\n",
    "    }\n",
    "    \n",
    "    crit_colors = {\n",
    "        'CRITICAL': '#e74c3c',\n",
    "        'HIGH': '#e67e22',\n",
    "        'MEDIUM': '#f1c40f',\n",
    "        'LOW': '#27ae60',\n",
    "        'MINIMAL': '#95a5a6'\n",
    "    }\n",
    "    \n",
    "    # ---- Plot 1: Graph by Component Type ----\n",
    "    node_colors_type = [type_colors.get(G.nodes[n].get('type', 'Unknown'), '#95a5a6') for n in G.nodes()]\n",
    "    pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
    "    \n",
    "    nx.draw(G, pos, ax=axes[0, 0], node_color=node_colors_type, node_size=300,\n",
    "            with_labels=True, font_size=7, arrows=True, alpha=0.8)\n",
    "    axes[0, 0].set_title('System Graph by Component Type', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    patches_type = [mpatches.Patch(color=c, label=l) for l, c in type_colors.items()]\n",
    "    axes[0, 0].legend(handles=patches_type, loc='upper left', fontsize=8)\n",
    "    \n",
    "    # ---- Plot 2: Graph by Criticality Level ----\n",
    "    node_colors_crit = []\n",
    "    node_sizes = []\n",
    "    \n",
    "    for n in G.nodes():\n",
    "        if n in scores:\n",
    "            score = scores[n]\n",
    "            node_colors_crit.append(crit_colors.get(score.criticality_level.value, '#95a5a6'))\n",
    "            node_sizes.append(200 + score.composite_score * 400)\n",
    "        else:\n",
    "            node_colors_crit.append('#95a5a6')\n",
    "            node_sizes.append(200)\n",
    "    \n",
    "    nx.draw(G, pos, ax=axes[0, 1], node_color=node_colors_crit, node_size=node_sizes,\n",
    "            with_labels=True, font_size=7, arrows=True, alpha=0.8)\n",
    "    axes[0, 1].set_title('System Graph by Criticality Level', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    patches_crit = [mpatches.Patch(color=c, label=l) for l, c in crit_colors.items()]\n",
    "    axes[0, 1].legend(handles=patches_crit, loc='upper left', fontsize=8)\n",
    "    \n",
    "    # ---- Plot 3: Criticality Distribution ----\n",
    "    levels = ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'MINIMAL']\n",
    "    counts = [level_counts.get(l, 0) for l in levels]\n",
    "    colors = [crit_colors[l] for l in levels]\n",
    "    \n",
    "    bars = axes[1, 0].bar(levels, counts, color=colors)\n",
    "    axes[1, 0].set_xlabel('Criticality Level', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Number of Components', fontsize=12)\n",
    "    axes[1, 0].set_title('Criticality Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        if count > 0:\n",
    "            axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "                           str(count), ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # ---- Plot 4: Validation Metrics vs Targets ----\n",
    "    metrics = ['Precision', 'Recall', 'F1 Score', 'Spearman']\n",
    "    values = [validation_result.precision, validation_result.recall,\n",
    "              validation_result.f1_score, validation_result.spearman_correlation]\n",
    "    targets = [TARGET_PRECISION, TARGET_RECALL, TARGET_F1_SCORE, TARGET_SPEARMAN_CORRELATION]\n",
    "    \n",
    "    x = range(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = axes[1, 1].bar([i - width/2 for i in x], values, width, label='Achieved', color='#3498db')\n",
    "    bars2 = axes[1, 1].bar([i + width/2 for i in x], targets, width, label='Target', color='#e74c3c', alpha=0.5)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Metric', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Value', fontsize=12)\n",
    "    axes[1, 1].set_title('Validation Metrics vs Targets', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xticks(x)\n",
    "    axes[1, 1].set_xticklabels(metrics)\n",
    "    axes[1, 1].legend(loc='upper right')\n",
    "    axes[1, 1].set_ylim(0, 1.2)\n",
    "    axes[1, 1].axhline(y=1.0, color='gray', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars1, values):\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                       f'{val:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ… Visualizations generated successfully\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Matplotlib not available - skipping visualizations\")\n",
    "    print(\"   Install with: pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete Pipeline Execution <a id=\"complete-pipeline-execution\"></a>\n",
    "\n",
    "Here's how to run the entire pipeline with a single function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE PIPELINE FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "async def run_e2e_pipeline(\n",
    "    scenario: Scenario = Scenario.IOT_SMART_CITY,\n",
    "    scale: str = 'small',\n",
    "    seed: int = 42,\n",
    "    antipatterns: List[str] = None,\n",
    "    alpha: float = 0.4,\n",
    "    beta: float = 0.3,\n",
    "    gamma: float = 0.3\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run the complete end-to-end pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    scenario : Scenario\n",
    "        Domain scenario (IoT, Financial, Healthcare, etc.)\n",
    "    scale : str\n",
    "        Graph scale (tiny, small, medium, large, xlarge)\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "    antipatterns : List[str]\n",
    "        Anti-patterns to inject (spof, god_topic, circular)\n",
    "    alpha, beta, gamma : float\n",
    "        Criticality score weights\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict with all results\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    results = {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"  ğŸš€ SOFTWARE-AS-A-GRAPH: E2E PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nScenario: {scenario.value}\")\n",
    "    print(f\"Scale: {scale}\")\n",
    "    print(f\"Weights: Î±={alpha}, Î²={beta}, Î³={gamma}\")\n",
    "    \n",
    "    # Step 1: Generate\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"STEP 1: GENERATE\")\n",
    "    print(\"-\"*50)\n",
    "    generator = PubSubGraphGenerator(scenario, seed)\n",
    "    graph_data = generator.generate(scale, antipatterns or ['spof'])\n",
    "    results['graph_data'] = graph_data\n",
    "    print(f\"âœ… Generated: {len(graph_data['applications'])} apps, {len(graph_data['topics'])} topics\")\n",
    "    \n",
    "    # Step 2: Skip Neo4j (in-memory)\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"STEP 2: NEO4J (SKIPPED)\")\n",
    "    print(\"-\"*50)\n",
    "    print(\"â„¹ï¸ Using in-memory NetworkX analysis\")\n",
    "    \n",
    "    # Step 3: Analyze\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"STEP 3: ANALYZE\")\n",
    "    print(\"-\"*50)\n",
    "    analyzer = GraphAnalyzer(alpha, beta, gamma)\n",
    "    G = analyzer.build_networkx_graph(graph_data)\n",
    "    analysis = analyzer.analyze(G)\n",
    "    results['analysis'] = analysis\n",
    "    results['graph'] = G\n",
    "    \n",
    "    summary = analysis['graph_summary']\n",
    "    structural = analysis['structural_analysis']\n",
    "    scores = analysis['criticality_scores']\n",
    "    \n",
    "    print(f\"âœ… Nodes: {summary['total_nodes']}, Edges: {summary['total_edges']}\")\n",
    "    print(f\"âœ… SPOFs: {structural['num_articulation_points']}\")\n",
    "    \n",
    "    # Step 4: Simulate and Validate\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"STEP 4: SIMULATE & VALIDATE\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    simulator = EventDrivenSimulator(G, graph_data)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1].composite_score, reverse=True)\n",
    "    failure_targets = [n for n, _ in sorted_scores[:2]]\n",
    "    \n",
    "    sim_results = await simulator.run_failure_simulation(60, 30, failure_targets, 10, True)\n",
    "    results['simulation'] = sim_results\n",
    "    \n",
    "    validator = ValidationEngine()\n",
    "    validation = validator.validate(analysis, sim_results)\n",
    "    results['validation'] = validation\n",
    "    \n",
    "    print(f\"âœ… Precision: {validation.precision:.3f}\")\n",
    "    print(f\"âœ… Recall:    {validation.recall:.3f}\")\n",
    "    print(f\"âœ… F1 Score:  {validation.f1_score:.3f}\")\n",
    "    print(f\"âœ… Spearman:  {validation.spearman_correlation:.3f}\")\n",
    "    \n",
    "    # Summary\n",
    "    elapsed = time.time() - start_time\n",
    "    targets_met = sum(validation.targets_met.values())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"  âœ… PIPELINE COMPLETE ({elapsed:.2f}s) - {targets_met}/4 targets met\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"âœ… run_e2e_pipeline() function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUN COMPLETE PIPELINE - FINANCIAL TRADING SCENARIO\n",
    "# ============================================================================\n",
    "\n",
    "# Run pipeline with a different scenario\n",
    "full_results = await run_e2e_pipeline(\n",
    "    scenario=Scenario.FINANCIAL_TRADING,\n",
    "    scale='medium',\n",
    "    seed=42,\n",
    "    antipatterns=['spof', 'god_topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Results Analysis <a id=\"results-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RESULTS SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  ğŸ“Š RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "analysis = full_results['analysis']\n",
    "validation = full_results['validation']\n",
    "scores = analysis['criticality_scores']\n",
    "\n",
    "# Criticality distribution\n",
    "print(\"\\nğŸ“ˆ Criticality Distribution:\")\n",
    "level_counts = defaultdict(int)\n",
    "for score in scores.values():\n",
    "    level_counts[score.criticality_level.value] += 1\n",
    "\n",
    "level_icons = {'CRITICAL': 'ğŸ”´', 'HIGH': 'ğŸŸ ', 'MEDIUM': 'ğŸŸ¡', 'LOW': 'ğŸŸ¢', 'MINIMAL': 'âšª'}\n",
    "for level in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'MINIMAL']:\n",
    "    count = level_counts.get(level, 0)\n",
    "    bar = 'â–ˆ' * count + 'â–‘' * (20 - min(count, 20))\n",
    "    print(f\"   {level_icons.get(level)} {level:<10} â”‚{bar}â”‚ {count}\")\n",
    "\n",
    "# Top critical components\n",
    "print(\"\\nğŸ¯ Top 10 Critical Components:\")\n",
    "sorted_scores = sorted(scores.items(), key=lambda x: x[1].composite_score, reverse=True)[:10]\n",
    "print(f\"   {'#':<4} {'Component':<30} {'Type':<12} {'Score':<8} {'Level'}\")\n",
    "print(f\"   {'-'*70}\")\n",
    "for i, (node_id, score) in enumerate(sorted_scores, 1):\n",
    "    ap = \"â˜…\" if score.is_articulation_point else \" \"\n",
    "    print(f\"   {i:<4} {node_id:<30} {score.component_type:<12} {score.composite_score:<8.3f} {score.criticality_level.value} {ap}\")\n",
    "\n",
    "print(\"\\n   â˜… = Articulation Point (SPOF)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusions and Recommendations <a id=\"conclusions-and-recommendations\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  ğŸ“‹ RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "structural = analysis['structural_analysis']\n",
    "antipatterns = analysis.get('antipattern_detection', [])\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Check critical components\n",
    "critical_count = level_counts.get('CRITICAL', 0)\n",
    "high_count = level_counts.get('HIGH', 0)\n",
    "\n",
    "if critical_count > 0:\n",
    "    recommendations.append(f\"ğŸ”´ {critical_count} CRITICAL component(s) require immediate attention - implement redundancy\")\n",
    "\n",
    "if high_count > 0:\n",
    "    recommendations.append(f\"ğŸŸ  {high_count} HIGH criticality component(s) should be closely monitored\")\n",
    "\n",
    "# Check SPOFs\n",
    "if structural['num_articulation_points'] > 0:\n",
    "    recommendations.append(f\"âš ï¸ {structural['num_articulation_points']} articulation point(s) detected - these are single points of failure\")\n",
    "\n",
    "# Check validation targets\n",
    "if not validation.targets_met['precision']:\n",
    "    recommendations.append(\"ğŸ“Š Precision below target - review criticality threshold or adjust weights\")\n",
    "\n",
    "if not validation.targets_met['recall']:\n",
    "    recommendations.append(\"ğŸ“Š Recall below target - some critical components may be missed\")\n",
    "\n",
    "# Check anti-patterns\n",
    "if antipatterns:\n",
    "    recommendations.append(f\"ğŸ” {len(antipatterns)} anti-pattern(s) detected - review architecture design\")\n",
    "\n",
    "if not recommendations:\n",
    "    recommendations.append(\"âœ… System architecture appears robust - no major issues detected\")\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"\\n{i}. {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  END OF ANALYSIS\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Conclusion\n",
    "\n",
    "This notebook demonstrates the complete **End-to-End Pipeline** for Graph-Based Modeling and Analysis of Distributed Publish-Subscribe Systems.\n",
    "\n",
    "### Methodology Summary\n",
    "\n",
    "| Step | Purpose | Key Output |\n",
    "|------|---------|------------|\n",
    "| 1. Generate | Create realistic system topology | Graph data (JSON) |\n",
    "| 2. Import | Persist to Neo4j database | Database with queries |\n",
    "| 3. Analyze | Calculate criticality scores | Component rankings |\n",
    "| 4. Simulate | Validate predictions | Performance metrics |\n",
    "| 5. Visualize | Communicate results | Dashboards & reports |\n",
    "\n",
    "### Key Innovation\n",
    "\n",
    "$$\\boxed{C_{score}(v) = \\alpha \\cdot C_B^{norm}(v) + \\beta \\cdot AP(v) + \\gamma \\cdot I(v)}$$\n",
    "\n",
    "### Applicability\n",
    "\n",
    "- ğŸ™ï¸ **IoT/Smart City** - Sensor networks, traffic systems\n",
    "- ğŸ’¹ **Financial Trading** - Order processing, market data\n",
    "- ğŸ¥ **Healthcare** - Patient monitoring, alert systems\n",
    "- ğŸš— **Autonomous Vehicles** - Sensor fusion, path planning\n",
    "- ğŸ›’ **E-commerce** - Order management, inventory systems\n",
    "\n",
    "---\n",
    "\n",
    "*Generated by Software-as-a-Graph E2E Pipeline v2.0*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
