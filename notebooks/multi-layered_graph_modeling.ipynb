{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layered Graph Analysis for Critical Component Identification\n",
    "## Multi-Perspective Analysis of Distributed Publish-Subscribe Systems\n",
    "\n",
    "**Objective:** Identify critical components across three distinct graph representations:\n",
    "\n",
    "1. **Application-Level Layer**: Dependencies among applications (DEPENDS_ON)\n",
    "2. **Infrastructure-Level Layer**: Node connectivity (CONNECTS_TO)\n",
    "3. **Complete System View**: Full multi-layer graph with all relationships\n",
    "\n",
    "This analysis reveals how criticality manifests differently at each architectural layer and enables comprehensive vulnerability assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Set\n",
    "from dataclasses import dataclass\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"✓ Environment ready for layered analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Complete System Graph\n",
    "\n",
    "We'll use the graph generated from the previous notebook or create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this demonstration, we'll create a synthetic graph\n",
    "# In practice, load your existing graph here\n",
    "\n",
    "def create_sample_system() -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Create a representative pub-sub system for layered analysis\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Physical nodes\n",
    "    nodes = ['Node1', 'Node2', 'Node3', 'Node4', 'Node5']\n",
    "    for node in nodes:\n",
    "        G.add_node(node, type='Node', name=node)\n",
    "    \n",
    "    # Node connectivity (CONNECTS_TO)\n",
    "    node_connections = [\n",
    "        ('Node1', 'Node2'), ('Node2', 'Node1'),\n",
    "        ('Node2', 'Node3'), ('Node3', 'Node2'),\n",
    "        ('Node3', 'Node4'), ('Node4', 'Node3'),\n",
    "        ('Node1', 'Node5'), ('Node5', 'Node1'),\n",
    "        ('Node4', 'Node5'), ('Node5', 'Node4')\n",
    "    ]\n",
    "    for src, tgt in node_connections:\n",
    "        G.add_edge(src, tgt, type='CONNECTS_TO')\n",
    "    \n",
    "    # Brokers\n",
    "    brokers = ['Broker1', 'Broker2']\n",
    "    for i, broker in enumerate(brokers, 1):\n",
    "        G.add_node(broker, type='Broker', name=broker)\n",
    "        G.add_edge(broker, f'Node{i}', type='RUNS_ON')\n",
    "    \n",
    "    # Topics\n",
    "    topics = ['payment/orders', 'user/events', 'metrics/system', \n",
    "              'notification/alerts', 'inventory/updates']\n",
    "    for i, topic in enumerate(topics, 1):\n",
    "        topic_id = f'T{i}'\n",
    "        G.add_node(topic_id, type='Topic', name=topic,\n",
    "                   durability='PERSISTENT' if 'payment' in topic or 'inventory' in topic else 'TRANSIENT_LOCAL',\n",
    "                   reliability='RELIABLE' if 'payment' in topic else 'BEST_EFFORT')\n",
    "        broker = brokers[i % 2]\n",
    "        G.add_edge(broker, topic_id, type='ROUTES')\n",
    "    \n",
    "    # Applications\n",
    "    apps = [\n",
    "        ('PaymentService', 'Node1', ['T1'], ['T2', 'T5']),\n",
    "        ('UserService', 'Node2', ['T2'], ['T1', 'T4']),\n",
    "        ('MetricsCollector', 'Node3', ['T3'], ['T1', 'T2', 'T5']),\n",
    "        ('NotificationService', 'Node3', ['T4'], ['T2', 'T5']),\n",
    "        ('InventoryService', 'Node4', ['T5'], ['T1']),\n",
    "        ('APIGateway', 'Node1', [], ['T1', 'T2', 'T3', 'T4', 'T5']),\n",
    "        ('LogAggregator', 'Node5', [], ['T3']),\n",
    "        ('AnalyticsEngine', 'Node5', [], ['T3', 'T5'])\n",
    "    ]\n",
    "    \n",
    "    for app_name, node, publishes, subscribes in apps:\n",
    "        G.add_node(app_name, type='Application', name=app_name)\n",
    "        G.add_edge(app_name, node, type='RUNS_ON')\n",
    "        \n",
    "        for topic in publishes:\n",
    "            G.add_edge(app_name, topic, type='PUBLISHES_TO')\n",
    "        \n",
    "        for topic in subscribes:\n",
    "            G.add_edge(app_name, topic, type='SUBSCRIBES_TO')\n",
    "    \n",
    "    # Create DEPENDS_ON relationships\n",
    "    for app1, _, publishes, _ in apps:\n",
    "        for topic in publishes:\n",
    "            for app2, _, _, subscribes in apps:\n",
    "                if app1 != app2 and topic in subscribes:\n",
    "                    G.add_edge(app2, app1, type='DEPENDS_ON')\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Create complete system graph\n",
    "complete_graph = create_sample_system()\n",
    "\n",
    "print(\"Complete System Graph Statistics:\")\n",
    "print(f\"  Total nodes: {complete_graph.number_of_nodes()}\")\n",
    "print(f\"  Total edges: {complete_graph.number_of_edges()}\")\n",
    "print(f\"\\nNode types:\")\n",
    "type_counts = {}\n",
    "for node, data in complete_graph.nodes(data=True):\n",
    "    node_type = data.get('type', 'Unknown')\n",
    "    type_counts[node_type] = type_counts.get(node_type, 0) + 1\n",
    "for node_type, count in sorted(type_counts.items()):\n",
    "    print(f\"    {node_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Layer Extraction Functions\n",
    "\n",
    "Extract specific layers from the complete graph based on node types and edge types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerExtractor:\n",
    "    \"\"\"Extract different architectural layers from the complete system graph\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_application_layer(graph: nx.DiGraph) -> nx.DiGraph:\n",
    "        \"\"\"\n",
    "        Extract application-level layer showing dependencies among applications\n",
    "        \n",
    "        Includes:\n",
    "        - Application nodes\n",
    "        - DEPENDS_ON edges (application dependencies)\n",
    "        \"\"\"\n",
    "        app_graph = nx.DiGraph()\n",
    "        \n",
    "        # Add application nodes\n",
    "        for node, data in graph.nodes(data=True):\n",
    "            if data.get('type') == 'Application':\n",
    "                app_graph.add_node(node, **data)\n",
    "        \n",
    "        # Add DEPENDS_ON edges between applications\n",
    "        for src, tgt, edge_data in graph.edges(data=True):\n",
    "            if (edge_data.get('type') == 'DEPENDS_ON' and \n",
    "                src in app_graph and tgt in app_graph):\n",
    "                app_graph.add_edge(src, tgt, **edge_data)\n",
    "        \n",
    "        return app_graph\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_infrastructure_layer(graph: nx.DiGraph) -> nx.DiGraph:\n",
    "        \"\"\"\n",
    "        Extract infrastructure-level layer showing physical node connectivity\n",
    "        \n",
    "        Includes:\n",
    "        - Node (physical/virtual machine) nodes\n",
    "        - CONNECTS_TO edges (network connectivity)\n",
    "        \"\"\"\n",
    "        infra_graph = nx.DiGraph()\n",
    "        \n",
    "        # Add physical nodes\n",
    "        for node, data in graph.nodes(data=True):\n",
    "            if data.get('type') == 'Node':\n",
    "                infra_graph.add_node(node, **data)\n",
    "        \n",
    "        # Add CONNECTS_TO edges between nodes\n",
    "        for src, tgt, edge_data in graph.edges(data=True):\n",
    "            if (edge_data.get('type') == 'CONNECTS_TO' and \n",
    "                src in infra_graph and tgt in infra_graph):\n",
    "                infra_graph.add_edge(src, tgt, **edge_data)\n",
    "        \n",
    "        return infra_graph\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_messaging_layer(graph: nx.DiGraph) -> nx.DiGraph:\n",
    "        \"\"\"\n",
    "        Extract messaging layer showing pub-sub communication patterns\n",
    "        \n",
    "        Includes:\n",
    "        - Application, Topic, and Broker nodes\n",
    "        - PUBLISHES_TO, SUBSCRIBES_TO, ROUTES edges\n",
    "        \"\"\"\n",
    "        msg_graph = nx.DiGraph()\n",
    "        \n",
    "        # Add relevant nodes\n",
    "        for node, data in graph.nodes(data=True):\n",
    "            if data.get('type') in ['Application', 'Topic', 'Broker']:\n",
    "                msg_graph.add_node(node, **data)\n",
    "        \n",
    "        # Add messaging edges\n",
    "        for src, tgt, edge_data in graph.edges(data=True):\n",
    "            edge_type = edge_data.get('type')\n",
    "            if (edge_type in ['PUBLISHES_TO', 'SUBSCRIBES_TO', 'ROUTES'] and\n",
    "                src in msg_graph and tgt in msg_graph):\n",
    "                msg_graph.add_edge(src, tgt, **edge_data)\n",
    "        \n",
    "        return msg_graph\n",
    "\n",
    "# Extract layers\n",
    "extractor = LayerExtractor()\n",
    "\n",
    "app_layer = extractor.extract_application_layer(complete_graph)\n",
    "infra_layer = extractor.extract_infrastructure_layer(complete_graph)\n",
    "msg_layer = extractor.extract_messaging_layer(complete_graph)\n",
    "\n",
    "print(\"Layer Extraction Summary:\")\n",
    "print(f\"\\nApplication Layer:\")\n",
    "print(f\"  Nodes: {app_layer.number_of_nodes()} applications\")\n",
    "print(f\"  Edges: {app_layer.number_of_edges()} dependencies\")\n",
    "\n",
    "print(f\"\\nInfrastructure Layer:\")\n",
    "print(f\"  Nodes: {infra_layer.number_of_nodes()} physical nodes\")\n",
    "print(f\"  Edges: {infra_layer.number_of_edges()} connections\")\n",
    "\n",
    "print(f\"\\nMessaging Layer:\")\n",
    "print(f\"  Nodes: {msg_layer.number_of_nodes()}\")\n",
    "print(f\"  Edges: {msg_layer.number_of_edges()}\")\n",
    "\n",
    "print(f\"\\nComplete System:\")\n",
    "print(f\"  Nodes: {complete_graph.number_of_nodes()}\")\n",
    "print(f\"  Edges: {complete_graph.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Criticality Analysis Engine\n",
    "\n",
    "Compute topological metrics for each layer independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticalityAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze criticality using topological metrics:\n",
    "    C_score(v) = α·C_B^norm(v) + β·AP(v) + γ·I(v)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.4, beta=0.3, gamma=0.3):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def analyze_layer(self, graph: nx.DiGraph, layer_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform comprehensive criticality analysis on a graph layer\n",
    "        \"\"\"\n",
    "        if graph.number_of_nodes() == 0:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"\\nAnalyzing {layer_name}...\")\n",
    "        \n",
    "        # Compute centrality metrics\n",
    "        centralities = self._compute_centralities(graph)\n",
    "        \n",
    "        # Find articulation points\n",
    "        articulation_points = self._find_articulation_points(graph)\n",
    "        \n",
    "        # Calculate impact scores\n",
    "        impact_scores = self._calculate_impact_scores(graph)\n",
    "        \n",
    "        # Build results DataFrame\n",
    "        results = []\n",
    "        for node in graph.nodes():\n",
    "            betweenness = centralities['betweenness'][node]\n",
    "            is_ap = node in articulation_points\n",
    "            impact = impact_scores[node]\n",
    "            \n",
    "            composite = self._calculate_composite_score(\n",
    "                betweenness, is_ap, impact\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'node': node,\n",
    "                'name': graph.nodes[node].get('name', node),\n",
    "                'type': graph.nodes[node].get('type', 'Unknown'),\n",
    "                'degree': centralities['degree'][node],\n",
    "                'betweenness': betweenness,\n",
    "                'closeness': centralities['closeness'][node],\n",
    "                'pagerank': centralities['pagerank'][node],\n",
    "                'is_articulation_point': is_ap,\n",
    "                'impact_score': impact,\n",
    "                'composite_criticality': composite,\n",
    "                'layer': layer_name\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        df = df.sort_values('composite_criticality', ascending=False)\n",
    "        \n",
    "        print(f\"  ✓ Analyzed {len(df)} components\")\n",
    "        print(f\"  ✓ Articulation points: {len(articulation_points)}\")\n",
    "        print(f\"  ✓ Avg criticality: {df['composite_criticality'].mean():.4f}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _compute_centralities(self, graph: nx.DiGraph) -> Dict:\n",
    "        \"\"\"Compute all centrality metrics\"\"\"\n",
    "        return {\n",
    "            'degree': nx.degree_centrality(graph),\n",
    "            'betweenness': nx.betweenness_centrality(graph, normalized=True),\n",
    "            'closeness': nx.closeness_centrality(graph) if nx.is_weakly_connected(graph) \n",
    "                        else {n: 0 for n in graph.nodes()},\n",
    "            'pagerank': nx.pagerank(graph, max_iter=100)\n",
    "        }\n",
    "    \n",
    "    def _find_articulation_points(self, graph: nx.DiGraph) -> Set:\n",
    "        \"\"\"Find articulation points (cut vertices)\"\"\"\n",
    "        undirected = graph.to_undirected()\n",
    "        return set(nx.articulation_points(undirected))\n",
    "    \n",
    "    def _calculate_impact_scores(self, graph: nx.DiGraph) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate impact scores: I(v) = 1 - |R(G-v)| / |R(G)|\n",
    "        Using sampling for performance\n",
    "        \"\"\"\n",
    "        original_reachable = self._count_reachable_pairs(graph)\n",
    "        \n",
    "        if original_reachable == 0:\n",
    "            return {node: 0.0 for node in graph.nodes()}\n",
    "        \n",
    "        impact_scores = {}\n",
    "        nodes = list(graph.nodes())\n",
    "        \n",
    "        # Sample nodes for large graphs\n",
    "        sample_size = min(len(nodes), 20)\n",
    "        sampled = np.random.choice(nodes, sample_size, replace=False) if len(nodes) > sample_size else nodes\n",
    "        \n",
    "        for node in sampled:\n",
    "            test_graph = graph.copy()\n",
    "            test_graph.remove_node(node)\n",
    "            new_reachable = self._count_reachable_pairs(test_graph)\n",
    "            impact_scores[node] = 1.0 - (new_reachable / original_reachable)\n",
    "        \n",
    "        # Estimate for non-sampled nodes\n",
    "        if len(nodes) > sample_size:\n",
    "            avg_impact = np.mean(list(impact_scores.values()))\n",
    "            centralities = self._compute_centralities(graph)\n",
    "            for node in graph.nodes():\n",
    "                if node not in impact_scores:\n",
    "                    impact_scores[node] = min(1.0, centralities['betweenness'][node] * avg_impact * 1.5)\n",
    "        \n",
    "        return impact_scores\n",
    "    \n",
    "    def _count_reachable_pairs(self, graph: nx.DiGraph) -> int:\n",
    "        \"\"\"Count reachable vertex pairs |R(G)|\"\"\"\n",
    "        count = 0\n",
    "        for source in graph.nodes():\n",
    "            try:\n",
    "                count += len(nx.descendants(graph, source))\n",
    "            except:\n",
    "                continue\n",
    "        return count\n",
    "    \n",
    "    def _calculate_composite_score(self, betweenness: float, \n",
    "                                   is_ap: bool, impact: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate: C_score(v) = α·C_B^norm(v) + β·AP(v) + γ·I(v)\n",
    "        \"\"\"\n",
    "        ap_value = 1.0 if is_ap else 0.0\n",
    "        score = (self.alpha * betweenness + \n",
    "                self.beta * ap_value + \n",
    "                self.gamma * impact)\n",
    "        return min(1.0, max(0.0, score))\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = CriticalityAnalyzer(alpha=0.4, beta=0.3, gamma=0.3)\n",
    "\n",
    "print(\"✓ Criticality analyzer initialized\")\n",
    "print(f\"  Weights: α={analyzer.alpha}, β={analyzer.beta}, γ={analyzer.gamma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Layer-by-Layer Analysis\n",
    "\n",
    "### 5.1 Application-Level Layer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze application layer\n",
    "app_analysis = analyzer.analyze_layer(app_layer, \"Application Layer\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"APPLICATION LAYER - CRITICAL COMPONENTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTop 5 Critical Applications:\")\n",
    "print(app_analysis[['name', 'composite_criticality', 'betweenness', \n",
    "                     'is_articulation_point', 'impact_score']].head().to_string(index=False))\n",
    "\n",
    "# Identify key dependencies\n",
    "print(\"\\nKey Insights:\")\n",
    "if len(app_analysis) > 0:\n",
    "    most_critical = app_analysis.iloc[0]\n",
    "    print(f\"  • Most critical: {most_critical['name']} (score: {most_critical['composite_criticality']:.4f})\")\n",
    "    \n",
    "    ap_count = app_analysis['is_articulation_point'].sum()\n",
    "    if ap_count > 0:\n",
    "        print(f\"  • Articulation points: {ap_count} applications are SPOFs\")\n",
    "        aps = app_analysis[app_analysis['is_articulation_point']]['name'].tolist()\n",
    "        print(f\"    → {', '.join(aps[:3])}\")\n",
    "    \n",
    "    high_impact = app_analysis[app_analysis['impact_score'] > 0.3]\n",
    "    print(f\"  • High-impact components: {len(high_impact)} applications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Infrastructure-Level Layer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze infrastructure layer\n",
    "infra_analysis = analyzer.analyze_layer(infra_layer, \"Infrastructure Layer\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INFRASTRUCTURE LAYER - CRITICAL COMPONENTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nCritical Infrastructure Nodes:\")\n",
    "print(infra_analysis[['name', 'composite_criticality', 'betweenness', \n",
    "                       'is_articulation_point', 'impact_score']].to_string(index=False))\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "if len(infra_analysis) > 0:\n",
    "    most_critical = infra_analysis.iloc[0]\n",
    "    print(f\"  • Most critical node: {most_critical['name']} (score: {most_critical['composite_criticality']:.4f})\")\n",
    "    \n",
    "    ap_count = infra_analysis['is_articulation_point'].sum()\n",
    "    if ap_count > 0:\n",
    "        print(f\"  • Infrastructure SPOFs: {ap_count} nodes\")\n",
    "        aps = infra_analysis[infra_analysis['is_articulation_point']]['name'].tolist()\n",
    "        print(f\"    → {', '.join(aps)}\")\n",
    "    \n",
    "    # Network connectivity analysis\n",
    "    avg_degree = infra_analysis['degree'].mean()\n",
    "    print(f\"  • Average node connectivity: {avg_degree:.2f}\")\n",
    "    print(f\"  • Network density: {nx.density(infra_layer):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Complete System View Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze complete system\n",
    "complete_analysis = analyzer.analyze_layer(complete_graph, \"Complete System\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLETE SYSTEM VIEW - CRITICAL COMPONENTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTop 10 Critical Components (All Layers):\")\n",
    "print(complete_analysis[['name', 'type', 'composite_criticality', 'betweenness', \n",
    "                          'is_articulation_point', 'impact_score']].head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "if len(complete_analysis) > 0:\n",
    "    # Most critical by type\n",
    "    print(\"  • Most critical components by type:\")\n",
    "    for comp_type in ['Application', 'Node', 'Broker', 'Topic']:\n",
    "        type_data = complete_analysis[complete_analysis['type'] == comp_type]\n",
    "        if len(type_data) > 0:\n",
    "            top = type_data.iloc[0]\n",
    "            print(f\"    - {comp_type}: {top['name']} (score: {top['composite_criticality']:.4f})\")\n",
    "    \n",
    "    # Articulation points\n",
    "    ap_by_type = complete_analysis[complete_analysis['is_articulation_point']].groupby('type').size()\n",
    "    if len(ap_by_type) > 0:\n",
    "        print(\"\\n  • Articulation points by type:\")\n",
    "        for comp_type, count in ap_by_type.items():\n",
    "            print(f\"    - {comp_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Layer Comparison\n",
    "\n",
    "Compare how criticality manifests differently across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all analyses\n",
    "all_analyses = pd.concat([\n",
    "    app_analysis,\n",
    "    infra_analysis,\n",
    "    complete_analysis\n",
    "], ignore_index=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CROSS-LAYER COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare criticality distributions\n",
    "print(\"\\nCriticality Statistics by Layer:\")\n",
    "layer_stats = all_analyses.groupby('layer')['composite_criticality'].agg(['mean', 'std', 'max', 'min'])\n",
    "print(layer_stats.to_string())\n",
    "\n",
    "# Compare articulation points\n",
    "print(\"\\nArticulation Points by Layer:\")\n",
    "ap_by_layer = all_analyses.groupby('layer')['is_articulation_point'].sum()\n",
    "for layer, count in ap_by_layer.items():\n",
    "    total = len(all_analyses[all_analyses['layer'] == layer])\n",
    "    percentage = (count / total * 100) if total > 0 else 0\n",
    "    print(f\"  {layer}: {count} ({percentage:.1f}% of layer)\")\n",
    "\n",
    "# Identify components critical across multiple layers\n",
    "print(\"\\nComponents Appearing in Multiple Layers:\")\n",
    "node_layer_count = all_analyses.groupby('node')['layer'].nunique()\n",
    "multi_layer = node_layer_count[node_layer_count > 1]\n",
    "if len(multi_layer) > 0:\n",
    "    for node in multi_layer.index:\n",
    "        node_data = all_analyses[all_analyses['node'] == node]\n",
    "        layers = ', '.join(node_data['layer'].unique())\n",
    "        avg_crit = node_data['composite_criticality'].mean()\n",
    "        print(f\"  {node}: {layers} (avg criticality: {avg_crit:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations\n",
    "\n",
    "### 7.1 Layer Comparison Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Criticality distribution by layer\n",
    "ax = axes[0, 0]\n",
    "layer_order = ['Application Layer', 'Infrastructure Layer', 'Complete System']\n",
    "layer_data = [all_analyses[all_analyses['layer'] == l]['composite_criticality'].values \n",
    "              for l in layer_order if l in all_analyses['layer'].values]\n",
    "layer_labels = [l.replace(' Layer', '').replace('Complete System', 'Complete') \n",
    "                for l in layer_order if l in all_analyses['layer'].values]\n",
    "ax.boxplot(layer_data, labels=layer_labels)\n",
    "ax.set_ylabel('Composite Criticality Score')\n",
    "ax.set_title('Criticality Distribution by Layer')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Top critical components comparison\n",
    "ax = axes[0, 1]\n",
    "top_app = app_analysis.head(5) if len(app_analysis) > 0 else pd.DataFrame()\n",
    "top_infra = infra_analysis.head(5) if len(infra_analysis) > 0 else pd.DataFrame()\n",
    "if len(top_app) > 0:\n",
    "    ax.barh(range(len(top_app)), top_app['composite_criticality'], \n",
    "            color='steelblue', alpha=0.7, label='Applications')\n",
    "if len(top_infra) > 0:\n",
    "    y_offset = len(top_app) if len(top_app) > 0 else 0\n",
    "    ax.barh(range(y_offset, y_offset + len(top_infra)), top_infra['composite_criticality'],\n",
    "            color='coral', alpha=0.7, label='Infrastructure')\n",
    "ax.set_xlabel('Criticality Score')\n",
    "ax.set_title('Top 5 Critical Components by Layer')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Articulation points by layer\n",
    "ax = axes[0, 2]\n",
    "ap_counts = []\n",
    "for layer in layer_order:\n",
    "    if layer in all_analyses['layer'].values:\n",
    "        count = all_analyses[all_analyses['layer'] == layer]['is_articulation_point'].sum()\n",
    "        ap_counts.append(count)\n",
    "ax.bar(layer_labels, ap_counts, color=['steelblue', 'coral', 'green'][:len(ap_counts)])\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Articulation Points by Layer')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Metric correlation - Application Layer\n",
    "ax = axes[1, 0]\n",
    "if len(app_analysis) > 0:\n",
    "    metrics = ['betweenness', 'impact_score', 'composite_criticality']\n",
    "    corr = app_analysis[metrics].corr()\n",
    "    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=ax,\n",
    "                cbar_kws={'label': 'Correlation'})\n",
    "    ax.set_title('Metric Correlation - Application Layer')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No data', ha='center', va='center')\n",
    "\n",
    "# 5. Metric correlation - Infrastructure Layer\n",
    "ax = axes[1, 1]\n",
    "if len(infra_analysis) > 0:\n",
    "    corr = infra_analysis[metrics].corr()\n",
    "    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=ax,\n",
    "                cbar_kws={'label': 'Correlation'})\n",
    "    ax.set_title('Metric Correlation - Infrastructure Layer')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No data', ha='center', va='center')\n",
    "\n",
    "# 6. Component type distribution in complete system\n",
    "ax = axes[1, 2]\n",
    "if len(complete_analysis) > 0:\n",
    "    type_criticality = complete_analysis.groupby('type')['composite_criticality'].mean().sort_values()\n",
    "    type_criticality.plot(kind='barh', ax=ax, color='teal')\n",
    "    ax.set_xlabel('Average Criticality Score')\n",
    "    ax.set_title('Average Criticality by Component Type')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('layered_analysis_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Layer comparison dashboard generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Graph Visualizations for Each Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layer_graph(graph: nx.DiGraph, analysis_df: pd.DataFrame, \n",
    "                          title: str, node_color: str = 'steelblue'):\n",
    "    \"\"\"\n",
    "    Visualize a graph layer with criticality-based node sizing\n",
    "    \"\"\"\n",
    "    if graph.number_of_nodes() == 0:\n",
    "        print(f\"No nodes to visualize for {title}\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Layout\n",
    "    if graph.number_of_nodes() <= 10:\n",
    "        pos = nx.spring_layout(graph, k=2, iterations=50, seed=42)\n",
    "    else:\n",
    "        pos = nx.kamada_kawai_layout(graph)\n",
    "    \n",
    "    # Node sizes based on criticality\n",
    "    criticality_map = dict(zip(analysis_df['node'], analysis_df['composite_criticality']))\n",
    "    node_sizes = [criticality_map.get(node, 0.1) * 2000 + 200 for node in graph.nodes()]\n",
    "    \n",
    "    # Node colors\n",
    "    node_colors_list = []\n",
    "    for node in graph.nodes():\n",
    "        if criticality_map.get(node, 0) > 0.6:\n",
    "            node_colors_list.append('#e74c3c')  # Red for high criticality\n",
    "        elif criticality_map.get(node, 0) > 0.4:\n",
    "            node_colors_list.append('#f39c12')  # Orange for medium\n",
    "        else:\n",
    "            node_colors_list.append(node_color)  # Default color\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(graph, pos, \n",
    "                          node_size=node_sizes,\n",
    "                          node_color=node_colors_list,\n",
    "                          alpha=0.8,\n",
    "                          ax=ax)\n",
    "    \n",
    "    # Highlight articulation points\n",
    "    ap_nodes = analysis_df[analysis_df['is_articulation_point']]['node'].tolist()\n",
    "    if ap_nodes:\n",
    "        ap_in_graph = [n for n in ap_nodes if n in graph.nodes()]\n",
    "        if ap_in_graph:\n",
    "            ap_sizes = [criticality_map.get(node, 0.1) * 2000 + 200 for node in ap_in_graph]\n",
    "            nx.draw_networkx_nodes(graph, pos,\n",
    "                                  nodelist=ap_in_graph,\n",
    "                                  node_size=ap_sizes,\n",
    "                                  node_color='none',\n",
    "                                  edgecolors='darkred',\n",
    "                                  linewidths=4,\n",
    "                                  ax=ax)\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(graph, pos,\n",
    "                          edge_color='gray',\n",
    "                          alpha=0.5,\n",
    "                          arrows=True,\n",
    "                          arrowsize=15,\n",
    "                          arrowstyle='-|>',\n",
    "                          width=2,\n",
    "                          ax=ax)\n",
    "    \n",
    "    # Labels for top nodes\n",
    "    top_nodes = analysis_df.head(min(8, len(analysis_df)))['node'].tolist()\n",
    "    labels = {node: graph.nodes[node].get('name', node) for node in top_nodes if node in graph.nodes()}\n",
    "    nx.draw_networkx_labels(graph, pos, labels, font_size=9, font_weight='bold', ax=ax)\n",
    "    \n",
    "    # Legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#e74c3c', label='High Criticality (>0.6)'),\n",
    "        Patch(facecolor='#f39c12', label='Medium Criticality (0.4-0.6)'),\n",
    "        Patch(facecolor=node_color, label='Low Criticality (<0.4)'),\n",
    "        Patch(facecolor='white', edgecolor='darkred', linewidth=2, label='Articulation Point')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper left', fontsize=9)\n",
    "    \n",
    "    ax.set_title(f'{title}\\n(Node size ∝ Criticality)', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = title.replace(' ', '_').lower() + '.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Saved {filename}\")\n",
    "\n",
    "# Visualize each layer\n",
    "print(\"\\nGenerating layer visualizations...\\n\")\n",
    "\n",
    "visualize_layer_graph(app_layer, app_analysis, \n",
    "                     \"Application Layer - Dependency Graph\", '#3498db')\n",
    "\n",
    "visualize_layer_graph(infra_layer, infra_analysis, \n",
    "                     \"Infrastructure Layer - Network Topology\", '#95a5a6')\n",
    "\n",
    "visualize_layer_graph(complete_graph, complete_analysis, \n",
    "                     \"Complete System View - Multi-Layer Graph\", '#2ecc71')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cross-Layer Impact Analysis\n",
    "\n",
    "Analyze how failures propagate across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cross_layer_impact(complete_graph: nx.DiGraph, \n",
    "                               complete_analysis: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze how component failures impact other layers\n",
    "    \"\"\"\n",
    "    cross_impacts = []\n",
    "    \n",
    "    # For each infrastructure node, find affected applications\n",
    "    for node, data in complete_graph.nodes(data=True):\n",
    "        if data.get('type') != 'Node':\n",
    "            continue\n",
    "        \n",
    "        # Find applications running on this node\n",
    "        affected_apps = []\n",
    "        for src, tgt, edge_data in complete_graph.in_edges(node, data=True):\n",
    "            if (edge_data.get('type') == 'RUNS_ON' and \n",
    "                complete_graph.nodes[src].get('type') == 'Application'):\n",
    "                affected_apps.append(src)\n",
    "        \n",
    "        # Find brokers on this node\n",
    "        affected_brokers = []\n",
    "        for src, tgt, edge_data in complete_graph.in_edges(node, data=True):\n",
    "            if (edge_data.get('type') == 'RUNS_ON' and \n",
    "                complete_graph.nodes[src].get('type') == 'Broker'):\n",
    "                affected_brokers.append(src)\n",
    "        \n",
    "        # Calculate cascading impact\n",
    "        cascading_apps = set()\n",
    "        for app in affected_apps:\n",
    "            # Find apps that depend on this app\n",
    "            for dependent, _, edge_data in complete_graph.in_edges(app, data=True):\n",
    "                if edge_data.get('type') == 'DEPENDS_ON':\n",
    "                    cascading_apps.add(dependent)\n",
    "        \n",
    "        node_crit = complete_analysis[complete_analysis['node'] == node]['composite_criticality'].iloc[0] if len(complete_analysis[complete_analysis['node'] == node]) > 0 else 0\n",
    "        \n",
    "        cross_impacts.append({\n",
    "            'infrastructure_node': node,\n",
    "            'infrastructure_criticality': node_crit,\n",
    "            'directly_affected_apps': len(affected_apps),\n",
    "            'affected_brokers': len(affected_brokers),\n",
    "            'cascading_apps': len(cascading_apps),\n",
    "            'total_app_impact': len(affected_apps) + len(cascading_apps),\n",
    "            'cross_layer_risk_score': (len(affected_apps) + len(affected_brokers) * 2 + len(cascading_apps)) * node_crit\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(cross_impacts)\n",
    "    df = df.sort_values('cross_layer_risk_score', ascending=False)\n",
    "    return df\n",
    "\n",
    "# Perform cross-layer analysis\n",
    "cross_layer_df = analyze_cross_layer_impact(complete_graph, complete_analysis)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CROSS-LAYER IMPACT ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nInfrastructure nodes ranked by cross-layer risk:\")\n",
    "print(cross_layer_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "if len(cross_layer_df) > 0:\n",
    "    highest_risk = cross_layer_df.iloc[0]\n",
    "    print(f\"  • Highest risk node: {highest_risk['infrastructure_node']}\")\n",
    "    print(f\"    - Directly affects {highest_risk['directly_affected_apps']} applications\")\n",
    "    print(f\"    - Cascading impact on {highest_risk['cascading_apps']} more applications\")\n",
    "    print(f\"    - Total application impact: {highest_risk['total_app_impact']}\")\n",
    "    print(f\"    - Cross-layer risk score: {highest_risk['cross_layer_risk_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Report\n",
    "\n",
    "Generate comprehensive findings across all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LAYERED CRITICALITY ANALYSIS - COMPREHENSIVE REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. EXECUTIVE SUMMARY\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Analysis performed on {complete_graph.number_of_nodes()} components across 3 layers\")\n",
    "print(f\"Total dependencies: {complete_graph.number_of_edges()}\")\n",
    "\n",
    "print(\"\\n2. LAYER STATISTICS\")\n",
    "print(\"-\" * 80)\n",
    "for layer_name, layer_graph, layer_df in [\n",
    "    ('Application Layer', app_layer, app_analysis),\n",
    "    ('Infrastructure Layer', infra_layer, infra_analysis),\n",
    "    ('Complete System', complete_graph, complete_analysis)\n",
    "]:\n",
    "    print(f\"\\n{layer_name}:\")\n",
    "    print(f\"  Components: {layer_graph.number_of_nodes()}\")\n",
    "    print(f\"  Relationships: {layer_graph.number_of_edges()}\")\n",
    "    if len(layer_df) > 0:\n",
    "        print(f\"  Avg Criticality: {layer_df['composite_criticality'].mean():.4f}\")\n",
    "        print(f\"  Max Criticality: {layer_df['composite_criticality'].max():.4f}\")\n",
    "        print(f\"  Articulation Points: {layer_df['is_articulation_point'].sum()}\")\n",
    "        most_critical = layer_df.iloc[0]\n",
    "        print(f\"  Most Critical: {most_critical['name']} ({most_critical['composite_criticality']:.4f})\")\n",
    "\n",
    "print(\"\\n3. CRITICAL COMPONENTS BY LAYER\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if len(app_analysis) > 0:\n",
    "    print(\"\\nApplication Layer (Top 3):\")\n",
    "    for i, row in app_analysis.head(3).iterrows():\n",
    "        ap_marker = \"[AP]\" if row['is_articulation_point'] else \"\"\n",
    "        print(f\"  {i+1}. {row['name']} {ap_marker}\")\n",
    "        print(f\"     Score: {row['composite_criticality']:.4f} | Impact: {row['impact_score']:.4f}\")\n",
    "\n",
    "if len(infra_analysis) > 0:\n",
    "    print(\"\\nInfrastructure Layer (All Nodes):\")\n",
    "    for i, row in infra_analysis.iterrows():\n",
    "        ap_marker = \"[AP]\" if row['is_articulation_point'] else \"\"\n",
    "        print(f\"  {i+1}. {row['name']} {ap_marker}\")\n",
    "        print(f\"     Score: {row['composite_criticality']:.4f} | Betweenness: {row['betweenness']:.4f}\")\n",
    "\n",
    "print(\"\\n4. CROSS-LAYER VULNERABILITIES\")\n",
    "print(\"-\" * 80)\n",
    "if len(cross_layer_df) > 0:\n",
    "    print(\"\\nInfrastructure nodes with highest cross-layer risk:\")\n",
    "    for i, row in cross_layer_df.head(3).iterrows():\n",
    "        print(f\"\\n  {row['infrastructure_node']}:\")\n",
    "        print(f\"    • Direct app impact: {row['directly_affected_apps']}\")\n",
    "        print(f\"    • Cascading impact: {row['cascading_apps']} apps\")\n",
    "        print(f\"    • Risk score: {row['cross_layer_risk_score']:.4f}\")\n",
    "\n",
    "print(\"\\n5. KEY RECOMMENDATIONS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Application layer recommendations\n",
    "if len(app_analysis) > 0:\n",
    "    app_aps = app_analysis[app_analysis['is_articulation_point']]\n",
    "    if len(app_aps) > 0:\n",
    "        recommendations.append(\n",
    "            f\"• CRITICAL: Implement redundancy for {len(app_aps)} application-level SPOFs: \"\n",
    "            f\"{', '.join(app_aps['name'].head(3).tolist())}\"\n",
    "        )\n",
    "\n",
    "# Infrastructure recommendations  \n",
    "if len(infra_analysis) > 0:\n",
    "    infra_aps = infra_analysis[infra_analysis['is_articulation_point']]\n",
    "    if len(infra_aps) > 0:\n",
    "        recommendations.append(\n",
    "            f\"• CRITICAL: Add network redundancy for infrastructure SPOFs: \"\n",
    "            f\"{', '.join(infra_aps['name'].tolist())}\"\n",
    "        )\n",
    "\n",
    "# Cross-layer recommendations\n",
    "if len(cross_layer_df) > 0:\n",
    "    high_risk = cross_layer_df[cross_layer_df['cross_layer_risk_score'] > \n",
    "                                cross_layer_df['cross_layer_risk_score'].quantile(0.75)]\n",
    "    if len(high_risk) > 0:\n",
    "        recommendations.append(\n",
    "            f\"• HIGH PRIORITY: Distribute applications across nodes to reduce concentration on: \"\n",
    "            f\"{', '.join(high_risk['infrastructure_node'].head(2).tolist())}\"\n",
    "        )\n",
    "\n",
    "# General recommendations\n",
    "recommendations.extend([\n",
    "    \"• Establish automated failover mechanisms for all articulation points\",\n",
    "    \"• Monitor high-criticality components with enhanced observability\",\n",
    "    \"• Conduct regular disaster recovery drills for top 5 critical components\",\n",
    "    \"• Review and optimize application dependencies to reduce coupling\"\n",
    "])\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"\\n{rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"END OF REPORT\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all analyses\n",
    "app_analysis.to_csv('application_layer_analysis.csv', index=False)\n",
    "infra_analysis.to_csv('infrastructure_layer_analysis.csv', index=False)\n",
    "complete_analysis.to_csv('complete_system_analysis.csv', index=False)\n",
    "cross_layer_df.to_csv('cross_layer_impact.csv', index=False)\n",
    "\n",
    "# Export summary JSON\n",
    "summary = {\n",
    "    'layers': {\n",
    "        'application': {\n",
    "            'nodes': app_layer.number_of_nodes(),\n",
    "            'edges': app_layer.number_of_edges(),\n",
    "            'avg_criticality': float(app_analysis['composite_criticality'].mean()) if len(app_analysis) > 0 else 0,\n",
    "            'articulation_points': int(app_analysis['is_articulation_point'].sum()) if len(app_analysis) > 0 else 0\n",
    "        },\n",
    "        'infrastructure': {\n",
    "            'nodes': infra_layer.number_of_nodes(),\n",
    "            'edges': infra_layer.number_of_edges(),\n",
    "            'avg_criticality': float(infra_analysis['composite_criticality'].mean()) if len(infra_analysis) > 0 else 0,\n",
    "            'articulation_points': int(infra_analysis['is_articulation_point'].sum()) if len(infra_analysis) > 0 else 0\n",
    "        },\n",
    "        'complete': {\n",
    "            'nodes': complete_graph.number_of_nodes(),\n",
    "            'edges': complete_graph.number_of_edges(),\n",
    "            'avg_criticality': float(complete_analysis['composite_criticality'].mean()),\n",
    "            'articulation_points': int(complete_analysis['is_articulation_point'].sum())\n",
    "        }\n",
    "    },\n",
    "    'top_critical_by_layer': {\n",
    "        'application': app_analysis.head(5)[['name', 'composite_criticality']].to_dict('records') if len(app_analysis) > 0 else [],\n",
    "        'infrastructure': infra_analysis.head(5)[['name', 'composite_criticality']].to_dict('records') if len(infra_analysis) > 0 else [],\n",
    "        'complete': complete_analysis.head(10)[['name', 'type', 'composite_criticality']].to_dict('records')\n",
    "    },\n",
    "    'cross_layer_risks': cross_layer_df.head(5).to_dict('records') if len(cross_layer_df) > 0 else []\n",
    "}\n",
    "\n",
    "with open('layered_analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ Results exported successfully:\")\n",
    "print(\"  - application_layer_analysis.csv\")\n",
    "print(\"  - infrastructure_layer_analysis.csv\")\n",
    "print(\"  - complete_system_analysis.csv\")\n",
    "print(\"  - cross_layer_impact.csv\")\n",
    "print(\"  - layered_analysis_summary.json\")\n",
    "print(\"  - layered_analysis_dashboard.png\")\n",
    "print(\"  - application_layer_-_dependency_graph.png\")\n",
    "print(\"  - infrastructure_layer_-_network_topology.png\")\n",
    "print(\"  - complete_system_view_-_multi-layer_graph.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This layered analysis reveals how criticality manifests differently across architectural perspectives:\n",
    "\n",
    "### **Key Insights**\n",
    "\n",
    "1. **Application Layer**: Identifies logical dependencies and application-level single points of failure through DEPENDS_ON relationships\n",
    "\n",
    "2. **Infrastructure Layer**: Reveals network topology vulnerabilities and physical connectivity bottlenecks through CONNECTS_TO relationships\n",
    "\n",
    "3. **Complete System**: Provides holistic view showing how logical and physical layers interact, revealing cross-layer vulnerabilities\n",
    "\n",
    "4. **Cross-Layer Impact**: Quantifies cascading failure risks showing how infrastructure failures propagate to applications\n",
    "\n",
    "### **Practical Applications**\n",
    "\n",
    "- **Application Layer**: Guide service mesh design, API gateway placement, circuit breaker configuration\n",
    "- **Infrastructure Layer**: Inform data center topology, network redundancy, disaster recovery planning\n",
    "- **Complete System**: Comprehensive risk assessment, capacity planning, incident response prioritization\n",
    "- **Cross-Layer**: Resource allocation, workload distribution, fault isolation strategies\n",
    "\n",
    "By analyzing each layer independently and then synthesizing findings, this approach provides actionable insights for building resilient distributed systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
