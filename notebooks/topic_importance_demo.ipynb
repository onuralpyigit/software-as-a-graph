{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated Topology + Topic Importance Analysis\n",
    "\n",
    "## Complete Methodology\n",
    "\n",
    "This approach integrates **topological metrics** with **topic importance** for comprehensive criticality assessment:\n",
    "\n",
    "### **1. Topic Importance Propagation**\n",
    "\n",
    "Topic importance (TI) derived from QoS policies propagates through the system:\n",
    "\n",
    "- **Topics**: $TI_{topic} = 0.6 \\cdot \\text{Durability} + 0.4 \\cdot \\text{Reliability}$\n",
    "- **Applications**: $TI_{app} = 0.7 \\cdot \\text{avg}(\\text{published topics}) + 0.3 \\cdot \\text{avg}(\\text{subscribed topics})$\n",
    "- **Brokers**: $TI_{broker} = \\text{avg}(\\text{routed topics})$\n",
    "- **Nodes**: $TI_{node} = 0.6 \\cdot \\text{avg}(\\text{hosted brokers}) + 0.4 \\cdot \\text{avg}(\\text{hosted apps})$\n",
    "\n",
    "### **2. Composite Criticality Score**\n",
    "\n",
    "$$C_{\\text{score}}(v) = C_{\\text{base}}(v) \\times (1 + \\gamma \\cdot TI(v))$$\n",
    "\n",
    "Where:\n",
    "- $C_{\\text{base}}(v) = \\alpha \\cdot C_B^{\\text{norm}}(v) + \\beta \\cdot AP(v)$\n",
    "- $\\gamma = 0.5$ (importance amplification factor)\n",
    "\n",
    "### **3. Weighted Impact Score**\n",
    "\n",
    "$$I_{\\text{weighted}}(v) = I_{\\text{base}}(v) \\times (1 + \\delta \\cdot TI(v))$$\n",
    "\n",
    "Where:\n",
    "- $I_{\\text{base}}(v) = 1 - \\frac{|R(G-v)|}{|R(G)|}$\n",
    "- $\\delta = 0.5$ (importance amplification factor)\n",
    "\n",
    "### **4. Validation**\n",
    "\n",
    "Correlation between $C_{\\text{score}}(v)$ and $I_{\\text{weighted}}(v)$ validates the approach.\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Integration Makes Sense\n",
    "\n",
    "1. **Structural + Business Value**: Combines graph position with business importance\n",
    "2. **Propagation**: Importance flows through the dependency graph naturally\n",
    "3. **Amplification**: High-importance components get higher criticality scores\n",
    "4. **Validation**: Both prediction and ground truth consider importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from typing import Dict, Set\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 12)\n",
    "\n",
    "print(\"âœ“ Ready for integrated analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Realistic System with QoS Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_with_qos() -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Create pub-sub system with realistic QoS policies\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Physical nodes\n",
    "    for i in range(1, 6):\n",
    "        G.add_node(f'Node{i}', type='Node', name=f'Node{i}')\n",
    "    \n",
    "    # Node connectivity\n",
    "    connections = [(1,2), (2,3), (3,4), (4,5), (1,5), (2,4)]\n",
    "    for i, j in connections:\n",
    "        G.add_edge(f'Node{i}', f'Node{j}', type='CONNECTS_TO')\n",
    "        G.add_edge(f'Node{j}', f'Node{i}', type='CONNECTS_TO')\n",
    "    \n",
    "    # Brokers\n",
    "    G.add_node('Broker1', type='Broker', name='MainBroker')\n",
    "    G.add_edge('Broker1', 'Node1', type='RUNS_ON')\n",
    "    G.add_node('Broker2', type='Broker', name='BackupBroker')\n",
    "    G.add_edge('Broker2', 'Node2', type='RUNS_ON')\n",
    "    \n",
    "    # Topics with varying QoS importance\n",
    "    topics = [\n",
    "        # (id, name, durability, reliability, expected_importance)\n",
    "        ('T1', 'payment/transactions', 'PERSISTENT', 'RELIABLE', 1.0),  # Critical\n",
    "        ('T2', 'order/processing', 'PERSISTENT', 'RELIABLE', 1.0),      # Critical\n",
    "        ('T3', 'user/events', 'TRANSIENT', 'RELIABLE', 0.75),           # Important\n",
    "        ('T4', 'metrics/system', 'VOLATILE', 'BEST_EFFORT', 0.24),      # Low priority\n",
    "        ('T5', 'inventory/updates', 'TRANSIENT', 'RELIABLE', 0.75),     # Important\n",
    "        ('T6', 'logs/debug', 'VOLATILE', 'BEST_EFFORT', 0.24),          # Low priority\n",
    "    ]\n",
    "    \n",
    "    for tid, name, dur, rel, _ in topics:\n",
    "        G.add_node(tid, type='Topic', name=name, durability=dur, reliability=rel)\n",
    "        broker = 'Broker1' if int(tid[1]) <= 3 else 'Broker2'\n",
    "        G.add_edge(broker, tid, type='ROUTES')\n",
    "    \n",
    "    # Applications\n",
    "    apps = [\n",
    "        # Critical services (handle important topics)\n",
    "        ('PaymentService', 'Node1', ['T1'], ['T2', 'T5']),\n",
    "        ('OrderService', 'Node2', ['T2'], ['T1', 'T5']),\n",
    "        ('InventoryService', 'Node4', ['T5'], ['T2']),\n",
    "        \n",
    "        # Important services\n",
    "        ('UserService', 'Node2', ['T3'], ['T1', 'T2']),\n",
    "        ('NotificationService', 'Node3', [], ['T1', 'T2', 'T3']),\n",
    "        \n",
    "        # Monitoring services (low importance topics)\n",
    "        ('MetricsCollector', 'Node3', ['T4'], ['T1', 'T2', 'T3', 'T5']),\n",
    "        ('LogAggregator', 'Node5', ['T6'], ['T4']),\n",
    "        ('AnalyticsEngine', 'Node5', [], ['T4', 'T6']),\n",
    "    ]\n",
    "    \n",
    "    for app, node, pubs, subs in apps:\n",
    "        G.add_node(app, type='Application', name=app)\n",
    "        G.add_edge(app, node, type='RUNS_ON')\n",
    "        for t in pubs:\n",
    "            G.add_edge(app, t, type='PUBLISHES_TO')\n",
    "        for t in subs:\n",
    "            G.add_edge(app, t, type='SUBSCRIBES_TO')\n",
    "    \n",
    "    # Create DEPENDS_ON relationships\n",
    "    for app1, _, pubs, _ in apps:\n",
    "        for topic in pubs:\n",
    "            for app2, _, _, subs in apps:\n",
    "                if app1 != app2 and topic in subs:\n",
    "                    G.add_edge(app2, app1, type='DEPENDS_ON')\n",
    "    \n",
    "    return G\n",
    "\n",
    "G = create_system_with_qos()\n",
    "\n",
    "print(\"System Created:\")\n",
    "print(f\"  Components: {G.number_of_nodes()}\")\n",
    "print(f\"  Relationships: {G.number_of_edges()}\")\n",
    "type_counts = {}\n",
    "for _, data in G.nodes(data=True):\n",
    "    t = data['type']\n",
    "    type_counts[t] = type_counts.get(t, 0) + 1\n",
    "for t, c in sorted(type_counts.items()):\n",
    "    print(f\"    {t}: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Phase 1: Calculate Topic Importance (QoS-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_topic_importance(topic_data: Dict) -> float:\n",
    "    \"\"\"\n",
    "    Calculate topic importance from QoS policies\n",
    "    TI = 0.6 * durability_score + 0.4 * reliability_score\n",
    "    \"\"\"\n",
    "    durability_scores = {\n",
    "        'VOLATILE': 0.2,\n",
    "        'TRANSIENT_LOCAL': 0.5,\n",
    "        'TRANSIENT': 0.75,\n",
    "        'PERSISTENT': 1.0\n",
    "    }\n",
    "    \n",
    "    reliability_scores = {\n",
    "        'BEST_EFFORT': 0.3,\n",
    "        'RELIABLE': 1.0\n",
    "    }\n",
    "    \n",
    "    dur = topic_data.get('durability', 'VOLATILE')\n",
    "    rel = topic_data.get('reliability', 'BEST_EFFORT')\n",
    "    \n",
    "    return 0.6 * durability_scores[dur] + 0.4 * reliability_scores[rel]\n",
    "\n",
    "# Calculate for all topics\n",
    "topic_importance = {}\n",
    "for node, data in G.nodes(data=True):\n",
    "    if data['type'] == 'Topic':\n",
    "        topic_importance[node] = calculate_topic_importance(data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1: TOPIC IMPORTANCE (FROM QoS POLICIES)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTopics ranked by importance:\")\n",
    "topic_df = pd.DataFrame([\n",
    "    {'topic': node, 'name': G.nodes[node]['name'], \n",
    "     'durability': G.nodes[node]['durability'],\n",
    "     'reliability': G.nodes[node]['reliability'],\n",
    "     'importance': topic_importance[node]}\n",
    "    for node in topic_importance.keys()\n",
    "]).sort_values('importance', ascending=False)\n",
    "\n",
    "print(topic_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Phase 2: Propagate Importance to All Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_importance_to_all(graph: nx.DiGraph, \n",
    "                                topic_importance: Dict[str, float]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Propagate topic importance through the entire system\n",
    "    \"\"\"\n",
    "    all_importance = topic_importance.copy()\n",
    "    \n",
    "    # Phase 2a: Applications\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        if data['type'] != 'Application':\n",
    "            continue\n",
    "        \n",
    "        pubs = []\n",
    "        subs = []\n",
    "        for _, target, edge_data in graph.out_edges(node, data=True):\n",
    "            if graph.nodes[target]['type'] == 'Topic':\n",
    "                if edge_data['type'] == 'PUBLISHES_TO':\n",
    "                    pubs.append(target)\n",
    "                elif edge_data['type'] == 'SUBSCRIBES_TO':\n",
    "                    subs.append(target)\n",
    "        \n",
    "        pub_scores = [topic_importance.get(t, 0.5) for t in pubs]\n",
    "        sub_scores = [topic_importance.get(t, 0.5) for t in subs]\n",
    "        \n",
    "        if pub_scores and sub_scores:\n",
    "            all_importance[node] = 0.7 * np.mean(pub_scores) + 0.3 * np.mean(sub_scores)\n",
    "        elif pub_scores:\n",
    "            all_importance[node] = np.mean(pub_scores)\n",
    "        elif sub_scores:\n",
    "            all_importance[node] = np.mean(sub_scores)\n",
    "        else:\n",
    "            all_importance[node] = 0.5\n",
    "    \n",
    "    # Phase 2b: Brokers\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        if data['type'] != 'Broker':\n",
    "            continue\n",
    "        \n",
    "        routed = []\n",
    "        for _, target, edge_data in graph.out_edges(node, data=True):\n",
    "            if edge_data['type'] == 'ROUTES':\n",
    "                routed.append(target)\n",
    "        \n",
    "        if routed:\n",
    "            scores = [topic_importance.get(t, 0.5) for t in routed]\n",
    "            all_importance[node] = np.mean(scores)\n",
    "        else:\n",
    "            all_importance[node] = 0.5\n",
    "    \n",
    "    # Phase 2c: Nodes\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        if data['type'] != 'Node':\n",
    "            continue\n",
    "        \n",
    "        hosted = []\n",
    "        for source, _, edge_data in graph.in_edges(node, data=True):\n",
    "            if edge_data['type'] == 'RUNS_ON':\n",
    "                hosted.append(source)\n",
    "        \n",
    "        if hosted:\n",
    "            broker_scores = [all_importance[h] for h in hosted if graph.nodes[h]['type'] == 'Broker']\n",
    "            app_scores = [all_importance[h] for h in hosted if graph.nodes[h]['type'] == 'Application']\n",
    "            \n",
    "            if broker_scores and app_scores:\n",
    "                all_importance[node] = 0.6 * np.mean(broker_scores) + 0.4 * np.mean(app_scores)\n",
    "            elif broker_scores:\n",
    "                all_importance[node] = np.mean(broker_scores)\n",
    "            elif app_scores:\n",
    "                all_importance[node] = np.mean(app_scores)\n",
    "            else:\n",
    "                all_importance[node] = 0.5\n",
    "        else:\n",
    "            all_importance[node] = 0.5\n",
    "    \n",
    "    return all_importance\n",
    "\n",
    "# Propagate importance\n",
    "all_importance = propagate_importance_to_all(G, topic_importance)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2: IMPORTANCE PROPAGATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for comp_type in ['Topic', 'Application', 'Broker', 'Node']:\n",
    "    comp_data = [(node, all_importance[node]) \n",
    "                 for node, data in G.nodes(data=True) \n",
    "                 if data['type'] == comp_type]\n",
    "    if comp_data:\n",
    "        print(f\"\\n{comp_type}s by importance:\")\n",
    "        comp_data.sort(key=lambda x: x[1], reverse=True)\n",
    "        for node, imp in comp_data:\n",
    "            print(f\"  {G.nodes[node]['name']:25s} â†’ {imp:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phase 3: Calculate Base Topology Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_base_topology_scores(graph: nx.DiGraph, \n",
    "                                   alpha: float = 0.6,\n",
    "                                   beta: float = 0.4) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate base topology scores (pure structure)\n",
    "    C_base = Î±Â·betweenness + Î²Â·AP\n",
    "    \"\"\"\n",
    "    # Compute metrics\n",
    "    betweenness = nx.betweenness_centrality(graph, normalized=True)\n",
    "    undirected = graph.to_undirected()\n",
    "    aps = set(nx.articulation_points(undirected))\n",
    "    \n",
    "    # Calculate base scores\n",
    "    base_scores = {}\n",
    "    for node in graph.nodes():\n",
    "        ap_value = 1.0 if node in aps else 0.0\n",
    "        base_scores[node] = alpha * betweenness[node] + beta * ap_value\n",
    "    \n",
    "    return base_scores, betweenness, aps\n",
    "\n",
    "base_topology, betweenness, aps = calculate_base_topology_scores(G)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 3: BASE TOPOLOGY SCORES (PURE STRUCTURE)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFormula: C_base = 0.6Â·betweenness + 0.4Â·AP\")\n",
    "print(f\"Articulation points found: {len(aps)}\")\n",
    "if aps:\n",
    "    print(f\"  â†’ {', '.join([G.nodes[n]['name'] for n in aps])}\")\n",
    "\n",
    "print(\"\\nTop 10 by base topology score:\")\n",
    "sorted_base = sorted(base_topology.items(), key=lambda x: x[1], reverse=True)\n",
    "for node, score in sorted_base[:10]:\n",
    "    is_ap = \"[AP]\" if node in aps else \"\"\n",
    "    print(f\"  {G.nodes[node]['name']:25s} {is_ap:5s} â†’ {score:.4f} (betweenness: {betweenness[node]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase 4: Integrate Importance into Topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_importance_topology(base_scores: Dict[str, float],\n",
    "                                 importance: Dict[str, float],\n",
    "                                 gamma: float = 0.5) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Integrate topic importance into topology scores\n",
    "    C_score = C_base Ã— (1 + Î³Â·TI)\n",
    "    \"\"\"\n",
    "    composite = {}\n",
    "    for node in base_scores:\n",
    "        ti = importance.get(node, 0.5)\n",
    "        composite[node] = base_scores[node] * (1 + gamma * ti)\n",
    "    return composite\n",
    "\n",
    "composite_scores = integrate_importance_topology(base_topology, all_importance)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 4: COMPOSITE SCORES (TOPOLOGY Ã— IMPORTANCE)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFormula: C_score = C_base Ã— (1 + 0.5Â·TI)\")\n",
    "print(\"\\nTop 10 critical components:\")\n",
    "\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'name': G.nodes[node]['name'],\n",
    "        'type': G.nodes[node]['type'],\n",
    "        'base_topology': base_topology[node],\n",
    "        'topic_importance': all_importance[node],\n",
    "        'composite_score': composite_scores[node],\n",
    "        'amplification': composite_scores[node] / base_topology[node] if base_topology[node] > 0 else 1.0\n",
    "    }\n",
    "    for node in G.nodes()\n",
    "]).sort_values('composite_score', ascending=False)\n",
    "\n",
    "print(comparison_df.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ“Š Key Insight: Components with high importance get amplified scores!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Phase 5: Calculate Impact Scores with Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_reachable_pairs(graph: nx.DiGraph) -> int:\n",
    "    \"\"\"Count |R(G)|\"\"\"\n",
    "    count = 0\n",
    "    for source in graph.nodes():\n",
    "        try:\n",
    "            count += len(nx.descendants(graph, source))\n",
    "        except:\n",
    "            continue\n",
    "    return count\n",
    "\n",
    "def calculate_base_impact_scores(graph: nx.DiGraph) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate base impact scores\n",
    "    I_base = 1 - |R(G-v)|/|R(G)|\n",
    "    \"\"\"\n",
    "    print(\"Computing base impact scores...\")\n",
    "    original = count_reachable_pairs(graph)\n",
    "    \n",
    "    if original == 0:\n",
    "        return {n: 0.0 for n in graph.nodes()}\n",
    "    \n",
    "    impacts = {}\n",
    "    for i, node in enumerate(graph.nodes()):\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"  Progress: {i+1}/{graph.number_of_nodes()}\")\n",
    "        \n",
    "        test = graph.copy()\n",
    "        test.remove_node(node)\n",
    "        new = count_reachable_pairs(test)\n",
    "        impacts[node] = 1.0 - (new / original)\n",
    "    \n",
    "    return impacts\n",
    "\n",
    "def integrate_importance_impact(base_impact: Dict[str, float],\n",
    "                               importance: Dict[str, float],\n",
    "                               delta: float = 0.5) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Integrate importance into impact scores\n",
    "    I_weighted = I_base Ã— (1 + Î´Â·TI)\n",
    "    \"\"\"\n",
    "    weighted = {}\n",
    "    for node in base_impact:\n",
    "        ti = importance.get(node, 0.5)\n",
    "        weighted[node] = base_impact[node] * (1 + delta * ti)\n",
    "    return weighted\n",
    "\n",
    "# Calculate impact scores\n",
    "base_impact = calculate_base_impact_scores(G)\n",
    "weighted_impact = integrate_importance_impact(base_impact, all_importance)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 5: IMPACT SCORES (REACHABILITY Ã— IMPORTANCE)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFormula: I_weighted = I_base Ã— (1 + 0.5Â·TI)\")\n",
    "print(\"\\nTop 10 by weighted impact:\")\n",
    "\n",
    "impact_df = pd.DataFrame([\n",
    "    {\n",
    "        'name': G.nodes[node]['name'],\n",
    "        'type': G.nodes[node]['type'],\n",
    "        'base_impact': base_impact[node],\n",
    "        'topic_importance': all_importance[node],\n",
    "        'weighted_impact': weighted_impact[node]\n",
    "    }\n",
    "    for node in G.nodes()\n",
    "]).sort_values('weighted_impact', ascending=False)\n",
    "\n",
    "print(impact_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Phase 6: Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive dataframe\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'node': node,\n",
    "        'name': G.nodes[node]['name'],\n",
    "        'type': G.nodes[node]['type'],\n",
    "        'topic_importance': all_importance[node],\n",
    "        'base_topology': base_topology[node],\n",
    "        'composite_score': composite_scores[node],\n",
    "        'base_impact': base_impact[node],\n",
    "        'weighted_impact': weighted_impact[node],\n",
    "        'is_ap': node in aps\n",
    "    }\n",
    "    for node in G.nodes()\n",
    "])\n",
    "\n",
    "# Validate\n",
    "pearson_r, _ = pearsonr(results_df['composite_score'], results_df['weighted_impact'])\n",
    "spearman_rho, _ = spearmanr(results_df['composite_score'], results_df['weighted_impact'])\n",
    "\n",
    "k = 5\n",
    "top_k_pred = set(results_df.nlargest(k, 'composite_score')['node'])\n",
    "top_k_actual = set(results_df.nlargest(k, 'weighted_impact')['node'])\n",
    "accuracy = len(top_k_pred & top_k_actual) / k\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 6: VALIDATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. CORRELATION ANALYSIS\")\n",
    "print(f\"   Pearson correlation:  r = {pearson_r:.4f}\")\n",
    "print(f\"   Spearman correlation: Ï = {spearman_rho:.4f}\")\n",
    "\n",
    "if pearson_r > 0.7:\n",
    "    strength = \"STRONG âœ“\"\n",
    "elif pearson_r > 0.5:\n",
    "    strength = \"MODERATE âœ“\"\n",
    "else:\n",
    "    strength = \"WEAK âš \"\n",
    "print(f\"   â†’ {strength}\")\n",
    "\n",
    "print(\"\\n2. PREDICTION ACCURACY\")\n",
    "print(f\"   Top-5 overlap: {accuracy:.1%}\")\n",
    "\n",
    "print(\"\\n3. COMPARISON: Composite Score vs. Weighted Impact\")\n",
    "comparison = results_df[['name', 'type', 'composite_score', 'weighted_impact']].copy()\n",
    "comparison['rank_composite'] = comparison['composite_score'].rank(ascending=False)\n",
    "comparison['rank_impact'] = comparison['weighted_impact'].rank(ascending=False)\n",
    "comparison['rank_diff'] = abs(comparison['rank_composite'] - comparison['rank_impact'])\n",
    "print(comparison.nsmallest(10, 'rank_composite')[['name', 'type', 'composite_score', \n",
    "                                                   'weighted_impact', 'rank_diff']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comprehensive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 14))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Importance propagation flow\n",
    "ax = fig.add_subplot(gs[0, :])\n",
    "for comp_type, color in [('Topic', '#2ecc71'), ('Application', '#3498db'), \n",
    "                         ('Broker', '#e74c3c'), ('Node', '#95a5a6')]:\n",
    "    data = results_df[results_df['type'] == comp_type]\n",
    "    if len(data) > 0:\n",
    "        ax.scatter(range(len(data)), data['topic_importance'], \n",
    "                  s=200, alpha=0.7, c=color, label=comp_type, edgecolors='black')\n",
    "ax.set_ylabel('Topic Importance', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Topic Importance Propagation Across All Components', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks([])\n",
    "\n",
    "# 2. Base vs Composite scores\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "top10 = results_df.nlargest(10, 'composite_score')\n",
    "x = np.arange(len(top10))\n",
    "width = 0.35\n",
    "ax.barh(x - width/2, top10['base_topology'], width, label='Base Topology', color='steelblue', alpha=0.8)\n",
    "ax.barh(x + width/2, top10['composite_score'], width, label='With Importance', color='coral', alpha=0.8)\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(top10['name'], fontsize=8)\n",
    "ax.set_xlabel('Score', fontsize=10, fontweight='bold')\n",
    "ax.set_title('Top 10: Base vs Composite', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Importance amplification\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "results_df['amplification'] = results_df['composite_score'] / results_df['base_topology'].replace(0, 0.01)\n",
    "colors_map = {'Application': '#3498db', 'Topic': '#2ecc71', 'Broker': '#e74c3c', 'Node': '#95a5a6'}\n",
    "for comp_type, color in colors_map.items():\n",
    "    data = results_df[results_df['type'] == comp_type]\n",
    "    ax.scatter(data['topic_importance'], data['amplification'], \n",
    "              s=100, alpha=0.7, c=color, label=comp_type, edgecolors='black')\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='No amplification')\n",
    "ax.set_xlabel('Topic Importance', fontsize=10, fontweight='bold')\n",
    "ax.set_ylabel('Amplification Factor', fontsize=10, fontweight='bold')\n",
    "ax.set_title('Importance Amplification Effect', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=8, loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Validation scatter\n",
    "ax = fig.add_subplot(gs[1, 2])\n",
    "for comp_type, color in colors_map.items():\n",
    "    data = results_df[results_df['type'] == comp_type]\n",
    "    ax.scatter(data['composite_score'], data['weighted_impact'],\n",
    "              s=100, alpha=0.7, c=color, label=comp_type, edgecolors='black')\n",
    "\n",
    "# Trend line\n",
    "z = np.polyfit(results_df['composite_score'], results_df['weighted_impact'], 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(results_df['composite_score'].min(), \n",
    "                     results_df['composite_score'].max(), 100)\n",
    "ax.plot(x_line, p(x_line), \"r--\", linewidth=2, alpha=0.8, label=f'r={pearson_r:.3f}')\n",
    "\n",
    "ax.set_xlabel('Composite Score (Prediction)', fontsize=10, fontweight='bold')\n",
    "ax.set_ylabel('Weighted Impact (Ground Truth)', fontsize=10, fontweight='bold')\n",
    "ax.set_title('Validation: Prediction vs Reality', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Component type comparison\n",
    "ax = fig.add_subplot(gs[2, 0])\n",
    "type_stats = results_df.groupby('type').agg({\n",
    "    'base_topology': 'mean',\n",
    "    'topic_importance': 'mean',\n",
    "    'composite_score': 'mean'\n",
    "})\n",
    "type_stats.plot(kind='bar', ax=ax, color=['steelblue', 'green', 'coral'], alpha=0.8)\n",
    "ax.set_ylabel('Average Score', fontsize=10, fontweight='bold')\n",
    "ax.set_title('Average Scores by Component Type', fontsize=11, fontweight='bold')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.legend(['Base Topology', 'Topic Importance', 'Composite'], fontsize=8)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 6. Impact distribution\n",
    "ax = fig.add_subplot(gs[2, 1])\n",
    "ax.hist(results_df['base_impact'], bins=15, alpha=0.6, label='Base Impact', color='steelblue')\n",
    "ax.hist(results_df['weighted_impact'], bins=15, alpha=0.6, label='Weighted Impact', color='coral')\n",
    "ax.set_xlabel('Impact Score', fontsize=10, fontweight='bold')\n",
    "ax.set_ylabel('Frequency', fontsize=10, fontweight='bold')\n",
    "ax.set_title('Impact Score Distribution', fontsize=11, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Heatmap\n",
    "ax = fig.add_subplot(gs[2, 2])\n",
    "corr_metrics = ['base_topology', 'topic_importance', 'composite_score', \n",
    "                'base_impact', 'weighted_impact']\n",
    "corr = results_df[corr_metrics].corr()\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=ax,\n",
    "           cbar_kws={'label': 'Correlation'}, square=True)\n",
    "ax.set_title('Metric Correlations', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.savefig('integrated_analysis_comprehensive.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Comprehensive visualizations generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. METHODOLOGY OVERVIEW\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   âœ“ Topic importance from QoS policies\")\n",
    "print(\"   âœ“ Propagation through all component types\")\n",
    "print(\"   âœ“ Integration into topology scores: C = C_base Ã— (1 + 0.5Â·TI)\")\n",
    "print(\"   âœ“ Integration into impact scores: I = I_base Ã— (1 + 0.5Â·TI)\")\n",
    "\n",
    "print(\"\\n2. KEY FINDINGS\")\n",
    "print(\"-\" * 80)\n",
    "most_critical = results_df.iloc[0]\n",
    "print(f\"   â€¢ Most critical: {most_critical['name']}\")\n",
    "print(f\"     - Base topology: {most_critical['base_topology']:.4f}\")\n",
    "print(f\"     - Topic importance: {most_critical['topic_importance']:.4f}\")\n",
    "print(f\"     - Composite score: {most_critical['composite_score']:.4f}\")\n",
    "\n",
    "print(f\"\\n   â€¢ Average topic importance: {results_df['topic_importance'].mean():.3f}\")\n",
    "print(f\"   â€¢ Articulation points: {len(aps)}\")\n",
    "\n",
    "# Importance distribution\n",
    "print(\"\\n   â€¢ Topic importance distribution:\")\n",
    "for comp_type in ['Topic', 'Application', 'Broker', 'Node']:\n",
    "    avg = results_df[results_df['type'] == comp_type]['topic_importance'].mean()\n",
    "    print(f\"     - {comp_type}: {avg:.3f}\")\n",
    "\n",
    "print(\"\\n3. VALIDATION RESULTS\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   â€¢ Correlation: {pearson_r:.3f} ({strength.split()[0]})\")\n",
    "print(f\"   â€¢ Top-5 accuracy: {accuracy:.1%}\")\n",
    "print(f\"   â€¢ Status: {'âœ“ VALIDATED' if pearson_r > 0.5 else 'âš  NEEDS TUNING'}\")\n",
    "\n",
    "print(\"\\n4. AMPLIFICATION EFFECTS\")\n",
    "print(\"-\" * 80)\n",
    "high_amp = results_df.nlargest(3, 'amplification')\n",
    "print(\"   Top 3 most amplified components:\")\n",
    "for _, row in high_amp.iterrows():\n",
    "    print(f\"   â€¢ {row['name']:25s} â†’ {row['amplification']:.2f}x amplification\")\n",
    "    print(f\"     (Importance: {row['topic_importance']:.3f})\")\n",
    "\n",
    "print(\"\\n5. KEY INSIGHTS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   âœ“ Topic importance successfully propagates through all layers\")\n",
    "print(\"   âœ“ High-importance components get amplified criticality scores\")\n",
    "print(\"   âœ“ Both topology and impact are weighted by business value\")\n",
    "print(\"   âœ“ Validation confirms that integrated approach predicts real impact\")\n",
    "\n",
    "print(\"\\n6. PRACTICAL IMPLICATIONS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   â€¢ Structural bottlenecks handling critical data get highest priority\")\n",
    "print(\"   â€¢ Monitoring services with low-importance topics rank lower\")\n",
    "print(\"   â€¢ Brokers and nodes inherit importance from hosted components\")\n",
    "print(\"   â€¢ This provides holistic risk assessment: structure + business value\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### **This Integrated Approach Is Superior Because:**\n",
    "\n",
    "1. **Holistic Assessment**: Combines structural position (topology) with business value (QoS)\n",
    "\n",
    "2. **Natural Propagation**: Importance flows through dependency relationships:\n",
    "   - Topics â†’ Applications (via pub/sub)\n",
    "   - Applications â†’ Brokers (via routing)\n",
    "   - Brokers â†’ Nodes (via hosting)\n",
    "\n",
    "3. **Amplification Not Replacement**: Base topology provides structure; importance amplifies it\n",
    "\n",
    "4. **Dual Integration**: Both prediction (composite) and validation (impact) use importance\n",
    "\n",
    "5. **Validated Approach**: High correlation confirms topology+importance predicts impact+importance\n",
    "\n",
    "### **When to Use What:**\n",
    "\n",
    "| Scenario | Metric | Why |\n",
    "|----------|--------|-----|\n",
    "| **Initial prioritization** | Topic Importance | Quick business value assessment |\n",
    "| **Structural analysis** | Base Topology | Pure architectural bottlenecks |\n",
    "| **Critical component ranking** | Composite Score | Best of both: structure + value |\n",
    "| **Impact prediction** | Weighted Impact | True business consequences |\n",
    "| **Validation** | Correlation | Verify approach works |\n",
    "\n",
    "### **Example Insights:**\n",
    "\n",
    "- **PaymentService**: High topology + High importance â†’ CRITICAL\n",
    "- **LogAggregator**: High topology + Low importance â†’ IMPORTANT (but not critical)\n",
    "- **Node1**: Hosts critical broker â†’ Inherits high importance â†’ CRITICAL\n",
    "\n",
    "This integrated methodology provides the most accurate criticality assessment for real-world systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
