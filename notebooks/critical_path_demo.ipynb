{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critical Path Analysis in Multi-Layer Pub-Sub Systems\n",
    "\n",
    "## Comprehensive Approach: Components + Paths\n",
    "\n",
    "This notebook demonstrates advanced analysis that identifies:\n",
    "\n",
    "### **Critical Components**\n",
    "- Individual nodes with high structural/business criticality\n",
    "- Articulation points (single points of failure)\n",
    "- High-impact components\n",
    "\n",
    "### **Critical Paths** (NEW)\n",
    "1. **Message Flow Paths**: Publisher → Topic → Subscriber chains\n",
    "2. **Dependency Chains**: Application dependency sequences\n",
    "3. **Infrastructure Paths**: Node connectivity paths\n",
    "4. **Cross-Layer Paths**: End-to-end system paths\n",
    "5. **Bottleneck Paths**: Paths through articulation points\n",
    "6. **Single-Point Paths**: Paths with no redundancy\n",
    "\n",
    "---\n",
    "\n",
    "## Why Path Analysis Matters\n",
    "\n",
    "- **Cascading Failures**: A single failure can propagate along paths\n",
    "- **Dependency Chains**: Long chains create vulnerability\n",
    "- **Redundancy Assessment**: Identifies paths with no alternatives\n",
    "- **Impact Prediction**: Shows how failures spread through the system\n",
    "- **Optimization**: Reveals where to add redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 12)\n",
    "\n",
    "print(\"✓ Environment ready for critical path analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Realistic Multi-Layer System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_complex_pubsub_system() -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Create a realistic pub-sub system with multiple layers and paths\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Layer 1: Physical Infrastructure (5 nodes)\n",
    "    nodes = ['Node1', 'Node2', 'Node3', 'Node4', 'Node5']\n",
    "    for node in nodes:\n",
    "        G.add_node(node, type='Node', name=node)\n",
    "    \n",
    "    # Node connectivity (mesh topology)\n",
    "    connections = [\n",
    "        ('Node1', 'Node2'), ('Node2', 'Node3'), ('Node3', 'Node4'),\n",
    "        ('Node1', 'Node3'), ('Node2', 'Node4'), ('Node4', 'Node5'),\n",
    "        ('Node1', 'Node5')  # Some redundancy\n",
    "    ]\n",
    "    for src, tgt in connections:\n",
    "        G.add_edge(src, tgt, type='CONNECTS_TO')\n",
    "        G.add_edge(tgt, src, type='CONNECTS_TO')\n",
    "    \n",
    "    # Layer 2: Brokers\n",
    "    brokers = [('MainBroker', 'Node1'), ('BackupBroker', 'Node3')]\n",
    "    for broker, node in brokers:\n",
    "        G.add_node(broker, type='Broker', name=broker)\n",
    "        G.add_edge(broker, node, type='RUNS_ON')\n",
    "    \n",
    "    # Layer 3: Topics (with QoS)\n",
    "    topics = [\n",
    "        ('T1', 'orders', 'PERSISTENT', 'RELIABLE', 'MainBroker'),\n",
    "        ('T2', 'payments', 'PERSISTENT', 'RELIABLE', 'MainBroker'),\n",
    "        ('T3', 'inventory', 'TRANSIENT', 'RELIABLE', 'MainBroker'),\n",
    "        ('T4', 'notifications', 'TRANSIENT', 'RELIABLE', 'BackupBroker'),\n",
    "        ('T5', 'analytics', 'VOLATILE', 'BEST_EFFORT', 'BackupBroker'),\n",
    "        ('T6', 'logs', 'VOLATILE', 'BEST_EFFORT', 'BackupBroker'),\n",
    "    ]\n",
    "    \n",
    "    for tid, name, dur, rel, broker in topics:\n",
    "        G.add_node(tid, type='Topic', name=name, durability=dur, reliability=rel)\n",
    "        G.add_edge(broker, tid, type='ROUTES')\n",
    "    \n",
    "    # Layer 4: Applications (creating dependency chains)\n",
    "    apps = [\n",
    "        # Core services (create a critical path)\n",
    "        ('OrderService', 'Node1', ['T1'], ['T2', 'T3']),\n",
    "        ('PaymentService', 'Node2', ['T2'], ['T1']),\n",
    "        ('InventoryService', 'Node3', ['T3'], ['T1']),\n",
    "        \n",
    "        # Support services (depend on core)\n",
    "        ('NotificationService', 'Node2', ['T4'], ['T1', 'T2', 'T3']),\n",
    "        ('ShippingService', 'Node4', [], ['T1', 'T3']),\n",
    "        \n",
    "        # Analytics pipeline (separate path)\n",
    "        ('DataCollector', 'Node4', ['T5'], ['T1', 'T2', 'T3']),\n",
    "        ('AnalyticsEngine', 'Node5', [], ['T5']),\n",
    "        \n",
    "        # Monitoring (low criticality)\n",
    "        ('LogAggregator', 'Node5', ['T6'], ['T5']),\n",
    "        ('MetricsCollector', 'Node5', [], ['T6']),\n",
    "        \n",
    "        # API Gateway (fan-out pattern)\n",
    "        ('APIGateway', 'Node1', [], ['T1', 'T2', 'T3', 'T4']),\n",
    "    ]\n",
    "    \n",
    "    for app, node, pubs, subs in apps:\n",
    "        G.add_node(app, type='Application', name=app)\n",
    "        G.add_edge(app, node, type='RUNS_ON')\n",
    "        for t in pubs:\n",
    "            G.add_edge(app, t, type='PUBLISHES_TO')\n",
    "        for t in subs:\n",
    "            G.add_edge(app, t, type='SUBSCRIBES_TO')\n",
    "    \n",
    "    # Create DEPENDS_ON relationships\n",
    "    for app1, _, pubs, _ in apps:\n",
    "        for topic in pubs:\n",
    "            for app2, _, _, subs in apps:\n",
    "                if app1 != app2 and topic in subs:\n",
    "                    G.add_edge(app2, app1, type='DEPENDS_ON')\n",
    "    \n",
    "    return G\n",
    "\n",
    "G = create_complex_pubsub_system()\n",
    "\n",
    "print(\"Multi-Layer System Created:\")\n",
    "print(f\"  Total components: {G.number_of_nodes()}\")\n",
    "print(f\"  Total relationships: {G.number_of_edges()}\")\n",
    "\n",
    "type_counts = {}\n",
    "for _, data in G.nodes(data=True):\n",
    "    t = data['type']\n",
    "    type_counts[t] = type_counts.get(t, 0) + 1\n",
    "\n",
    "print(\"\\nComponent Distribution:\")\n",
    "for t, c in sorted(type_counts.items()):\n",
    "    print(f\"  {t}: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Component Criticality\n",
    "\n",
    "First, we need component-level criticality scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute component criticality (simplified version)\n",
    "\n",
    "# 1. Topology metrics\n",
    "betweenness = nx.betweenness_centrality(G, normalized=True)\n",
    "undirected = G.to_undirected()\n",
    "articulation_points = set(nx.articulation_points(undirected))\n",
    "\n",
    "# 2. Topic importance\n",
    "def get_topic_importance(node_data):\n",
    "    if node_data.get('type') != 'Topic':\n",
    "        return 0.5\n",
    "    \n",
    "    dur_scores = {'VOLATILE': 0.2, 'TRANSIENT_LOCAL': 0.5, \n",
    "                  'TRANSIENT': 0.75, 'PERSISTENT': 1.0}\n",
    "    rel_scores = {'BEST_EFFORT': 0.3, 'RELIABLE': 1.0}\n",
    "    \n",
    "    dur = dur_scores.get(node_data.get('durability', 'VOLATILE'), 0.2)\n",
    "    rel = rel_scores.get(node_data.get('reliability', 'BEST_EFFORT'), 0.3)\n",
    "    \n",
    "    return 0.6 * dur + 0.4 * rel\n",
    "\n",
    "# Calculate importance for all nodes\n",
    "importance = {}\n",
    "for node, data in G.nodes(data=True):\n",
    "    importance[node] = get_topic_importance(data)\n",
    "\n",
    "# Propagate to applications\n",
    "for node, data in G.nodes(data=True):\n",
    "    if data['type'] == 'Application':\n",
    "        pubs = [t for _, t, e in G.out_edges(node, data=True) \n",
    "                if e['type'] == 'PUBLISHES_TO']\n",
    "        subs = [t for _, t, e in G.out_edges(node, data=True) \n",
    "                if e['type'] == 'SUBSCRIBES_TO']\n",
    "        \n",
    "        pub_imp = [importance[t] for t in pubs] if pubs else [0.5]\n",
    "        sub_imp = [importance[t] for t in subs] if subs else [0.5]\n",
    "        \n",
    "        importance[node] = 0.7 * np.mean(pub_imp) + 0.3 * np.mean(sub_imp)\n",
    "\n",
    "# 3. Composite criticality\n",
    "criticality = {}\n",
    "for node in G.nodes():\n",
    "    base = 0.6 * betweenness[node] + 0.4 * (1.0 if node in articulation_points else 0.0)\n",
    "    criticality[node] = base * (1 + 0.5 * importance[node])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPONENT CRITICALITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "crit_df = pd.DataFrame([\n",
    "    {'name': G.nodes[n]['name'], 'type': G.nodes[n]['type'],\n",
    "     'criticality': criticality[n], 'is_ap': n in articulation_points}\n",
    "    for n in G.nodes()\n",
    "]).sort_values('criticality', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Critical Components:\")\n",
    "print(crit_df.head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\nArticulation Points: {len(articulation_points)}\")\n",
    "if articulation_points:\n",
    "    ap_names = [G.nodes[n]['name'] for n in articulation_points]\n",
    "    print(f\"  → {', '.join(ap_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Critical Paths\n",
    "\n",
    "Now we identify critical paths across all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified path analyzer implementation\n",
    "\n",
    "def analyze_message_flow_paths(graph, criticality, importance, aps):\n",
    "    \"\"\"Find Publisher → Topic → Subscriber paths\"\"\"\n",
    "    paths = []\n",
    "    \n",
    "    # Find publishers and subscribers per topic\n",
    "    pubs = defaultdict(list)\n",
    "    subs = defaultdict(list)\n",
    "    \n",
    "    for node in graph.nodes():\n",
    "        if graph.nodes[node]['type'] != 'Application':\n",
    "            continue\n",
    "        for _, target, edge_data in graph.out_edges(node, data=True):\n",
    "            if graph.nodes[target]['type'] == 'Topic':\n",
    "                if edge_data['type'] == 'PUBLISHES_TO':\n",
    "                    pubs[target].append(node)\n",
    "                elif edge_data['type'] == 'SUBSCRIBES_TO':\n",
    "                    subs[target].append(node)\n",
    "    \n",
    "    # Create paths\n",
    "    for topic in set(pubs.keys()) & set(subs.keys()):\n",
    "        for pub in pubs[topic]:\n",
    "            for sub in subs[topic]:\n",
    "                if pub != sub:\n",
    "                    path = [pub, topic, sub]\n",
    "                    crits = [criticality[n] for n in path]\n",
    "                    paths.append({\n",
    "                        'path': path,\n",
    "                        'path_str': ' → '.join([graph.nodes[n]['name'] for n in path]),\n",
    "                        'length': len(path),\n",
    "                        'avg_criticality': np.mean(crits),\n",
    "                        'min_criticality': np.min(crits),\n",
    "                        'has_ap': any(n in aps for n in path),\n",
    "                        'type': 'message_flow'\n",
    "                    })\n",
    "    \n",
    "    return sorted(paths, key=lambda x: x['avg_criticality'], reverse=True)\n",
    "\n",
    "def analyze_dependency_chains(graph, criticality, importance, aps, max_length=6):\n",
    "    \"\"\"Find application dependency chains\"\"\"\n",
    "    paths = []\n",
    "    \n",
    "    apps = [n for n, d in graph.nodes(data=True) if d['type'] == 'Application']\n",
    "    \n",
    "    for source in apps:\n",
    "        for target in apps:\n",
    "            if source != target:\n",
    "                try:\n",
    "                    # Find dependency paths\n",
    "                    simple_paths = nx.all_simple_paths(\n",
    "                        graph, source, target, cutoff=max_length\n",
    "                    )\n",
    "                    \n",
    "                    for path in simple_paths:\n",
    "                        # Check if all edges are DEPENDS_ON\n",
    "                        is_dep_chain = all(\n",
    "                            graph.edges[path[i], path[i+1]].get('type') == 'DEPENDS_ON'\n",
    "                            for i in range(len(path)-1)\n",
    "                        )\n",
    "                        \n",
    "                        if is_dep_chain and len(path) >= 2:\n",
    "                            crits = [criticality[n] for n in path]\n",
    "                            paths.append({\n",
    "                                'path': path,\n",
    "                                'path_str': ' → '.join([graph.nodes[n]['name'] for n in path]),\n",
    "                                'length': len(path),\n",
    "                                'avg_criticality': np.mean(crits),\n",
    "                                'min_criticality': np.min(crits),\n",
    "                                'has_ap': any(n in aps for n in path),\n",
    "                                'type': 'dependency_chain'\n",
    "                            })\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    # Sort by length first, then criticality\n",
    "    paths.sort(key=lambda x: (x['length'], x['avg_criticality']), reverse=True)\n",
    "    return paths[:50]  # Top 50\n",
    "\n",
    "# Analyze paths\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CRITICAL PATH ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nAnalyzing message flow paths...\")\n",
    "message_paths = analyze_message_flow_paths(G, criticality, importance, articulation_points)\n",
    "print(f\"  Found {len(message_paths)} message flow paths\")\n",
    "\n",
    "print(\"\\nAnalyzing dependency chains...\")\n",
    "dependency_paths = analyze_dependency_chains(G, criticality, importance, articulation_points)\n",
    "print(f\"  Found {len(dependency_paths)} dependency chains\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TOP 10 CRITICAL MESSAGE FLOW PATHS\")\n",
    "print(\"-\"*80)\n",
    "msg_df = pd.DataFrame(message_paths[:10])\n",
    "if len(msg_df) > 0:\n",
    "    print(msg_df[['path_str', 'avg_criticality', 'min_criticality', 'has_ap']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"LONGEST DEPENDENCY CHAINS\")\n",
    "print(\"-\"*80)\n",
    "dep_df = pd.DataFrame(dependency_paths[:10])\n",
    "if len(dep_df) > 0:\n",
    "    print(dep_df[['path_str', 'length', 'avg_criticality', 'has_ap']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Identify Path Vulnerabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_bottleneck_paths(paths):\n",
    "    \"\"\"Paths through articulation points\"\"\"\n",
    "    return [p for p in paths if p['has_ap']]\n",
    "\n",
    "def calculate_path_redundancy(graph, path):\n",
    "    \"\"\"Calculate if alternative paths exist\"\"\"\n",
    "    if len(path) < 2:\n",
    "        return 1.0\n",
    "    \n",
    "    source, target = path[0], path[-1]\n",
    "    try:\n",
    "        all_paths = list(nx.all_simple_paths(graph, source, target, cutoff=len(path)+2))\n",
    "        num_paths = len(all_paths)\n",
    "        return min(1.0, (num_paths - 1) / 4.0)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Analyze vulnerabilities\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PATH VULNERABILITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_paths = message_paths + dependency_paths\n",
    "\n",
    "# Bottleneck paths\n",
    "bottleneck_paths = identify_bottleneck_paths(all_paths)\n",
    "print(f\"\\n1. BOTTLENECK PATHS (through articulation points): {len(bottleneck_paths)}\")\n",
    "if bottleneck_paths:\n",
    "    print(\"\\nTop 5:\")\n",
    "    for i, p in enumerate(bottleneck_paths[:5], 1):\n",
    "        print(f\"  {i}. {p['path_str']}\")\n",
    "        print(f\"     Avg Criticality: {p['avg_criticality']:.4f}\")\n",
    "\n",
    "# Calculate redundancy\n",
    "print(\"\\n2. SINGLE-POINT FAILURE PATHS (no redundancy)\")\n",
    "for path_data in message_paths:\n",
    "    path_data['redundancy'] = calculate_path_redundancy(G, path_data['path'])\n",
    "\n",
    "spof_paths = [p for p in message_paths if p['redundancy'] < 0.3]\n",
    "print(f\"   Found {len(spof_paths)} paths with low redundancy (<30%)\")\n",
    "if spof_paths:\n",
    "    print(\"\\nTop 5:\")\n",
    "    for i, p in enumerate(spof_paths[:5], 1):\n",
    "        print(f\"  {i}. {p['path_str']}\")\n",
    "        print(f\"     Redundancy: {p['redundancy']:.2f}, Criticality: {p['avg_criticality']:.4f}\")\n",
    "\n",
    "# Component frequency in paths\n",
    "print(\"\\n3. CRITICAL PATH JUNCTIONS (components in many paths)\")\n",
    "component_freq = defaultdict(int)\n",
    "for path_data in all_paths:\n",
    "    for node in path_data['path']:\n",
    "        component_freq[node] += 1\n",
    "\n",
    "top_junctions = sorted(component_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"\\nComponents appearing in most paths:\")\n",
    "for node, count in top_junctions:\n",
    "    print(f\"  {G.nodes[node]['name']:25s} → {count} paths (criticality: {criticality[node]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. Path length distribution\n",
    "ax = axes[0, 0]\n",
    "msg_lengths = [p['length'] for p in message_paths]\n",
    "dep_lengths = [p['length'] for p in dependency_paths]\n",
    "ax.hist([msg_lengths, dep_lengths], label=['Message Flows', 'Dependencies'], bins=10, alpha=0.7)\n",
    "ax.set_xlabel('Path Length', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Path Length Distribution', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Path criticality distribution\n",
    "ax = axes[0, 1]\n",
    "msg_crits = [p['avg_criticality'] for p in message_paths]\n",
    "dep_crits = [p['avg_criticality'] for p in dependency_paths]\n",
    "ax.hist([msg_crits, dep_crits], label=['Message Flows', 'Dependencies'], bins=15, alpha=0.7)\n",
    "ax.set_xlabel('Average Path Criticality', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Path Criticality Distribution', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Redundancy analysis\n",
    "ax = axes[0, 2]\n",
    "redundancies = [p['redundancy'] for p in message_paths]\n",
    "colors = ['red' if r < 0.3 else 'orange' if r < 0.6 else 'green' for r in redundancies]\n",
    "ax.scatter(range(len(redundancies)), sorted(redundancies), c=colors, s=50, alpha=0.6)\n",
    "ax.axhline(y=0.3, color='red', linestyle='--', linewidth=2, label='Low redundancy')\n",
    "ax.axhline(y=0.6, color='orange', linestyle='--', linewidth=2, label='Medium redundancy')\n",
    "ax.set_xlabel('Path Index (sorted)', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Redundancy Score', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Path Redundancy Analysis', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Component frequency in paths\n",
    "ax = axes[1, 0]\n",
    "top_10_junctions = sorted(component_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "names = [G.nodes[n]['name'] for n, _ in top_10_junctions]\n",
    "counts = [c for _, c in top_10_junctions]\n",
    "colors_by_type = [{'Application': '#3498db', 'Topic': '#2ecc71', \n",
    "                   'Broker': '#e74c3c', 'Node': '#95a5a6'}[G.nodes[n]['type']] \n",
    "                  for n, _ in top_10_junctions]\n",
    "ax.barh(range(len(names)), counts, color=colors_by_type, alpha=0.8)\n",
    "ax.set_yticks(range(len(names)))\n",
    "ax.set_yticklabels(names, fontsize=9)\n",
    "ax.set_xlabel('Number of Paths', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Critical Path Junctions', fontsize=12, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 5. Criticality vs Redundancy scatter\n",
    "ax = axes[1, 1]\n",
    "crits = [p['avg_criticality'] for p in message_paths]\n",
    "redunds = [p['redundancy'] for p in message_paths]\n",
    "has_aps = [p['has_ap'] for p in message_paths]\n",
    "colors = ['red' if ap else 'blue' for ap in has_aps]\n",
    "ax.scatter(crits, redunds, c=colors, s=100, alpha=0.6, edgecolors='black')\n",
    "ax.set_xlabel('Path Criticality', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Redundancy Score', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Risk Matrix: Criticality vs Redundancy', fontsize=12, fontweight='bold')\n",
    "ax.axvline(x=np.mean(crits), color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.text(0.05, 0.95, 'HIGH RISK\\n(High crit, Low redundancy)', \n",
    "        transform=ax.transAxes, fontsize=9, va='top', \n",
    "        bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Path type comparison\n",
    "ax = axes[1, 2]\n",
    "path_stats = pd.DataFrame([\n",
    "    {'Type': 'Message Flow', 'Count': len(message_paths), \n",
    "     'Avg Criticality': np.mean([p['avg_criticality'] for p in message_paths])},\n",
    "    {'Type': 'Dependencies', 'Count': len(dependency_paths),\n",
    "     'Avg Criticality': np.mean([p['avg_criticality'] for p in dependency_paths]) if dependency_paths else 0}\n",
    "])\n",
    "x = np.arange(len(path_stats))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, path_stats['Count'], width, label='Count', color='steelblue', alpha=0.8)\n",
    "ax2 = ax.twinx()\n",
    "ax2.bar(x + width/2, path_stats['Avg Criticality'], width, label='Avg Criticality', \n",
    "        color='coral', alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(path_stats['Type'], fontsize=10)\n",
    "ax.set_ylabel('Path Count', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Average Criticality', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Path Type Summary', fontsize=12, fontweight='bold')\n",
    "ax.legend(loc='upper left', fontsize=9)\n",
    "ax2.legend(loc='upper right', fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('critical_path_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualizations generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Path-Based Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PATH-BASED RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. IMMEDIATE ACTIONS (High Priority)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Recommendation 1: Address bottleneck paths\n",
    "if bottleneck_paths:\n",
    "    print(\"\\n✓ CRITICAL: Address Bottleneck Paths\")\n",
    "    print(f\"  → {len(bottleneck_paths)} paths go through articulation points\")\n",
    "    critical_aps = set()\n",
    "    for p in bottleneck_paths:\n",
    "        for node in p['path']:\n",
    "            if node in articulation_points:\n",
    "                critical_aps.add(node)\n",
    "    print(f\"  → Implement redundancy for: {', '.join([G.nodes[n]['name'] for n in critical_aps])}\")\n",
    "\n",
    "# Recommendation 2: Add redundancy to single-point paths\n",
    "if spof_paths:\n",
    "    print(\"\\n✓ HIGH PRIORITY: Increase Path Redundancy\")\n",
    "    print(f\"  → {len(spof_paths)} critical paths have no alternatives\")\n",
    "    print(\"  → Actions:\")\n",
    "    for i, p in enumerate(spof_paths[:3], 1):\n",
    "        print(f\"     {i}. Add alternative path for: {p['path_str']}\")\n",
    "\n",
    "# Recommendation 3: Monitor critical junctions\n",
    "print(\"\\n✓ MONITORING: Focus on Critical Junctions\")\n",
    "print(\"  → Enhanced monitoring for components in many paths:\")\n",
    "for node, count in top_junctions[:5]:\n",
    "    print(f\"     - {G.nodes[node]['name']:25s} (appears in {count} paths)\")\n",
    "\n",
    "print(\"\\n2. MEDIUM-TERM IMPROVEMENTS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Dependency chain length\n",
    "if dependency_paths:\n",
    "    long_chains = [p for p in dependency_paths if p['length'] >= 4]\n",
    "    if long_chains:\n",
    "        print(\"\\n✓ Shorten Dependency Chains\")\n",
    "        print(f\"  → {len(long_chains)} chains have 4+ dependencies\")\n",
    "        print(f\"  → Consider decoupling or introducing intermediate services\")\n",
    "\n",
    "# Redundancy improvement\n",
    "low_redundancy = [p for p in message_paths if p['redundancy'] < 0.5]\n",
    "if low_redundancy:\n",
    "    print(\"\\n✓ Improve System Redundancy\")\n",
    "    print(f\"  → {len(low_redundancy)} paths have <50% redundancy\")\n",
    "    print(f\"  → Overall redundancy score: {np.mean([p['redundancy'] for p in message_paths]):.2f}\")\n",
    "    print(f\"  → Target: >0.70 for critical paths\")\n",
    "\n",
    "print(\"\\n3. ARCHITECTURAL IMPROVEMENTS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Circuit breaker recommendations\n",
    "print(\"\\n✓ Implement Circuit Breakers\")\n",
    "print(\"  → For longest dependency chains to prevent cascading failures\")\n",
    "for i, p in enumerate(sorted(dependency_paths, key=lambda x: x['length'], reverse=True)[:3], 1):\n",
    "    print(f\"     {i}. {p['path_str']} (length: {p['length']})\")\n",
    "\n",
    "# Failover recommendations\n",
    "print(\"\\n✓ Configure Automatic Failover\")\n",
    "print(\"  → For single-point failure paths\")\n",
    "print(\"  → Implement active-passive or active-active patterns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE PATH ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. SYSTEM OVERVIEW\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  Total Components: {G.number_of_nodes()}\")\n",
    "print(f\"  Total Relationships: {G.number_of_edges()}\")\n",
    "print(f\"  Articulation Points: {len(articulation_points)}\")\n",
    "\n",
    "print(\"\\n2. PATH STATISTICS\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  Message Flow Paths: {len(message_paths)}\")\n",
    "print(f\"  Dependency Chains: {len(dependency_paths)}\")\n",
    "print(f\"  Average Path Length: {np.mean([p['length'] for p in all_paths]):.2f}\")\n",
    "print(f\"  Longest Chain: {max([p['length'] for p in dependency_paths])} hops\" if dependency_paths else \"  Longest Chain: N/A\")\n",
    "\n",
    "print(\"\\n3. CRITICALITY METRICS\")\n",
    "print(\"-\" * 80)\n",
    "all_crits = [p['avg_criticality'] for p in all_paths]\n",
    "print(f\"  Average Path Criticality: {np.mean(all_crits):.4f}\")\n",
    "print(f\"  Max Path Criticality: {np.max(all_crits):.4f}\")\n",
    "print(f\"  Paths with High Criticality (>0.5): {len([c for c in all_crits if c > 0.5])}\")\n",
    "\n",
    "print(\"\\n4. VULNERABILITY ASSESSMENT\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  Bottleneck Paths (through APs): {len(bottleneck_paths)}\")\n",
    "print(f\"  Single-Point Failure Paths: {len(spof_paths)}\")\n",
    "redund_scores = [p['redundancy'] for p in message_paths]\n",
    "print(f\"  Average Redundancy: {np.mean(redund_scores):.2f}\")\n",
    "print(f\"  Paths with No Redundancy: {len([r for r in redund_scores if r < 0.1])}\")\n",
    "\n",
    "print(\"\\n5. KEY FINDINGS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Most critical path\n",
    "most_critical_path = max(all_paths, key=lambda x: x['avg_criticality'])\n",
    "print(f\"  Most Critical Path:\")\n",
    "print(f\"    → {most_critical_path['path_str']}\")\n",
    "print(f\"    → Criticality: {most_critical_path['avg_criticality']:.4f}\")\n",
    "print(f\"    → Type: {most_critical_path['type']}\")\n",
    "\n",
    "# Longest dependency chain\n",
    "if dependency_paths:\n",
    "    longest_chain = max(dependency_paths, key=lambda x: x['length'])\n",
    "    print(f\"\\n  Longest Dependency Chain:\")\n",
    "    print(f\"    → {longest_chain['path_str']}\")\n",
    "    print(f\"    → Length: {longest_chain['length']} hops\")\n",
    "    print(f\"    → Criticality: {longest_chain['avg_criticality']:.4f}\")\n",
    "\n",
    "# Most vulnerable path\n",
    "if spof_paths:\n",
    "    most_vulnerable = min(spof_paths, key=lambda x: x['redundancy'])\n",
    "    print(f\"\\n  Most Vulnerable Path (lowest redundancy):\")\n",
    "    print(f\"    → {most_vulnerable['path_str']}\")\n",
    "    print(f\"    → Redundancy: {most_vulnerable['redundancy']:.2f}\")\n",
    "    print(f\"    → Criticality: {most_vulnerable['avg_criticality']:.4f}\")\n",
    "\n",
    "print(\"\\n6. RISK ASSESSMENT\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "high_risk = len([p for p in message_paths if p['avg_criticality'] > 0.6 and p['redundancy'] < 0.3])\n",
    "medium_risk = len([p for p in message_paths if p['avg_criticality'] > 0.4 and p['redundancy'] < 0.5])\n",
    "\n",
    "print(f\"  HIGH RISK paths: {high_risk} (high criticality + low redundancy)\")\n",
    "print(f\"  MEDIUM RISK paths: {medium_risk}\")\n",
    "print(f\"  Overall System Risk: {'HIGH' if high_risk > 5 else 'MEDIUM' if high_risk > 2 else 'LOW'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"END OF REPORT\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### **Critical Path Analysis Provides:**\n",
    "\n",
    "1. **Cascading Failure Prediction**: Identifies chains where failures propagate\n",
    "2. **Redundancy Assessment**: Quantifies alternative paths and single points of failure\n",
    "3. **Bottleneck Identification**: Finds paths through articulation points\n",
    "4. **Dependency Chain Analysis**: Reveals long coupling sequences\n",
    "5. **Actionable Recommendations**: Specific paths requiring redundancy\n",
    "\n",
    "### **Integration with Component Analysis:**\n",
    "\n",
    "| Analysis Type | Focus | Output |\n",
    "|--------------|-------|--------|\n",
    "| **Component** | Individual nodes | Which components are critical |\n",
    "| **Path** | Chains & sequences | How failures cascade |\n",
    "| **Combined** | System-wide | Complete risk assessment |\n",
    "\n",
    "### **Practical Applications:**\n",
    "\n",
    "- **Design**: Identify where to add redundancy\n",
    "- **Monitoring**: Focus on critical path junctions\n",
    "- **Incident Response**: Understand cascading failure potential\n",
    "- **Optimization**: Shorten long dependency chains\n",
    "- **Capacity Planning**: Strengthen weak links in critical paths\n",
    "\n",
    "This multi-dimensional analysis (components + paths) provides the most comprehensive risk assessment for distributed systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
