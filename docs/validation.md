# Step 5: Validation & Comparison

**The Moment of Truth: Do topological metrics actually predict reality?**

Validation is the final and most critical step in the Software-as-a-Graph methodology. It statistically compares the **Predicted Criticality Scores ()** generated in Step 2/3 against the **Actual Impact Scores ()** measured in Step 4.

This step answers the question: *"Can I trust this graph model to predict failures before they happen?"*

---

## 1. The Validation Pipeline

The validation process acts as an automated judge, evaluating the accuracy of your graph model using three distinct analytical lenses:

1. **Correlation Analysis:** Do the predictions rank components in the same order as reality? (e.g., Is the most critical node in the graph also the most destructive when it crashes?)
2. **Classification Accuracy:** Does the model correctly flag "Critical" components while ignoring "Non-Critical" ones?
3. **Ranking Agreement:** Do the "Top-5" predicted critical components match the "Top-5" actual bottlenecks?

### The Comparison Logic

The validator aligns data from two sources:

| Source | Metric | Represents | Generated By |
| --- | --- | --- | --- |
| **Prediction** | **** | Theoretical Criticality | `analyze_graph.py` |
| **Ground Truth** | **** | Empirical Failure Impact | `simulate_graph.py` |

---

## 2. Using the Validator (`validate_graph.py`)

The CLI script automates the ingestion of analysis and simulation files, aligns the component IDs, calculates statistical metrics, and generates a Pass/Fail report.

### Basic Usage

To validate the entire system (Application, Infrastructure, and Combined layers) using default thresholds:

```bash
python validate_graph.py --all

```

### Layer-Specific Validation

If you only want to validate the software architecture (Application layer):

```bash
python validate_graph.py --layer app

```

### Quick Validation (File-to-File)

If you have specific JSON exports you wish to compare directly without querying the database:

```bash
python validate_graph.py --quick results/predicted_scores.json results/actual_impact.json

```

### Customizing Targets

You can override the default success criteria for stricter or looser validation:

```bash
# Require 90% accuracy and 0.8 correlation to pass
python validate_graph.py --all --spearman 0.80 --f1 0.90

```

---

## 3. Success Criteria (The Targets)

The framework defines rigorous statistical targets that must be met for the model to be considered "Production Ready."

| Metric | Target | What It Measures |
| --- | --- | --- |
| **Spearman ** | **** | **Rank Correlation.** A score of 1.0 means perfect alignment in ranking. <br>

<br> *Target:* Strong positive correlation. |
| **F1 Score** | **** | **Classification Accuracy.** Harmonic mean of Precision and Recall. <br>

<br> *Target:* High accuracy in identifying critical nodes. |
| **Precision** | **** | **Trustworthiness.** When the model says "Critical," is it right? <br>

<br> *Target:* Low False Positive rate. |
| **Recall** | **** | **Sensitivity.** Did the model catch all the actual critical nodes? <br>

<br> *Target:* Low False Negative rate. |
| **Top-5 Overlap** | **** | **Focus.** Does the model identify at least 3 of the top 5 actual worst-case failures? |

---

## 4. Interpreting the Output

The CLI provides a detailed, color-coded terminal report.

### Sample Output Breakdown

```text
VALIDATION PIPELINE RESULTS
==============================================================================

  Timestamp:      2025-01-14T10:30:00
  Overall Status: PASSED

>> Layer Summary

  Layer        N      Spearman   F1         Precision  Recall     Top-5      Status
  --------------------------------------------------------------------------------
  app          35     0.8921     0.9091     0.9091     0.9091     1.0000     PASSED
  infra        12     0.7540     0.8571     0.8571     0.8571     0.8000     PASSED
  system       47     0.8760     0.9430     0.9120     0.8570     0.8000     PASSED

>> Validation Targets
  Spearman ρ ≥ 0.70  |  F1 ≥ 0.80  |  Precision ≥ 0.80  |  Top-5 ≥ 0.60

>> Cross-Layer Insights
  • 'sensor_fusion' is critical in both Prediction and Simulation (Correctly Identified)
  • 'gateway_node' was predicted Critical but had Low Impact (False Positive)

```

### Understanding Status Codes

* **PASSED (Green):** The metric meets or exceeds the target. The model is accurate.
* **WARNING (Yellow):** The metric is close (within 10%) of the target.
* **FAILED (Red):** The prediction diverges significantly from reality.

---

## 5. Troubleshooting Failed Validation

If your validation fails, use the error metrics to diagnose the issue:

1. **Low Spearman ():**
* *Issue:* The overall ranking is scattered. The model doesn't understand the relative importance of nodes.
* *Fix:* Revisit **Step 1**. Are your `Topic Weights` correct? Is a low-priority topic dominating the graph?


2. **Low Precision (Many False Positives):**
* *Issue:* The model is "crying wolf." It flags safe components as Critical.
* *Fix:* Increase the classification threshold or adjust `Maintainability` weights in **Step 3**.


3. **Low Recall (Many False Negatives):**
* *Issue:* The model is missing dangerous components.
* *Fix:* Check **Step 2**. Are you accounting for `Reverse PageRank` (Cascading failure)? Increase the weight of `Reliability` in the Quality formula.


4. **Low Top-5 Overlap:**
* *Issue:* The model finds "generally" critical things but misses the absolute worst offenders.
* *Fix:* Check for **Single Points of Failure (SPOF)**. Ensure `Availability` score (Articulation Points) is weighted heavily enough.