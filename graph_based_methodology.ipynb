{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph-Based Modeling and Analysis of Distributed Publish-Subscribe Systems\n",
    "\n",
    "## A Comprehensive Six-Step Methodology\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates a comprehensive methodology for analyzing distributed publish-subscribe (pub-sub) systems using graph-based modeling. The approach enables:\n",
    "\n",
    "- **Predictive Analysis**: Identify critical components before failures occur\n",
    "- **Structural Insights**: Detect anti-patterns and architectural vulnerabilities\n",
    "- **Validation**: Correlate predictions with simulation outcomes\n",
    "- **Visualization**: Multi-layer graph representations for complex systems\n",
    "\n",
    "### Target Metrics\n",
    "\n",
    "| Metric | Target | Description |\n",
    "|--------|--------|-------------|\n",
    "| Spearman Correlation | ‚â• 0.7 | Correlation between predicted criticality and actual failure impact |\n",
    "| F1 Score | ‚â• 0.9 | Harmonic mean of precision and recall for critical component identification |\n",
    "| Precision | ‚â• 0.9 | Correctly identified critical components / Total identified |\n",
    "| Recall | ‚â• 0.85 | Correctly identified critical components / Actual critical components |\n",
    "\n",
    "### Composite Criticality Formula\n",
    "\n",
    "$$C_{score}(v) = \\alpha \\cdot C_B^{norm}(v) + \\beta \\cdot AP(v) + \\gamma \\cdot I(v)$$\n",
    "\n",
    "Where:\n",
    "- $C_B^{norm}(v)$ = Normalized betweenness centrality\n",
    "- $AP(v)$ = Articulation point indicator (1 if node is an articulation point, 0 otherwise)\n",
    "- $I(v)$ = Impact score based on reachability loss\n",
    "- $\\alpha, \\beta, \\gamma$ = Tunable weights (default: 0.4, 0.3, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Dependencies](#1.-Setup-and-Dependencies)\n",
    "2. [Step 1: Graph Data Generation](#Step-1:-Graph-Data-Generation)\n",
    "3. [Step 2: Neo4j Database Import](#Step-2:-Neo4j-Database-Import)\n",
    "4. [Step 3: Graph Analysis](#Step-3:-Graph-Analysis)\n",
    "5. [Step 4: Simulation and Validation](#Step-4:-Simulation-and-Validation)\n",
    "6. [Step 5: Visualization](#Step-5:-Visualization)\n",
    "7. [Complete Pipeline Execution](#Complete-Pipeline-Execution)\n",
    "8. [Results Interpretation](#Results-Interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, let's import the required libraries and define our configuration classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install networkx scipy neo4j matplotlib --quiet\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import statistics\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Set, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core dependencies\n",
    "import networkx as nx\n",
    "\n",
    "# Optional dependencies\n",
    "try:\n",
    "    from scipy import stats as scipy_stats\n",
    "    SCIPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SCIPY_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è scipy not available - using fallback correlation calculation\")\n",
    "\n",
    "try:\n",
    "    from neo4j import GraphDatabase\n",
    "    NEO4J_AVAILABLE = True\n",
    "except ImportError:\n",
    "    NEO4J_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è neo4j driver not available - Neo4j import will be skipped\")\n",
    "\n",
    "print(f\"‚úÖ NetworkX version: {nx.__version__}\")\n",
    "print(f\"‚úÖ Python async support: enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Classes\n",
    "\n",
    "We define enumerations and data classes to configure our analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scenario(Enum):\n",
    "    \"\"\"Domain scenarios for pub-sub system generation\"\"\"\n",
    "    GENERIC = \"generic\"\n",
    "    IOT_SMART_CITY = \"iot_smart_city\"\n",
    "    FINANCIAL_TRADING = \"financial_trading\"\n",
    "    HEALTHCARE = \"healthcare\"\n",
    "    ECOMMERCE = \"ecommerce\"\n",
    "\n",
    "\n",
    "class CriticalityLevel(Enum):\n",
    "    \"\"\"Criticality classification levels based on composite score\"\"\"\n",
    "    CRITICAL = \"CRITICAL\"   # Score >= 0.8\n",
    "    HIGH = \"HIGH\"           # Score >= 0.6\n",
    "    MEDIUM = \"MEDIUM\"       # Score >= 0.4\n",
    "    LOW = \"LOW\"             # Score >= 0.2\n",
    "    MINIMAL = \"MINIMAL\"     # Score < 0.2\n",
    "\n",
    "\n",
    "# Target validation thresholds from research methodology\n",
    "TARGET_SPEARMAN_CORRELATION = 0.7\n",
    "TARGET_F1_SCORE = 0.9\n",
    "TARGET_PRECISION = 0.9\n",
    "TARGET_RECALL = 0.85\n",
    "\n",
    "# Default criticality scoring weights\n",
    "DEFAULT_ALPHA = 0.4  # Betweenness centrality weight\n",
    "DEFAULT_BETA = 0.3   # Articulation point weight\n",
    "DEFAULT_GAMMA = 0.3  # Impact score weight\n",
    "\n",
    "print(\"Configuration classes defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GraphConfig:\n",
    "    \"\"\"Configuration for graph generation\"\"\"\n",
    "    scale: str = 'medium'\n",
    "    scenario: Scenario = Scenario.IOT_SMART_CITY\n",
    "    num_nodes: int = 10\n",
    "    num_applications: int = 30\n",
    "    num_topics: int = 20\n",
    "    num_brokers: int = 3\n",
    "    edge_density: float = 0.4\n",
    "    antipatterns: List[str] = field(default_factory=list)\n",
    "    seed: int = 42\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CriticalityScore:\n",
    "    \"\"\"Composite criticality score for a component\"\"\"\n",
    "    component_id: str\n",
    "    component_type: str\n",
    "    betweenness_centrality: float\n",
    "    is_articulation_point: bool\n",
    "    impact_score: float\n",
    "    composite_score: float\n",
    "    criticality_level: CriticalityLevel\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ValidationResult:\n",
    "    \"\"\"Validation results comparing predictions to simulation outcomes\"\"\"\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1_score: float\n",
    "    spearman_correlation: float\n",
    "    targets_met: Dict[str, bool]\n",
    "\n",
    "\n",
    "print(\"Data classes defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Graph Data Generation\n",
    "\n",
    "The first step involves generating realistic pub-sub system topologies. Our generator supports:\n",
    "\n",
    "- **Multiple Scales**: From tiny (3 nodes) to xlarge (50+ nodes)\n",
    "- **Domain Scenarios**: IoT, Financial Trading, Healthcare, eCommerce\n",
    "- **QoS Profiles**: Different reliability/latency requirements per domain\n",
    "- **Anti-pattern Injection**: SPOF, God Topics, Tight Coupling\n",
    "\n",
    "### Multi-Layer Graph Model\n",
    "\n",
    "The system is modeled as a multi-layer graph:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ           Application Layer             ‚îÇ\n",
    "‚îÇ   (Producers, Consumers, Prosumers)     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                      ‚îÇ PUBLISHES/SUBSCRIBES\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              Topic Layer                ‚îÇ\n",
    "‚îÇ    (Message channels with QoS)          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                      ‚îÇ ROUTES\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ             Broker Layer                ‚îÇ\n",
    "‚îÇ      (Message routing nodes)            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                      ‚îÇ RUNS_ON\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ         Infrastructure Layer            ‚îÇ\n",
    "‚îÇ   (Physical/virtual compute nodes)      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PubSubGraphGenerator:\n",
    "    \"\"\"\n",
    "    Generates realistic pub-sub system graphs for different domain scenarios.\n",
    "    \n",
    "    The generator creates a multi-layer graph model representing:\n",
    "    - Infrastructure nodes (servers, edge devices)\n",
    "    - Message brokers\n",
    "    - Topics with QoS profiles\n",
    "    - Applications (producers, consumers, prosumers)\n",
    "    - Relationships between all components\n",
    "    \"\"\"\n",
    "    \n",
    "    # Scale presets defining system size\n",
    "    SCALES = {\n",
    "        'tiny': {'nodes': 3, 'apps': 8, 'topics': 5, 'brokers': 1},\n",
    "        'small': {'nodes': 5, 'apps': 15, 'topics': 10, 'brokers': 2},\n",
    "        'medium': {'nodes': 10, 'apps': 30, 'topics': 20, 'brokers': 3},\n",
    "        'large': {'nodes': 25, 'apps': 80, 'topics': 50, 'brokers': 5},\n",
    "        'xlarge': {'nodes': 50, 'apps': 150, 'topics': 100, 'brokers': 8}\n",
    "    }\n",
    "    \n",
    "    # Domain-specific application types\n",
    "    APP_TYPES = {\n",
    "        Scenario.GENERIC: ['ServiceA', 'ServiceB', 'Processor', 'Handler', 'Monitor'],\n",
    "        Scenario.IOT_SMART_CITY: [\n",
    "            'TrafficSensor', 'ParkingSensor', 'AirQualityMonitor', 'EmergencyDispatcher',\n",
    "            'LightingController', 'WasteManager', 'WeatherStation', 'TransitTracker'\n",
    "        ],\n",
    "        Scenario.FINANCIAL_TRADING: [\n",
    "            'MarketDataFeed', 'OrderProcessor', 'RiskEngine', 'TradeExecutor',\n",
    "            'PositionTracker', 'ComplianceMonitor', 'MatchingEngine', 'PricingService'\n",
    "        ],\n",
    "        Scenario.HEALTHCARE: [\n",
    "            'VitalSignsMonitor', 'PatientTracker', 'AlertDispatcher', 'MedicationManager',\n",
    "            'LabResultsProcessor', 'ImagingService', 'BillingService', 'AppointmentScheduler'\n",
    "        ],\n",
    "        Scenario.ECOMMERCE: [\n",
    "            'OrderService', 'InventoryManager', 'PaymentProcessor', 'ShippingCalculator',\n",
    "            'RecommendationEngine', 'CartService', 'NotificationService', 'FraudDetector'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Domain-specific topic patterns\n",
    "    TOPIC_PATTERNS = {\n",
    "        Scenario.GENERIC: ['events', 'data', 'commands', 'status', 'metrics'],\n",
    "        Scenario.IOT_SMART_CITY: [\n",
    "            'traffic/flow', 'traffic/congestion', 'parking/availability', 'air_quality/readings',\n",
    "            'emergency/alerts', 'lighting/status', 'weather/current', 'transit/location'\n",
    "        ],\n",
    "        Scenario.FINANCIAL_TRADING: [\n",
    "            'market/prices', 'market/quotes', 'orders/new', 'orders/filled',\n",
    "            'trades/executed', 'risk/alerts', 'positions/updates', 'compliance/events'\n",
    "        ],\n",
    "        Scenario.HEALTHCARE: [\n",
    "            'patient/vitals', 'patient/alerts', 'lab/results', 'imaging/completed',\n",
    "            'medication/administered', 'appointments/scheduled', 'billing/claims'\n",
    "        ],\n",
    "        Scenario.ECOMMERCE: [\n",
    "            'orders/created', 'inventory/updates', 'payments/processed', 'shipping/tracking',\n",
    "            'recommendations/generated', 'notifications/sent', 'fraud/detected'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # QoS profiles by criticality level\n",
    "    QOS_PROFILES = {\n",
    "        'CRITICAL': {\n",
    "            'reliability': 'RELIABLE',\n",
    "            'durability': 'TRANSIENT_LOCAL',\n",
    "            'deadline_ms': 10,\n",
    "            'latency_budget_ms': 5\n",
    "        },\n",
    "        'HIGH': {\n",
    "            'reliability': 'RELIABLE',\n",
    "            'durability': 'VOLATILE',\n",
    "            'deadline_ms': 50,\n",
    "            'latency_budget_ms': 25\n",
    "        },\n",
    "        'MEDIUM': {\n",
    "            'reliability': 'RELIABLE',\n",
    "            'durability': 'VOLATILE',\n",
    "            'deadline_ms': 100,\n",
    "            'latency_budget_ms': 50\n",
    "        },\n",
    "        'LOW': {\n",
    "            'reliability': 'BEST_EFFORT',\n",
    "            'durability': 'VOLATILE',\n",
    "            'deadline_ms': 500,\n",
    "            'latency_budget_ms': 200\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, config: GraphConfig):\n",
    "        self.config = config\n",
    "        random.seed(config.seed)\n",
    "    \n",
    "    def generate(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate complete pub-sub system graph\"\"\"\n",
    "        print(f\"üîÑ Generating {self.config.scenario.value} scenario...\")\n",
    "        \n",
    "        graph = {\n",
    "            'metadata': {\n",
    "                'scenario': self.config.scenario.value,\n",
    "                'scale': self.config.scale,\n",
    "                'generated_at': datetime.now().isoformat(),\n",
    "                'seed': self.config.seed\n",
    "            },\n",
    "            'nodes': [],\n",
    "            'brokers': [],\n",
    "            'topics': [],\n",
    "            'applications': [],\n",
    "            'relationships': {\n",
    "                'publishes_to': [],\n",
    "                'subscribes_to': [],\n",
    "                'runs_on': [],\n",
    "                'routes': []\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Generate all components\n",
    "        self._generate_nodes(graph)\n",
    "        self._generate_brokers(graph)\n",
    "        self._generate_topics(graph)\n",
    "        self._generate_applications(graph)\n",
    "        \n",
    "        # Generate relationships\n",
    "        self._generate_runs_on(graph)\n",
    "        self._generate_routes(graph)\n",
    "        self._generate_pub_sub_relationships(graph)\n",
    "        \n",
    "        # Inject anti-patterns if specified\n",
    "        if self.config.antipatterns:\n",
    "            self._inject_antipatterns(graph)\n",
    "        \n",
    "        # Ensure connectivity\n",
    "        self._ensure_connectivity(graph)\n",
    "        \n",
    "        return graph\n",
    "    \n",
    "    def _generate_nodes(self, graph: Dict):\n",
    "        \"\"\"Generate infrastructure nodes\"\"\"\n",
    "        node_types = ['edge_gateway', 'fog_server', 'cloud_server', 'edge_device']\n",
    "        \n",
    "        for i in range(1, self.config.num_nodes + 1):\n",
    "            node_type = node_types[(i - 1) % len(node_types)]\n",
    "            zone = f'zone-{(i - 1) % 3 + 1}'\n",
    "            \n",
    "            graph['nodes'].append({\n",
    "                'id': f'N{i}',\n",
    "                'name': f'{node_type.replace(\"_\", \" \").title()} {i}',\n",
    "                'type': node_type,\n",
    "                'zone': zone,\n",
    "                'cpu_capacity': random.choice([4, 8, 16, 32]),\n",
    "                'memory_gb': random.choice([8, 16, 32, 64])\n",
    "            })\n",
    "    \n",
    "    def _generate_brokers(self, graph: Dict):\n",
    "        \"\"\"Generate message brokers\"\"\"\n",
    "        for i in range(1, self.config.num_brokers + 1):\n",
    "            graph['brokers'].append({\n",
    "                'id': f'B{i}',\n",
    "                'name': f'Broker-{i}',\n",
    "                'zone': f'zone-{(i - 1) % 3 + 1}',\n",
    "                'max_connections': random.choice([1000, 5000, 10000]),\n",
    "                'protocol': random.choice(['DDS', 'MQTT', 'AMQP'])\n",
    "            })\n",
    "    \n",
    "    def _generate_topics(self, graph: Dict):\n",
    "        \"\"\"Generate topics with QoS profiles\"\"\"\n",
    "        patterns = self.TOPIC_PATTERNS.get(self.config.scenario, self.TOPIC_PATTERNS[Scenario.GENERIC])\n",
    "        qos_levels = ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']\n",
    "        \n",
    "        for i in range(1, self.config.num_topics + 1):\n",
    "            pattern = patterns[(i - 1) % len(patterns)]\n",
    "            qos_level = qos_levels[(i - 1) % len(qos_levels)]\n",
    "            \n",
    "            # Adjust QoS based on scenario\n",
    "            if self.config.scenario == Scenario.FINANCIAL_TRADING:\n",
    "                if 'market' in pattern or 'order' in pattern:\n",
    "                    qos_level = 'CRITICAL'\n",
    "            elif self.config.scenario == Scenario.HEALTHCARE:\n",
    "                if 'vital' in pattern or 'alert' in pattern:\n",
    "                    qos_level = 'CRITICAL'\n",
    "            \n",
    "            graph['topics'].append({\n",
    "                'id': f'T{i}',\n",
    "                'name': f'{pattern}_{i}',\n",
    "                'qos': self.QOS_PROFILES[qos_level].copy(),\n",
    "                'qos_level': qos_level,\n",
    "                'message_rate_hz': self._get_message_rate(pattern),\n",
    "                'avg_message_size_bytes': self._get_message_size(pattern)\n",
    "            })\n",
    "    \n",
    "    def _generate_applications(self, graph: Dict):\n",
    "        \"\"\"Generate applications\"\"\"\n",
    "        app_types_list = self.APP_TYPES.get(self.config.scenario, self.APP_TYPES[Scenario.GENERIC])\n",
    "        criticality_levels = ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']\n",
    "        role_types = ['PRODUCER', 'CONSUMER', 'PROSUMER']\n",
    "        \n",
    "        for i in range(1, self.config.num_applications + 1):\n",
    "            app_type = app_types_list[(i - 1) % len(app_types_list)]\n",
    "            criticality = criticality_levels[(i - 1) % len(criticality_levels)]\n",
    "            role = role_types[(i - 1) % len(role_types)]\n",
    "            \n",
    "            graph['applications'].append({\n",
    "                'id': f'A{i}',\n",
    "                'name': f'{app_type}_{i}',\n",
    "                'type': role,\n",
    "                'criticality': criticality,\n",
    "                'replicas': 1 if criticality == 'LOW' else random.choice([2, 3])\n",
    "            })\n",
    "    \n",
    "    def _generate_runs_on(self, graph: Dict):\n",
    "        \"\"\"Generate application to node relationships\"\"\"\n",
    "        for app in graph['applications']:\n",
    "            node = random.choice(graph['nodes'])\n",
    "            graph['relationships']['runs_on'].append({\n",
    "                'from': app['id'],\n",
    "                'to': node['id']\n",
    "            })\n",
    "    \n",
    "    def _generate_routes(self, graph: Dict):\n",
    "        \"\"\"Generate topic to broker routing\"\"\"\n",
    "        for topic in graph['topics']:\n",
    "            num_brokers = random.randint(1, min(2, len(graph['brokers'])))\n",
    "            selected_brokers = random.sample(graph['brokers'], num_brokers)\n",
    "            for broker in selected_brokers:\n",
    "                graph['relationships']['routes'].append({\n",
    "                    'from': topic['id'],\n",
    "                    'to': broker['id']\n",
    "                })\n",
    "    \n",
    "    def _generate_pub_sub_relationships(self, graph: Dict):\n",
    "        \"\"\"Generate publish/subscribe relationships\"\"\"\n",
    "        for app in graph['applications']:\n",
    "            app_type = app['type']\n",
    "            \n",
    "            if app_type == 'PRODUCER':\n",
    "                num_publish, num_subscribe = random.randint(1, 3), 0\n",
    "            elif app_type == 'CONSUMER':\n",
    "                num_publish, num_subscribe = 0, random.randint(1, 4)\n",
    "            else:\n",
    "                num_publish, num_subscribe = random.randint(1, 2), random.randint(1, 3)\n",
    "            \n",
    "            if num_publish > 0:\n",
    "                for topic in random.sample(graph['topics'], min(num_publish, len(graph['topics']))):\n",
    "                    graph['relationships']['publishes_to'].append({'from': app['id'], 'to': topic['id']})\n",
    "            \n",
    "            if num_subscribe > 0:\n",
    "                for topic in random.sample(graph['topics'], min(num_subscribe, len(graph['topics']))):\n",
    "                    graph['relationships']['subscribes_to'].append({'from': app['id'], 'to': topic['id']})\n",
    "    \n",
    "    def _inject_antipatterns(self, graph: Dict):\n",
    "        \"\"\"Inject specified anti-patterns for analysis validation\"\"\"\n",
    "        for pattern in self.config.antipatterns:\n",
    "            if pattern == 'spof':\n",
    "                # Single Point of Failure: Route all topics through one broker\n",
    "                if len(graph['brokers']) > 1:\n",
    "                    spof_broker = graph['brokers'][0]\n",
    "                    for route in graph['relationships']['routes']:\n",
    "                        route['to'] = spof_broker['id']\n",
    "                    print(f\"  ‚ö†Ô∏è Injected SPOF: {spof_broker['id']}\")\n",
    "            \n",
    "            elif pattern == 'god_topic':\n",
    "                # God Topic: One topic has excessive subscribers\n",
    "                if graph['topics']:\n",
    "                    god_topic = graph['topics'][0]\n",
    "                    for app in graph['applications']:\n",
    "                        if random.random() < 0.7:\n",
    "                            graph['relationships']['subscribes_to'].append({\n",
    "                                'from': app['id'], 'to': god_topic['id']\n",
    "                            })\n",
    "                    print(f\"  ‚ö†Ô∏è Injected God Topic: {god_topic['id']}\")\n",
    "    \n",
    "    def _ensure_connectivity(self, graph: Dict):\n",
    "        \"\"\"Ensure all components are connected\"\"\"\n",
    "        for app in graph['applications']:\n",
    "            has_pub = any(r['from'] == app['id'] for r in graph['relationships']['publishes_to'])\n",
    "            has_sub = any(r['from'] == app['id'] for r in graph['relationships']['subscribes_to'])\n",
    "            if not has_pub and not has_sub:\n",
    "                topic = random.choice(graph['topics'])\n",
    "                graph['relationships']['subscribes_to'].append({'from': app['id'], 'to': topic['id']})\n",
    "    \n",
    "    def _get_message_rate(self, pattern: str) -> float:\n",
    "        \"\"\"Get realistic message rate based on scenario\"\"\"\n",
    "        if self.config.scenario == Scenario.FINANCIAL_TRADING:\n",
    "            return random.choice([100, 500, 1000]) if 'market' in pattern else random.choice([10, 50, 100])\n",
    "        elif self.config.scenario == Scenario.HEALTHCARE:\n",
    "            return random.choice([10, 20, 50]) if 'vital' in pattern else random.choice([1, 5, 10])\n",
    "        return random.choice([1, 10, 50, 100])\n",
    "    \n",
    "    def _get_message_size(self, pattern: str) -> int:\n",
    "        \"\"\"Get realistic message size based on scenario\"\"\"\n",
    "        if self.config.scenario == Scenario.FINANCIAL_TRADING:\n",
    "            return random.choice([64, 128, 256])\n",
    "        elif self.config.scenario == Scenario.IOT_SMART_CITY:\n",
    "            return random.choice([32, 64, 128])\n",
    "        return random.choice([128, 256, 512, 1024])\n",
    "\n",
    "\n",
    "print(\"‚úÖ PubSubGraphGenerator class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Sample Graph\n",
    "\n",
    "Let's generate an IoT Smart City scenario to demonstrate the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the graph generation\n",
    "config = GraphConfig(\n",
    "    scale='medium',\n",
    "    scenario=Scenario.IOT_SMART_CITY,\n",
    "    num_nodes=10,\n",
    "    num_applications=30,\n",
    "    num_topics=20,\n",
    "    num_brokers=3,\n",
    "    antipatterns=['spof'],  # Inject a Single Point of Failure\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Generate the graph\n",
    "generator = PubSubGraphGenerator(config)\n",
    "graph_data = generator.generate()\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nüìä Generated Graph Summary:\")\n",
    "print(f\"   ‚Ä¢ Infrastructure Nodes: {len(graph_data['nodes'])}\")\n",
    "print(f\"   ‚Ä¢ Message Brokers: {len(graph_data['brokers'])}\")\n",
    "print(f\"   ‚Ä¢ Topics: {len(graph_data['topics'])}\")\n",
    "print(f\"   ‚Ä¢ Applications: {len(graph_data['applications'])}\")\n",
    "print(f\"   ‚Ä¢ Publish Relationships: {len(graph_data['relationships']['publishes_to'])}\")\n",
    "print(f\"   ‚Ä¢ Subscribe Relationships: {len(graph_data['relationships']['subscribes_to'])}\")\n",
    "print(f\"   ‚Ä¢ Routing Relationships: {len(graph_data['relationships']['routes'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine sample components\n",
    "print(\"\\nüìã Sample Components:\\n\")\n",
    "\n",
    "print(\"Infrastructure Node:\")\n",
    "print(json.dumps(graph_data['nodes'][0], indent=2))\n",
    "\n",
    "print(\"\\nTopic with QoS:\")\n",
    "print(json.dumps(graph_data['topics'][0], indent=2))\n",
    "\n",
    "print(\"\\nApplication:\")\n",
    "print(json.dumps(graph_data['applications'][0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Neo4j Database Import\n",
    "\n",
    "For persistent storage and advanced graph queries, we can import the generated graph into Neo4j.\n",
    "\n",
    "**Note**: This step requires a running Neo4j instance. If unavailable, the pipeline continues with in-memory analysis.\n",
    "\n",
    "### Neo4j Schema\n",
    "\n",
    "```cypher\n",
    "// Node types\n",
    "(:Node {id, name, type, zone, cpu_capacity, memory_gb})\n",
    "(:Broker {id, name, zone, max_connections, protocol})\n",
    "(:Topic {id, name, qos_level, message_rate_hz})\n",
    "(:Application {id, name, type, criticality, replicas})\n",
    "\n",
    "// Relationships\n",
    "(Application)-[:RUNS_ON]->(Node)\n",
    "(Application)-[:PUBLISHES]->(Topic)\n",
    "(Application)-[:SUBSCRIBES]->(Topic)\n",
    "(Topic)-[:ROUTED_BY]->(Broker)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neo4jImporter:\n",
    "    \"\"\"\n",
    "    Imports graph data into Neo4j database for persistent storage and advanced queries.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, uri: str, user: str, password: str, database: str = 'neo4j'):\n",
    "        self.uri = uri\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.database = database\n",
    "        self.driver = None\n",
    "    \n",
    "    def connect(self) -> bool:\n",
    "        \"\"\"Establish connection to Neo4j\"\"\"\n",
    "        if not NEO4J_AVAILABLE:\n",
    "            print(\"‚ö†Ô∏è Neo4j driver not available\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            self.driver = GraphDatabase.driver(self.uri, auth=(self.user, self.password))\n",
    "            with self.driver.session(database=self.database) as session:\n",
    "                session.run(\"RETURN 1\")\n",
    "            print(\"‚úÖ Connected to Neo4j\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to connect: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close database connection\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.close()\n",
    "    \n",
    "    def clear_database(self):\n",
    "        \"\"\"Clear all data from database\"\"\"\n",
    "        if self.driver:\n",
    "            with self.driver.session(database=self.database) as session:\n",
    "                session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "            print(\"üóëÔ∏è Database cleared\")\n",
    "    \n",
    "    def create_schema(self):\n",
    "        \"\"\"Create database schema with constraints and indexes\"\"\"\n",
    "        if not self.driver:\n",
    "            return\n",
    "        \n",
    "        constraints = [\n",
    "            \"CREATE CONSTRAINT app_id IF NOT EXISTS FOR (a:Application) REQUIRE a.id IS UNIQUE\",\n",
    "            \"CREATE CONSTRAINT topic_id IF NOT EXISTS FOR (t:Topic) REQUIRE t.id IS UNIQUE\",\n",
    "            \"CREATE CONSTRAINT broker_id IF NOT EXISTS FOR (b:Broker) REQUIRE b.id IS UNIQUE\",\n",
    "            \"CREATE CONSTRAINT node_id IF NOT EXISTS FOR (n:Node) REQUIRE n.id IS UNIQUE\"\n",
    "        ]\n",
    "        \n",
    "        with self.driver.session(database=self.database) as session:\n",
    "            for constraint in constraints:\n",
    "                try:\n",
    "                    session.run(constraint)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        print(\"üìê Schema created\")\n",
    "    \n",
    "    def import_graph(self, graph_data: Dict):\n",
    "        \"\"\"Import complete graph data\"\"\"\n",
    "        if not self.driver:\n",
    "            print(\"‚ö†Ô∏è Not connected, skipping import\")\n",
    "            return\n",
    "        \n",
    "        print(\"üì• Importing graph data...\")\n",
    "        \n",
    "        with self.driver.session(database=self.database) as session:\n",
    "            # Import nodes\n",
    "            for node in graph_data.get('nodes', []):\n",
    "                session.run(\n",
    "                    \"MERGE (n:Node {id: $id}) SET n.name = $name, n.type = $type\",\n",
    "                    id=node['id'], name=node['name'], type=node.get('type', 'generic')\n",
    "                )\n",
    "            \n",
    "            # Import brokers\n",
    "            for broker in graph_data.get('brokers', []):\n",
    "                session.run(\n",
    "                    \"MERGE (b:Broker {id: $id}) SET b.name = $name\",\n",
    "                    id=broker['id'], name=broker['name']\n",
    "                )\n",
    "            \n",
    "            # Import topics\n",
    "            for topic in graph_data.get('topics', []):\n",
    "                session.run(\n",
    "                    \"MERGE (t:Topic {id: $id}) SET t.name = $name, t.qos_level = $qos\",\n",
    "                    id=topic['id'], name=topic['name'], qos=topic.get('qos_level', 'MEDIUM')\n",
    "                )\n",
    "            \n",
    "            # Import applications\n",
    "            for app in graph_data.get('applications', []):\n",
    "                session.run(\n",
    "                    \"MERGE (a:Application {id: $id}) SET a.name = $name, a.criticality = $crit\",\n",
    "                    id=app['id'], name=app['name'], crit=app.get('criticality', 'MEDIUM')\n",
    "                )\n",
    "            \n",
    "            # Import relationships\n",
    "            for rel in graph_data.get('relationships', {}).get('runs_on', []):\n",
    "                session.run(\n",
    "                    \"MATCH (a:Application {id: $from}), (n:Node {id: $to}) MERGE (a)-[:RUNS_ON]->(n)\",\n",
    "                    **rel\n",
    "                )\n",
    "            \n",
    "            for rel in graph_data.get('relationships', {}).get('publishes_to', []):\n",
    "                session.run(\n",
    "                    \"MATCH (a:Application {id: $from}), (t:Topic {id: $to}) MERGE (a)-[:PUBLISHES]->(t)\",\n",
    "                    **rel\n",
    "                )\n",
    "            \n",
    "            for rel in graph_data.get('relationships', {}).get('subscribes_to', []):\n",
    "                session.run(\n",
    "                    \"MATCH (a:Application {id: $from}), (t:Topic {id: $to}) MERGE (a)-[:SUBSCRIBES]->(t)\",\n",
    "                    **rel\n",
    "                )\n",
    "            \n",
    "            for rel in graph_data.get('relationships', {}).get('routes', []):\n",
    "                session.run(\n",
    "                    \"MATCH (t:Topic {id: $from}), (b:Broker {id: $to}) MERGE (t)-[:ROUTED_BY]->(b)\",\n",
    "                    **rel\n",
    "                )\n",
    "        \n",
    "        print(\"‚úÖ Graph imported successfully\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Neo4jImporter class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j import (optional - uncomment and configure if Neo4j is available)\n",
    "\n",
    "# neo4j_importer = Neo4jImporter(\n",
    "#     uri='bolt://localhost:7687',\n",
    "#     user='neo4j',\n",
    "#     password='password'\n",
    "# )\n",
    "\n",
    "# if neo4j_importer.connect():\n",
    "#     neo4j_importer.clear_database()\n",
    "#     neo4j_importer.create_schema()\n",
    "#     neo4j_importer.import_graph(graph_data)\n",
    "#     neo4j_importer.close()\n",
    "\n",
    "print(\"‚ÑπÔ∏è Neo4j import step (configure connection settings to enable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Graph Analysis\n",
    "\n",
    "This is the core analytical step where we apply graph theory algorithms to identify critical components.\n",
    "\n",
    "### Analysis Components\n",
    "\n",
    "1. **Centrality Metrics**\n",
    "   - Betweenness Centrality: Identifies nodes that act as bridges\n",
    "   - Degree Centrality: Measures connectivity\n",
    "   - PageRank: Importance based on incoming connections\n",
    "\n",
    "2. **Structural Analysis**\n",
    "   - Articulation Points: Nodes whose removal disconnects the graph\n",
    "   - Bridges: Edges whose removal disconnects the graph\n",
    "   - Cycle Detection: Identifies circular dependencies\n",
    "\n",
    "3. **Composite Criticality Score**\n",
    "   $$C_{score}(v) = \\alpha \\cdot C_B^{norm}(v) + \\beta \\cdot AP(v) + \\gamma \\cdot I(v)$$\n",
    "\n",
    "4. **Anti-Pattern Detection**\n",
    "   - SPOF (Single Points of Failure)\n",
    "   - God Topics (excessive connectivity)\n",
    "   - Circular Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive graph analysis for pub-sub systems.\n",
    "    \n",
    "    Implements the composite criticality scoring formula:\n",
    "    C_score(v) = Œ±¬∑C_B^norm(v) + Œ≤¬∑AP(v) + Œ≥¬∑I(v)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float = DEFAULT_ALPHA, \n",
    "                 beta: float = DEFAULT_BETA, \n",
    "                 gamma: float = DEFAULT_GAMMA):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def build_networkx_graph(self, graph_data: Dict) -> nx.DiGraph:\n",
    "        \"\"\"Convert graph data to NetworkX directed graph\"\"\"\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes by type\n",
    "        for node in graph_data.get('nodes', []):\n",
    "            G.add_node(node['id'], type='Node', name=node.get('name', node['id']),\n",
    "                      node_type=node.get('type', 'generic'))\n",
    "        \n",
    "        for broker in graph_data.get('brokers', []):\n",
    "            G.add_node(broker['id'], type='Broker', name=broker.get('name', broker['id']))\n",
    "        \n",
    "        for topic in graph_data.get('topics', []):\n",
    "            G.add_node(topic['id'], type='Topic', name=topic.get('name', topic['id']),\n",
    "                      qos_level=topic.get('qos_level', 'MEDIUM'))\n",
    "        \n",
    "        for app in graph_data.get('applications', []):\n",
    "            G.add_node(app['id'], type='Application', name=app.get('name', app['id']),\n",
    "                      app_type=app.get('type', 'PROSUMER'),\n",
    "                      criticality=app.get('criticality', 'MEDIUM'))\n",
    "        \n",
    "        # Add edges\n",
    "        relationships = graph_data.get('relationships', {})\n",
    "        for rel in relationships.get('runs_on', []):\n",
    "            G.add_edge(rel['from'], rel['to'], type='RUNS_ON')\n",
    "        for rel in relationships.get('publishes_to', []):\n",
    "            G.add_edge(rel['from'], rel['to'], type='PUBLISHES')\n",
    "        for rel in relationships.get('subscribes_to', []):\n",
    "            G.add_edge(rel['from'], rel['to'], type='SUBSCRIBES')\n",
    "        for rel in relationships.get('routes', []):\n",
    "            G.add_edge(rel['from'], rel['to'], type='ROUTES')\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def analyze(self, G: nx.DiGraph) -> Dict[str, Any]:\n",
    "        \"\"\"Run comprehensive analysis on the graph\"\"\"\n",
    "        print(\"üîç Running comprehensive analysis...\")\n",
    "        \n",
    "        results = {\n",
    "            'graph_summary': self._get_graph_summary(G),\n",
    "            'centrality_metrics': self._calculate_centrality(G),\n",
    "            'structural_analysis': self._analyze_structure(G),\n",
    "            'layer_analysis': self._analyze_layers(G),\n",
    "            'anti_patterns': self._detect_anti_patterns(G),\n",
    "            'criticality_scores': {}\n",
    "        }\n",
    "        \n",
    "        # Calculate composite criticality scores\n",
    "        results['criticality_scores'] = self._calculate_criticality_scores(\n",
    "            G, results['centrality_metrics'], results['structural_analysis']\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _get_graph_summary(self, G: nx.DiGraph) -> Dict:\n",
    "        \"\"\"Get summary statistics\"\"\"\n",
    "        node_types = defaultdict(int)\n",
    "        for node, data in G.nodes(data=True):\n",
    "            node_types[data.get('type', 'Unknown')] += 1\n",
    "        \n",
    "        return {\n",
    "            'total_nodes': G.number_of_nodes(),\n",
    "            'total_edges': G.number_of_edges(),\n",
    "            'node_types': dict(node_types),\n",
    "            'density': nx.density(G),\n",
    "            'is_connected': nx.is_weakly_connected(G),\n",
    "            'num_components': nx.number_weakly_connected_components(G)\n",
    "        }\n",
    "    \n",
    "    def _calculate_centrality(self, G: nx.DiGraph) -> Dict:\n",
    "        \"\"\"Calculate various centrality metrics\"\"\"\n",
    "        return {\n",
    "            'betweenness': nx.betweenness_centrality(G),\n",
    "            'in_degree': dict(G.in_degree()),\n",
    "            'out_degree': dict(G.out_degree()),\n",
    "            'pagerank': nx.pagerank(G) if G.number_of_nodes() > 0 else {},\n",
    "            'closeness': nx.closeness_centrality(G)\n",
    "        }\n",
    "    \n",
    "    def _analyze_structure(self, G: nx.DiGraph) -> Dict:\n",
    "        \"\"\"Analyze structural properties\"\"\"\n",
    "        G_undirected = G.to_undirected()\n",
    "        \n",
    "        # Find articulation points (SPOFs)\n",
    "        articulation_points = set()\n",
    "        if nx.is_connected(G_undirected):\n",
    "            articulation_points = set(nx.articulation_points(G_undirected))\n",
    "        \n",
    "        # Find bridges\n",
    "        bridges = set()\n",
    "        if nx.is_connected(G_undirected):\n",
    "            bridges = set(nx.bridges(G_undirected))\n",
    "        \n",
    "        # Detect cycles\n",
    "        try:\n",
    "            cycles = list(nx.simple_cycles(G))\n",
    "            has_cycles = len(cycles) > 0\n",
    "        except:\n",
    "            has_cycles = False\n",
    "            cycles = []\n",
    "        \n",
    "        return {\n",
    "            'articulation_points': articulation_points,\n",
    "            'num_articulation_points': len(articulation_points),\n",
    "            'bridges': bridges,\n",
    "            'num_bridges': len(bridges),\n",
    "            'has_cycles': has_cycles,\n",
    "            'num_cycles': min(len(cycles), 100)\n",
    "        }\n",
    "    \n",
    "    def _analyze_layers(self, G: nx.DiGraph) -> Dict:\n",
    "        \"\"\"Analyze graph by layer\"\"\"\n",
    "        layers = {'Application': [], 'Topic': [], 'Broker': [], 'Node': []}\n",
    "        \n",
    "        for node, data in G.nodes(data=True):\n",
    "            node_type = data.get('type', 'Unknown')\n",
    "            if node_type in layers:\n",
    "                layers[node_type].append(node)\n",
    "        \n",
    "        layer_stats = {}\n",
    "        for layer, nodes in layers.items():\n",
    "            if nodes:\n",
    "                subgraph = G.subgraph(nodes)\n",
    "                layer_stats[layer] = {\n",
    "                    'count': len(nodes),\n",
    "                    'edges': subgraph.number_of_edges(),\n",
    "                    'density': nx.density(subgraph) if len(nodes) > 1 else 0\n",
    "                }\n",
    "        \n",
    "        return layer_stats\n",
    "    \n",
    "    def _detect_anti_patterns(self, G: nx.DiGraph) -> Dict:\n",
    "        \"\"\"Detect common anti-patterns\"\"\"\n",
    "        anti_patterns = {\n",
    "            'spof_candidates': [],\n",
    "            'god_topics': [],\n",
    "            'isolated_components': [],\n",
    "            'circular_dependencies': []\n",
    "        }\n",
    "        \n",
    "        # SPOF: High betweenness centrality\n",
    "        betweenness = nx.betweenness_centrality(G)\n",
    "        for node, bc in betweenness.items():\n",
    "            if bc > 0.3:\n",
    "                anti_patterns['spof_candidates'].append({'node': node, 'betweenness': bc})\n",
    "        \n",
    "        # God Topics: Excessive connections\n",
    "        for node, data in G.nodes(data=True):\n",
    "            if data.get('type') == 'Topic':\n",
    "                in_edges = G.in_degree(node)\n",
    "                if in_edges > 10:\n",
    "                    anti_patterns['god_topics'].append({'topic': node, 'connections': in_edges})\n",
    "        \n",
    "        # Circular dependencies\n",
    "        try:\n",
    "            cycles = list(nx.simple_cycles(G))\n",
    "            anti_patterns['circular_dependencies'] = cycles[:10]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return anti_patterns\n",
    "    \n",
    "    def _calculate_criticality_scores(self, G: nx.DiGraph, \n",
    "                                      centrality: Dict, \n",
    "                                      structural: Dict) -> Dict[str, CriticalityScore]:\n",
    "        \"\"\"\n",
    "        Calculate composite criticality scores using:\n",
    "        C_score(v) = Œ±¬∑C_B^norm(v) + Œ≤¬∑AP(v) + Œ≥¬∑I(v)\n",
    "        \"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        betweenness = centrality['betweenness']\n",
    "        articulation_points = structural['articulation_points']\n",
    "        \n",
    "        # Normalize betweenness\n",
    "        max_bc = max(betweenness.values()) if betweenness else 1\n",
    "        min_bc = min(betweenness.values()) if betweenness else 0\n",
    "        \n",
    "        for node in G.nodes():\n",
    "            node_data = G.nodes[node]\n",
    "            node_type = node_data.get('type', 'Unknown')\n",
    "            \n",
    "            # Normalized betweenness centrality\n",
    "            bc_norm = (betweenness.get(node, 0) - min_bc) / (max_bc - min_bc) if max_bc > min_bc else 0\n",
    "            \n",
    "            # Articulation point indicator\n",
    "            ap = 1.0 if node in articulation_points else 0.0\n",
    "            \n",
    "            # Impact score (based on reachability loss)\n",
    "            impact = self._calculate_impact_score(G, node)\n",
    "            \n",
    "            # Composite score: C = Œ±¬∑BC + Œ≤¬∑AP + Œ≥¬∑I\n",
    "            composite = self.alpha * bc_norm + self.beta * ap + self.gamma * impact\n",
    "            \n",
    "            # Determine criticality level\n",
    "            if composite >= 0.8:\n",
    "                level = CriticalityLevel.CRITICAL\n",
    "            elif composite >= 0.6:\n",
    "                level = CriticalityLevel.HIGH\n",
    "            elif composite >= 0.4:\n",
    "                level = CriticalityLevel.MEDIUM\n",
    "            elif composite >= 0.2:\n",
    "                level = CriticalityLevel.LOW\n",
    "            else:\n",
    "                level = CriticalityLevel.MINIMAL\n",
    "            \n",
    "            scores[node] = CriticalityScore(\n",
    "                component_id=node,\n",
    "                component_type=node_type,\n",
    "                betweenness_centrality=bc_norm,\n",
    "                is_articulation_point=node in articulation_points,\n",
    "                impact_score=impact,\n",
    "                composite_score=composite,\n",
    "                criticality_level=level\n",
    "            )\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def _calculate_impact_score(self, G: nx.DiGraph, node: str) -> float:\n",
    "        \"\"\"Calculate impact score based on reachability loss when node is removed\"\"\"\n",
    "        if G.number_of_nodes() <= 1:\n",
    "            return 0.0\n",
    "        \n",
    "        # Create graph without the node\n",
    "        G_removed = G.copy()\n",
    "        G_removed.remove_node(node)\n",
    "        \n",
    "        if G_removed.number_of_nodes() == 0:\n",
    "            return 1.0\n",
    "        \n",
    "        # Calculate reachability loss\n",
    "        original_pairs = G.number_of_nodes() * (G.number_of_nodes() - 1)\n",
    "        \n",
    "        if nx.is_weakly_connected(G_removed):\n",
    "            remaining_pairs = G_removed.number_of_nodes() * (G_removed.number_of_nodes() - 1)\n",
    "        else:\n",
    "            remaining_pairs = sum(\n",
    "                len(comp) * (len(comp) - 1) \n",
    "                for comp in nx.weakly_connected_components(G_removed)\n",
    "            )\n",
    "        \n",
    "        reachability_loss = 1 - (remaining_pairs / original_pairs) if original_pairs > 0 else 0\n",
    "        return min(1.0, reachability_loss)\n",
    "\n",
    "\n",
    "print(\"‚úÖ GraphAnalyzer class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis on our generated graph\n",
    "analyzer = GraphAnalyzer(alpha=0.4, beta=0.3, gamma=0.3)\n",
    "\n",
    "# Build NetworkX graph\n",
    "G = analyzer.build_networkx_graph(graph_data)\n",
    "\n",
    "# Run comprehensive analysis\n",
    "analysis_results = analyzer.analyze(G)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìä Analysis Results:\")\n",
    "print(f\"\\nGraph Summary:\")\n",
    "for key, value in analysis_results['graph_summary'].items():\n",
    "    print(f\"   ‚Ä¢ {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display layer analysis\n",
    "print(\"\\nüìä Layer Analysis:\")\n",
    "for layer, stats in analysis_results['layer_analysis'].items():\n",
    "    print(f\"   {layer}: {stats['count']} nodes, {stats['edges']} edges, density={stats['density']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display structural analysis\n",
    "structural = analysis_results['structural_analysis']\n",
    "print(\"\\nüîß Structural Analysis:\")\n",
    "print(f\"   ‚Ä¢ Articulation Points (SPOFs): {structural['num_articulation_points']}\")\n",
    "print(f\"   ‚Ä¢ Bridges: {structural['num_bridges']}\")\n",
    "print(f\"   ‚Ä¢ Has Cycles: {structural['has_cycles']}\")\n",
    "print(f\"   ‚Ä¢ Number of Cycles: {structural['num_cycles']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 10 critical components\n",
    "criticality_scores = analysis_results['criticality_scores']\n",
    "sorted_scores = sorted(criticality_scores.items(), key=lambda x: x[1].composite_score, reverse=True)[:10]\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Top 10 Critical Components:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Rank':<6}{'Component':<12}{'Type':<15}{'Score':<10}{'Level':<12}{'Is AP'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, (node_id, score) in enumerate(sorted_scores, 1):\n",
    "    print(f\"{i:<6}{node_id:<12}{score.component_type:<15}{score.composite_score:<10.3f}{score.criticality_level.value:<12}{score.is_articulation_point}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count criticality level distribution\n",
    "level_counts = defaultdict(int)\n",
    "for score in criticality_scores.values():\n",
    "    level_counts[score.criticality_level.value] += 1\n",
    "\n",
    "print(\"\\nüìà Criticality Distribution:\")\n",
    "for level in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'MINIMAL']:\n",
    "    count = level_counts.get(level, 0)\n",
    "    bar = '‚ñà' * count + '‚ñë' * (20 - min(count, 20))\n",
    "    print(f\"   {level:<10}: {bar} {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display anti-patterns detected\n",
    "anti_patterns = analysis_results['anti_patterns']\n",
    "print(\"\\nüêõ Anti-Patterns Detected:\")\n",
    "print(f\"   ‚Ä¢ SPOF Candidates: {len(anti_patterns['spof_candidates'])}\")\n",
    "print(f\"   ‚Ä¢ God Topics: {len(anti_patterns['god_topics'])}\")\n",
    "print(f\"   ‚Ä¢ Circular Dependencies: {len(anti_patterns['circular_dependencies'])}\")\n",
    "\n",
    "if anti_patterns['spof_candidates']:\n",
    "    print(\"\\n   SPOF Details:\")\n",
    "    for spof in anti_patterns['spof_candidates'][:5]:\n",
    "        print(f\"      ‚Üí {spof['node']}: betweenness={spof['betweenness']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Simulation and Validation\n",
    "\n",
    "This step validates our analysis predictions through simulation:\n",
    "\n",
    "1. **Baseline Simulation**: Run normal traffic to establish baseline metrics\n",
    "2. **Failure Injection**: Simulate component failures\n",
    "3. **Impact Measurement**: Measure actual impact on system performance\n",
    "4. **Validation**: Compare predictions to actual outcomes\n",
    "\n",
    "### Validation Metrics\n",
    "\n",
    "- **Precision**: True Positives / (True Positives + False Positives)\n",
    "- **Recall**: True Positives / (True Positives + False Negatives)\n",
    "- **F1 Score**: 2 √ó (Precision √ó Recall) / (Precision + Recall)\n",
    "- **Spearman Correlation**: Rank correlation between predicted and actual impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulationEngine:\n",
    "    \"\"\"\n",
    "    Lightweight event-driven simulation for pub-sub systems.\n",
    "    \n",
    "    Features:\n",
    "    - Baseline traffic simulation\n",
    "    - Failure injection\n",
    "    - Cascading failure propagation\n",
    "    - Performance impact measurement\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, G: nx.DiGraph, graph_data: Dict):\n",
    "        self.G = G\n",
    "        self.graph_data = graph_data\n",
    "        \n",
    "        # Simulation state\n",
    "        self.messages_sent = 0\n",
    "        self.messages_delivered = 0\n",
    "        self.messages_dropped = 0\n",
    "        self.total_latency_ms = 0.0\n",
    "        self.active_failures = set()\n",
    "    \n",
    "    async def run_baseline_simulation(self, duration: int = 10) -> Dict:\n",
    "        \"\"\"Run baseline simulation without failures\"\"\"\n",
    "        print(f\"üîÑ Running baseline simulation ({duration}s)...\")\n",
    "        self._reset_stats()\n",
    "        await self._simulate_traffic(duration)\n",
    "        return self._get_stats()\n",
    "    \n",
    "    async def run_failure_simulation(self, \n",
    "                                     duration: int = 60,\n",
    "                                     failure_time: int = 30,\n",
    "                                     failure_components: List[str] = None,\n",
    "                                     enable_cascading: bool = True) -> Dict:\n",
    "        \"\"\"Run simulation with failure injection\"\"\"\n",
    "        print(f\"üí• Running failure simulation ({duration}s, failure at {failure_time}s)...\")\n",
    "        self._reset_stats()\n",
    "        \n",
    "        # Pre-failure phase\n",
    "        await self._simulate_traffic(failure_time)\n",
    "        pre_failure_stats = self._get_stats()\n",
    "        \n",
    "        # Inject failures\n",
    "        if failure_components:\n",
    "            print(f\"   Injecting failures: {failure_components}\")\n",
    "            for comp in failure_components:\n",
    "                self.active_failures.add(comp)\n",
    "                if enable_cascading:\n",
    "                    cascaded = self._propagate_failure(comp)\n",
    "                    self.active_failures.update(cascaded)\n",
    "                    if cascaded:\n",
    "                        print(f\"   Cascaded to: {cascaded}\")\n",
    "        \n",
    "        # Post-failure phase\n",
    "        await self._simulate_traffic(duration - failure_time)\n",
    "        post_failure_stats = self._get_stats()\n",
    "        \n",
    "        # Calculate impact\n",
    "        impact = self._calculate_failure_impact(pre_failure_stats, post_failure_stats)\n",
    "        \n",
    "        return {\n",
    "            'pre_failure': pre_failure_stats,\n",
    "            'post_failure': post_failure_stats,\n",
    "            'failed_components': list(self.active_failures),\n",
    "            'impact': impact\n",
    "        }\n",
    "    \n",
    "    async def _simulate_traffic(self, duration: int):\n",
    "        \"\"\"Simulate message traffic (time-compressed)\"\"\"\n",
    "        topics = self.graph_data.get('topics', [])\n",
    "        apps = self.graph_data.get('applications', [])\n",
    "        \n",
    "        total_rate = sum(t.get('message_rate_hz', 10) for t in topics)\n",
    "        messages_per_second = min(total_rate, 1000)\n",
    "        sim_steps = duration * 10\n",
    "        \n",
    "        for step in range(sim_steps):\n",
    "            messages_this_step = int(messages_per_second / 10)\n",
    "            \n",
    "            for _ in range(messages_this_step):\n",
    "                publishers = [a for a in apps if a.get('type') in ['PRODUCER', 'PROSUMER']]\n",
    "                if not publishers:\n",
    "                    continue\n",
    "                \n",
    "                publisher = random.choice(publishers)\n",
    "                \n",
    "                if publisher['id'] in self.active_failures:\n",
    "                    self.messages_dropped += 1\n",
    "                    continue\n",
    "                \n",
    "                pub_rels = [r for r in self.graph_data['relationships']['publishes_to'] \n",
    "                           if r['from'] == publisher['id']]\n",
    "                if not pub_rels:\n",
    "                    continue\n",
    "                \n",
    "                topic_id = random.choice(pub_rels)['to']\n",
    "                \n",
    "                if topic_id in self.active_failures:\n",
    "                    self.messages_dropped += 1\n",
    "                    continue\n",
    "                \n",
    "                sub_rels = [r for r in self.graph_data['relationships']['subscribes_to']\n",
    "                           if r['to'] == topic_id]\n",
    "                \n",
    "                self.messages_sent += 1\n",
    "                \n",
    "                for sub_rel in sub_rels:\n",
    "                    subscriber_id = sub_rel['from']\n",
    "                    if subscriber_id in self.active_failures:\n",
    "                        self.messages_dropped += 1\n",
    "                    else:\n",
    "                        self.messages_delivered += 1\n",
    "                        base_latency = random.uniform(1, 20)\n",
    "                        if self.active_failures:\n",
    "                            base_latency *= 1.5\n",
    "                        self.total_latency_ms += base_latency\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                await asyncio.sleep(0.001)\n",
    "    \n",
    "    def _propagate_failure(self, component: str) -> Set[str]:\n",
    "        \"\"\"Propagate failure to dependent components\"\"\"\n",
    "        cascaded = set()\n",
    "        if component in self.G:\n",
    "            for successor in self.G.successors(component):\n",
    "                if random.random() < 0.5:\n",
    "                    cascaded.add(successor)\n",
    "        return cascaded\n",
    "    \n",
    "    def _reset_stats(self):\n",
    "        \"\"\"Reset simulation statistics\"\"\"\n",
    "        self.messages_sent = 0\n",
    "        self.messages_delivered = 0\n",
    "        self.messages_dropped = 0\n",
    "        self.total_latency_ms = 0.0\n",
    "        self.active_failures.clear()\n",
    "    \n",
    "    def _get_stats(self) -> Dict:\n",
    "        \"\"\"Get current statistics\"\"\"\n",
    "        delivery_rate = self.messages_delivered / self.messages_sent if self.messages_sent > 0 else 0\n",
    "        avg_latency = self.total_latency_ms / self.messages_delivered if self.messages_delivered > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'messages_sent': self.messages_sent,\n",
    "            'messages_delivered': self.messages_delivered,\n",
    "            'messages_dropped': self.messages_dropped,\n",
    "            'delivery_rate': delivery_rate,\n",
    "            'avg_latency_ms': avg_latency\n",
    "        }\n",
    "    \n",
    "    def _calculate_failure_impact(self, pre: Dict, post: Dict) -> Dict:\n",
    "        \"\"\"Calculate impact of failures\"\"\"\n",
    "        latency_increase = post['avg_latency_ms'] - pre['avg_latency_ms']\n",
    "        delivery_decrease = pre['delivery_rate'] - post['delivery_rate']\n",
    "        \n",
    "        return {\n",
    "            'latency_increase_ms': latency_increase,\n",
    "            'latency_increase_pct': (latency_increase / pre['avg_latency_ms'] * 100) if pre['avg_latency_ms'] > 0 else 0,\n",
    "            'delivery_rate_decrease': delivery_decrease,\n",
    "            'messages_lost': post['messages_dropped'] - pre['messages_dropped'],\n",
    "            'affected_components': len(self.active_failures)\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"‚úÖ SimulationEngine class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationEngine:\n",
    "    \"\"\"\n",
    "    Validates analysis predictions against simulation outcomes.\n",
    "    \n",
    "    Computes:\n",
    "    - Precision, Recall, F1 Score\n",
    "    - Spearman rank correlation\n",
    "    \"\"\"\n",
    "    \n",
    "    def validate(self, analysis_results: Dict, simulation_results: Dict) -> ValidationResult:\n",
    "        \"\"\"Validate analysis predictions against simulation\"\"\"\n",
    "        print(\"‚úÖ Validating analysis results...\")\n",
    "        \n",
    "        criticality_scores = analysis_results.get('criticality_scores', {})\n",
    "        \n",
    "        # Predicted critical: HIGH or CRITICAL from analysis\n",
    "        predicted_critical = {\n",
    "            node_id for node_id, score in criticality_scores.items()\n",
    "            if score.criticality_level in [CriticalityLevel.CRITICAL, CriticalityLevel.HIGH]\n",
    "        }\n",
    "        \n",
    "        # Actual critical: Failed components + high impact components\n",
    "        failed_components = set(simulation_results.get('failed_components', []))\n",
    "        actual_critical = failed_components.copy()\n",
    "        \n",
    "        for node_id, score in criticality_scores.items():\n",
    "            if score.impact_score > 0.3:\n",
    "                actual_critical.add(node_id)\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        if predicted_critical:\n",
    "            true_positives = len(predicted_critical & actual_critical)\n",
    "            precision = true_positives / len(predicted_critical)\n",
    "        else:\n",
    "            precision = 0.0\n",
    "        \n",
    "        if actual_critical:\n",
    "            true_positives = len(predicted_critical & actual_critical)\n",
    "            recall = true_positives / len(actual_critical)\n",
    "        else:\n",
    "            recall = 1.0\n",
    "        \n",
    "        # F1 Score\n",
    "        f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Spearman correlation\n",
    "        spearman = self._calculate_spearman(criticality_scores, simulation_results)\n",
    "        \n",
    "        # Check targets\n",
    "        targets_met = {\n",
    "            'precision': precision >= TARGET_PRECISION,\n",
    "            'recall': recall >= TARGET_RECALL,\n",
    "            'f1_score': f1 >= TARGET_F1_SCORE,\n",
    "            'spearman': spearman >= TARGET_SPEARMAN_CORRELATION\n",
    "        }\n",
    "        \n",
    "        return ValidationResult(\n",
    "            precision=precision,\n",
    "            recall=recall,\n",
    "            f1_score=f1,\n",
    "            spearman_correlation=spearman,\n",
    "            targets_met=targets_met\n",
    "        )\n",
    "    \n",
    "    def _calculate_spearman(self, criticality_scores: Dict, simulation_results: Dict) -> float:\n",
    "        \"\"\"Calculate Spearman rank correlation\"\"\"\n",
    "        nodes = list(criticality_scores.keys())\n",
    "        predicted = [criticality_scores[n].composite_score for n in nodes]\n",
    "        \n",
    "        failed = set(simulation_results.get('failed_components', []))\n",
    "        actual = [1.0 if n in failed else criticality_scores[n].impact_score for n in nodes]\n",
    "        \n",
    "        if len(predicted) < 3:\n",
    "            return 0.0\n",
    "        \n",
    "        if SCIPY_AVAILABLE:\n",
    "            try:\n",
    "                correlation, _ = scipy_stats.spearmanr(predicted, actual)\n",
    "                return correlation if not math.isnan(correlation) else 0.0\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Fallback: simple rank correlation\n",
    "        return self._simple_spearman(predicted, actual)\n",
    "    \n",
    "    def _simple_spearman(self, x: List[float], y: List[float]) -> float:\n",
    "        \"\"\"Simple Spearman correlation calculation\"\"\"\n",
    "        n = len(x)\n",
    "        if n < 2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Compute ranks\n",
    "        def rank(vals):\n",
    "            sorted_idx = sorted(range(len(vals)), key=lambda i: vals[i], reverse=True)\n",
    "            ranks = [0] * len(vals)\n",
    "            for r, idx in enumerate(sorted_idx):\n",
    "                ranks[idx] = r + 1\n",
    "            return ranks\n",
    "        \n",
    "        rx, ry = rank(x), rank(y)\n",
    "        d_squared = sum((rx[i] - ry[i]) ** 2 for i in range(n))\n",
    "        \n",
    "        return 1 - (6 * d_squared) / (n * (n**2 - 1))\n",
    "\n",
    "\n",
    "print(\"‚úÖ ValidationEngine class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation and validation\n",
    "\n",
    "# Select failure target (highest criticality component)\n",
    "sorted_scores = sorted(criticality_scores.items(), key=lambda x: x[1].composite_score, reverse=True)\n",
    "failure_target = sorted_scores[0][0] if sorted_scores else None\n",
    "\n",
    "print(f\"\\nüéØ Selected failure target: {failure_target}\")\n",
    "print(f\"   Criticality Score: {sorted_scores[0][1].composite_score:.3f}\")\n",
    "print(f\"   Type: {sorted_scores[0][1].component_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simulation engine and run simulations\n",
    "simulation = SimulationEngine(G, graph_data)\n",
    "\n",
    "# Run baseline simulation\n",
    "baseline_results = await simulation.run_baseline_simulation(duration=10)\n",
    "\n",
    "print(f\"\\nüìä Baseline Results:\")\n",
    "print(f\"   ‚Ä¢ Messages Sent: {baseline_results['messages_sent']:,}\")\n",
    "print(f\"   ‚Ä¢ Messages Delivered: {baseline_results['messages_delivered']:,}\")\n",
    "print(f\"   ‚Ä¢ Delivery Rate: {baseline_results['delivery_rate']:.1%}\")\n",
    "print(f\"   ‚Ä¢ Avg Latency: {baseline_results['avg_latency_ms']:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run failure simulation\n",
    "failure_results = await simulation.run_failure_simulation(\n",
    "    duration=60,\n",
    "    failure_time=30,\n",
    "    failure_components=[failure_target] if failure_target else [],\n",
    "    enable_cascading=True\n",
    ")\n",
    "\n",
    "print(f\"\\nüí• Failure Simulation Results:\")\n",
    "print(f\"\\n   Pre-Failure:\")\n",
    "print(f\"      ‚Ä¢ Delivery Rate: {failure_results['pre_failure']['delivery_rate']:.1%}\")\n",
    "print(f\"      ‚Ä¢ Avg Latency: {failure_results['pre_failure']['avg_latency_ms']:.2f}ms\")\n",
    "\n",
    "print(f\"\\n   Post-Failure:\")\n",
    "print(f\"      ‚Ä¢ Delivery Rate: {failure_results['post_failure']['delivery_rate']:.1%}\")\n",
    "print(f\"      ‚Ä¢ Avg Latency: {failure_results['post_failure']['avg_latency_ms']:.2f}ms\")\n",
    "\n",
    "print(f\"\\n   Impact:\")\n",
    "impact = failure_results['impact']\n",
    "print(f\"      ‚Ä¢ Latency Increase: +{impact['latency_increase_pct']:.1f}%\")\n",
    "print(f\"      ‚Ä¢ Components Affected: {impact['affected_components']}\")\n",
    "print(f\"      ‚Ä¢ Failed Components: {failure_results['failed_components']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate predictions\n",
    "validator = ValidationEngine()\n",
    "validation_result = validator.validate(analysis_results, failure_results)\n",
    "\n",
    "print(\"\\n‚úÖ Validation Results:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Metric':<25}{'Value':<15}{'Target':<15}{'Status'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "metrics = [\n",
    "    ('Precision', validation_result.precision, TARGET_PRECISION, 'precision'),\n",
    "    ('Recall', validation_result.recall, TARGET_RECALL, 'recall'),\n",
    "    ('F1 Score', validation_result.f1_score, TARGET_F1_SCORE, 'f1_score'),\n",
    "    ('Spearman Correlation', validation_result.spearman_correlation, TARGET_SPEARMAN_CORRELATION, 'spearman')\n",
    "]\n",
    "\n",
    "for name, value, target, key in metrics:\n",
    "    status = \"‚úÖ Met\" if validation_result.targets_met[key] else \"‚ùå Not Met\"\n",
    "    print(f\"{name:<25}{value:<15.3f}‚â•{target:<14}{status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Visualization\n",
    "\n",
    "The final step generates comprehensive visualizations:\n",
    "\n",
    "1. **Interactive Graph**: Vis.js-based interactive network visualization\n",
    "2. **Multi-Layer View**: Separation of Application, Topic, Broker, Infrastructure layers\n",
    "3. **Dashboard**: Metrics and validation results summary\n",
    "4. **Report**: Markdown documentation of findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For notebook display, let's create a simple matplotlib visualization\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as mpatches\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    \n",
    "    # Color nodes by type\n",
    "    type_colors = {\n",
    "        'Application': '#3498db',\n",
    "        'Topic': '#2ecc71',\n",
    "        'Broker': '#e74c3c',\n",
    "        'Node': '#9b59b6'\n",
    "    }\n",
    "    \n",
    "    node_colors = [type_colors.get(G.nodes[n].get('type', 'Unknown'), '#95a5a6') for n in G.nodes()]\n",
    "    \n",
    "    # Left plot: By component type\n",
    "    pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
    "    nx.draw(G, pos, ax=axes[0], node_color=node_colors, node_size=300, \n",
    "            with_labels=True, font_size=8, arrows=True, alpha=0.8)\n",
    "    axes[0].set_title('Graph by Component Type', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Legend for left plot\n",
    "    patches = [mpatches.Patch(color=color, label=label) for label, color in type_colors.items()]\n",
    "    axes[0].legend(handles=patches, loc='upper left')\n",
    "    \n",
    "    # Right plot: By criticality\n",
    "    crit_colors = {\n",
    "        'CRITICAL': '#e74c3c',\n",
    "        'HIGH': '#e67e22',\n",
    "        'MEDIUM': '#f1c40f',\n",
    "        'LOW': '#27ae60',\n",
    "        'MINIMAL': '#95a5a6'\n",
    "    }\n",
    "    \n",
    "    node_crit_colors = []\n",
    "    for n in G.nodes():\n",
    "        score = criticality_scores.get(n)\n",
    "        if score:\n",
    "            node_crit_colors.append(crit_colors.get(score.criticality_level.value, '#95a5a6'))\n",
    "        else:\n",
    "            node_crit_colors.append('#95a5a6')\n",
    "    \n",
    "    nx.draw(G, pos, ax=axes[1], node_color=node_crit_colors, node_size=300,\n",
    "            with_labels=True, font_size=8, arrows=True, alpha=0.8)\n",
    "    axes[1].set_title('Graph by Criticality Level', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Legend for right plot\n",
    "    patches = [mpatches.Patch(color=color, label=label) for label, color in crit_colors.items()]\n",
    "    axes[1].legend(handles=patches, loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Visualization generated!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è matplotlib not available - skipping visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create criticality distribution bar chart\n",
    "try:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Criticality distribution\n",
    "    levels = ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'MINIMAL']\n",
    "    counts = [level_counts.get(l, 0) for l in levels]\n",
    "    colors = ['#e74c3c', '#e67e22', '#f1c40f', '#27ae60', '#95a5a6']\n",
    "    \n",
    "    axes[0].bar(levels, counts, color=colors)\n",
    "    axes[0].set_xlabel('Criticality Level')\n",
    "    axes[0].set_ylabel('Number of Components')\n",
    "    axes[0].set_title('Criticality Distribution', fontweight='bold')\n",
    "    \n",
    "    # Validation metrics\n",
    "    metrics_names = ['Precision', 'Recall', 'F1 Score', 'Spearman']\n",
    "    metrics_values = [\n",
    "        validation_result.precision,\n",
    "        validation_result.recall,\n",
    "        validation_result.f1_score,\n",
    "        validation_result.spearman_correlation\n",
    "    ]\n",
    "    targets = [TARGET_PRECISION, TARGET_RECALL, TARGET_F1_SCORE, TARGET_SPEARMAN_CORRELATION]\n",
    "    \n",
    "    x = range(len(metrics_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = axes[1].bar([i - width/2 for i in x], metrics_values, width, label='Actual', color='#3498db')\n",
    "    bars2 = axes[1].bar([i + width/2 for i in x], targets, width, label='Target', color='#e74c3c', alpha=0.5)\n",
    "    \n",
    "    axes[1].set_xlabel('Metric')\n",
    "    axes[1].set_ylabel('Value')\n",
    "    axes[1].set_title('Validation Metrics vs Targets', fontweight='bold')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(metrics_names)\n",
    "    axes[1].legend()\n",
    "    axes[1].set_ylim(0, 1.2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è matplotlib not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete Pipeline Execution\n",
    "\n",
    "Here's how to run the entire pipeline with a single function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_complete_pipeline(scenario: Scenario = Scenario.IOT_SMART_CITY,\n",
    "                                 scale: str = 'medium',\n",
    "                                 seed: int = 42,\n",
    "                                 antipatterns: List[str] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Run the complete end-to-end analysis pipeline.\n",
    "    \n",
    "    Steps:\n",
    "    1. Generate graph data\n",
    "    2. Build NetworkX graph\n",
    "    3. Run comprehensive analysis\n",
    "    4. Simulate traffic and failures\n",
    "    5. Validate predictions\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all results\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"  GRAPH-BASED MODELING AND ANALYSIS PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nScenario: {scenario.value}\")\n",
    "    print(f\"Scale: {scale}\")\n",
    "    print(f\"Seed: {seed}\\n\")\n",
    "    \n",
    "    results = {}\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step 1: Generate\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"STEP 1: GENERATE GRAPH DATA\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    scale_params = PubSubGraphGenerator.SCALES.get(scale, PubSubGraphGenerator.SCALES['medium'])\n",
    "    config = GraphConfig(\n",
    "        scale=scale,\n",
    "        scenario=scenario,\n",
    "        num_nodes=scale_params['nodes'],\n",
    "        num_applications=scale_params['apps'],\n",
    "        num_topics=scale_params['topics'],\n",
    "        num_brokers=scale_params['brokers'],\n",
    "        antipatterns=antipatterns or ['spof'],\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    generator = PubSubGraphGenerator(config)\n",
    "    graph_data = generator.generate()\n",
    "    results['graph_data'] = graph_data\n",
    "    \n",
    "    print(f\"‚úÖ Generated: {len(graph_data['nodes'])} nodes, {len(graph_data['applications'])} apps, \"\n",
    "          f\"{len(graph_data['topics'])} topics, {len(graph_data['brokers'])} brokers\")\n",
    "    \n",
    "    # Step 2 & 3: Analyze\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"STEP 2-3: BUILD GRAPH AND ANALYZE\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    analyzer = GraphAnalyzer(alpha=0.4, beta=0.3, gamma=0.3)\n",
    "    G = analyzer.build_networkx_graph(graph_data)\n",
    "    analysis_results = analyzer.analyze(G)\n",
    "    results['analysis'] = analysis_results\n",
    "    results['graph'] = G\n",
    "    \n",
    "    print(f\"‚úÖ Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "    print(f\"‚úÖ Articulation Points: {analysis_results['structural_analysis']['num_articulation_points']}\")\n",
    "    \n",
    "    # Count criticality\n",
    "    levels = defaultdict(int)\n",
    "    for score in analysis_results['criticality_scores'].values():\n",
    "        levels[score.criticality_level.value] += 1\n",
    "    print(f\"‚úÖ Criticality: CRITICAL={levels['CRITICAL']}, HIGH={levels['HIGH']}, \"\n",
    "          f\"MEDIUM={levels['MEDIUM']}, LOW={levels['LOW']}\")\n",
    "    \n",
    "    # Step 4: Simulate and Validate\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"STEP 4: SIMULATE AND VALIDATE\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # Select failure target\n",
    "    sorted_scores = sorted(\n",
    "        analysis_results['criticality_scores'].items(),\n",
    "        key=lambda x: x[1].composite_score,\n",
    "        reverse=True\n",
    "    )\n",
    "    failure_target = sorted_scores[0][0] if sorted_scores else None\n",
    "    \n",
    "    simulation = SimulationEngine(G, graph_data)\n",
    "    baseline = await simulation.run_baseline_simulation(10)\n",
    "    failure_results = await simulation.run_failure_simulation(\n",
    "        duration=60,\n",
    "        failure_time=30,\n",
    "        failure_components=[failure_target] if failure_target else []\n",
    "    )\n",
    "    results['simulation'] = failure_results\n",
    "    \n",
    "    print(f\"‚úÖ Baseline: {baseline['delivery_rate']:.1%} delivery, {baseline['avg_latency_ms']:.2f}ms latency\")\n",
    "    print(f\"‚úÖ Post-failure: {failure_results['post_failure']['delivery_rate']:.1%} delivery\")\n",
    "    \n",
    "    validator = ValidationEngine()\n",
    "    validation = validator.validate(analysis_results, failure_results)\n",
    "    results['validation'] = validation\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"STEP 5: RESULTS SUMMARY\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    targets_met = sum(validation.targets_met.values())\n",
    "    \n",
    "    print(f\"\\n‚úÖ Pipeline completed in {elapsed:.2f}s\")\n",
    "    print(f\"\\nValidation Results:\")\n",
    "    print(f\"   Precision:  {validation.precision:.3f} {'‚úÖ' if validation.targets_met['precision'] else '‚ùå'}\")\n",
    "    print(f\"   Recall:     {validation.recall:.3f} {'‚úÖ' if validation.targets_met['recall'] else '‚ùå'}\")\n",
    "    print(f\"   F1 Score:   {validation.f1_score:.3f} {'‚úÖ' if validation.targets_met['f1_score'] else '‚ùå'}\")\n",
    "    print(f\"   Spearman:   {validation.spearman_correlation:.3f} {'‚úÖ' if validation.targets_met['spearman'] else '‚ùå'}\")\n",
    "    print(f\"\\nTargets Met: {targets_met}/4\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"‚úÖ Pipeline function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete pipeline for Financial Trading scenario\n",
    "results = await run_complete_pipeline(\n",
    "    scenario=Scenario.FINANCIAL_TRADING,\n",
    "    scale='medium',\n",
    "    seed=42,\n",
    "    antipatterns=['spof']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Results Interpretation\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "The graph-based analysis methodology provides:\n",
    "\n",
    "1. **Proactive Identification**: Critical components are identified before failures occur through topological analysis.\n",
    "\n",
    "2. **Multi-Metric Approach**: The composite criticality score combines:\n",
    "   - Betweenness centrality (information flow importance)\n",
    "   - Articulation point status (structural importance)\n",
    "   - Impact score (failure consequence)\n",
    "\n",
    "3. **Validation Framework**: Predictions are validated against simulation outcomes using standard metrics (Precision, Recall, F1, Spearman).\n",
    "\n",
    "4. **Anti-Pattern Detection**: Common architectural issues are automatically detected.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "Based on the analysis:\n",
    "\n",
    "1. **Replicate High-Criticality Components**: Components with CRITICAL or HIGH scores should have redundancy.\n",
    "\n",
    "2. **Address SPOFs**: Articulation points represent single points of failure.\n",
    "\n",
    "3. **Review God Topics**: Topics with excessive connections may indicate design issues.\n",
    "\n",
    "4. **Break Circular Dependencies**: Cycles can lead to cascading failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top recommendations based on analysis\n",
    "print(\"üìã RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get analysis results\n",
    "analysis = results['analysis']\n",
    "criticality = analysis['criticality_scores']\n",
    "anti_patterns = analysis['anti_patterns']\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Check for critical components\n",
    "critical_components = [n for n, s in criticality.items() if s.criticality_level == CriticalityLevel.CRITICAL]\n",
    "if critical_components:\n",
    "    recommendations.append(f\"üî¥ {len(critical_components)} CRITICAL components require immediate attention: {critical_components[:5]}\")\n",
    "\n",
    "# Check for SPOFs\n",
    "if anti_patterns['spof_candidates']:\n",
    "    recommendations.append(f\"‚ö†Ô∏è {len(anti_patterns['spof_candidates'])} potential SPOFs detected - consider adding redundancy\")\n",
    "\n",
    "# Check for god topics\n",
    "if anti_patterns['god_topics']:\n",
    "    recommendations.append(f\"üì¢ {len(anti_patterns['god_topics'])} 'God Topics' detected - consider splitting high-traffic topics\")\n",
    "\n",
    "# Check validation results\n",
    "validation = results['validation']\n",
    "if not validation.targets_met['spearman']:\n",
    "    recommendations.append(\"üìä Spearman correlation below target - consider tuning Œ±, Œ≤, Œ≥ weights\")\n",
    "\n",
    "if not recommendations:\n",
    "    recommendations.append(\"‚úÖ No major issues detected - system architecture appears robust\")\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"\\n{i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the complete methodology for **Graph-Based Modeling and Analysis of Distributed Publish-Subscribe Systems**. The approach enables:\n",
    "\n",
    "- **Predictive Analysis** of critical components before failures\n",
    "- **Quantification** of typically qualitative architectural attributes\n",
    "- **Validation** through simulation-based testing\n",
    "- **Visualization** for stakeholder communication\n",
    "\n",
    "The methodology is applicable across multiple domains:\n",
    "- IoT/Smart City deployments\n",
    "- Financial trading platforms\n",
    "- Healthcare monitoring systems\n",
    "- E-commerce infrastructure\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Tune Parameters**: Adjust Œ±, Œ≤, Œ≥ weights for your specific domain\n",
    "2. **Extend Analysis**: Add domain-specific metrics\n",
    "3. **Integrate with Neo4j**: Enable persistent graph storage and advanced queries\n",
    "4. **Deploy Monitoring**: Integrate with production monitoring systems\n",
    "\n",
    "---\n",
    "\n",
    "*Generated by Software-as-a-Graph Research Framework*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
